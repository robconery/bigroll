<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Big Machine - a Home for Self-taught Programmers</title>
  <link href="https://bigmachine.io" rel="alternate" type="text/html"/>
  <link href="https://bigmachine.io/feed.xml" rel="self" type="application/atom+xml"/>
  <updated>2025-05-19T22:09:32.733Z</updated>
  <id>https://bigmachine.io/</id>
  <author>
    <name>Rob Conery</name>
  </author>

  <entry>
    <title>Turning a Blog Into a Book</title>
    <link href="https://bigmachine.io/posts/turning-a-blog-into-a-book" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.733Z</updated>
    <id>https://bigmachine.io/posts/turning-a-blog-into-a-book</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/troy_cover.jpeg" alt="Turning a Blog Into a Book" /></p>
I find programmers to be fascinating people. We're all human but there's an additional _something_ that, to me, is fascinating to dig for. It's the main reason I created [This Developer's Life](https://thisdeveloperslife.com) with Scott Hanselman: _I like digging into the human side of programmers_.

## We're All Human... But...

There's usually something more to our stories. Maybe it's an extra bit of analysis we give to a life decision or, in my case at least, not reading a social situation in the same way as someone else which can cause issues.

I was listening to [my favorite TDL interview](https://thisdeveloperslife.com/post/2-0-5-typo) the other day and marveled at the way Bill Hill, with his thick Scottish Brogue, was able to effortlessly weave a story about fonts and tracking animals in a forest. I also couldn't get over how taken [Geoffrey Grosenbach](https://twitter.com/topfunky) was the first time he saw the Klavika font!

I **loved** having the time to sit with people during the interviews I did for TDL. We would talk for hours and for the first 45 minutes I would usually ask "easy" questions, letting the person I was talking to ease into the idea of going _deeper_ into the more human side of things.

I got surprising candid interviews this way! And then a few years ago, a thought occurred to me...

## Why Not Try This With a Blog?

Let's be honest: most blog posts are written with little time given to the story being told. Even if the time is taken, blog writers (myself included) have become much more anodyne and cautious with what they're writing about.

There are a few writers who are a bit different, however. Some are a bit more brazen and don't give a toss if people are offended and yet others write with a degree of personal candor that you feel like their a good friend of yours!

That, my friends, is [Troy Hunt](https://troyhunt.com). It's entirely likely you already know who Troy is or, more likely, have used (directly or not) his service [Have I Been Pwnd!](https://haveibeenpwnd.com). He's an outstanding writer and has no problem sharing the ups and downs of his career and personal life, but.

**But.**

## There's Something More There

As open as Troy is, I _know_ there are details in his story that I would find fascinating. If I was to interview him I would probably start with a simple question like:

> How did you end up doing security stuff? How does anyone?

From there I would keep digging because I would want to know if it was a conscious decision, dumb luck, or both! The fun thing is that Troy has _sort of_ blogged about this stuff, but like I said, _there's more there_.

This, to me, is the pivotal thing about this idea: **Troy is writing introductions and epilogs to each post**, often times providing answers to my questions. Mind you this isn't juicy gossipy stuff (for the most part) - often it's from a simple question like "wait - you worked at Pfizer for a long time - until 2015 or something! What made you quit? Did you leave because of the Big Training Bucks or... what?"

He told me a few things, a lot of which he had written in the post (which I had read). It all made sense but there was something missing. He then said to me "OK, let me tell you about Gerard..."

It was a _fantastic_ story and is _exactly_ what I was looking for. The small, major details that make us human and make stories come alive.

## Help Us?

We don't have a title, a cover and we're not entirely sure of the posts we're going to include. Troy is hard at work refining the intros and epilogs (which are all written but we're in the editing process) and we would both love your help.

To be specific:

- I feel good about the presentation/focus of the book, however I could be missing something
- Try as I might, I just can't find the right title. We have a few candidates but I think something from the public would be good
- Same goes for the cover. I keep vascilating between dramatic, goofy and personable. Would love some feedback!

To that end [I created a mailing list](https://book.troyhunt.com) and we'll be sending out updates when they happen... maybe twice a week. We're also going to live stream an editing/writing session as an experiment to see if doing this completely transparently helps unblock us on some things.

I can't post the link to the live stream because of YouTube reasons (which I don't understand) but if you want to watch we're "doin it live" on April 13th at 2PM PDT. I'll post the link here when I can but you can also check twitter (@robconery and @troyhunt) as we'll post it there too.

I'm excited about this idea. I'd love to work with others in the future if it all pans out.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/troy_cover.jpeg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/troy_cover.jpeg" />
  </entry>
  <entry>
    <title>Writing, Editing and Formatting a Technical Ebook in 2021</title>
    <link href="https://bigmachine.io/posts/writing-editing-and-formatting-a-technical-ebook-in-2021" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.733Z</updated>
    <id>https://bigmachine.io/posts/writing-editing-and-formatting-a-technical-ebook-in-2021</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/book_2021.jpeg" alt="Writing, Editing and Formatting a Technical Ebook in 2021" /></p>
This post could easily be 20,000 words... _there is just so much shit to wade through to get your book looking just so_. What do I mean? Here are my concerns, which are simple ones I think. I want:

- **Fonts to be crisp and readable**, the line-heights appealing to the eye and perhaps a few flourishes here and there like a drop-cap or small-capped chapter intro
- **Images to layout with proper spacing**, a title for accessibility and to be center-aligned
- **Code samples to be formatted** with syntax highlighting
- **Callouts** for ... callouts. Like a colored box with a title maybe.

Things like this are easy for word processing tools like Word or Pages but when it comes to digital publishing, not a chance.

I could fill up paragraphs but I won't. I'll just summarize by saying: **formatting ebooks is a massive pain**. Thankfully I've figured a few things out.

## Have You Tried?

Yes, I'm 99% sure I have. I've used:

- **Scrivener** (and I love it). Great for writing, crap for formatting.
- **Markdown** turned into HTML and then formatted with Calibre. Works great - I did this with _Take Off with Elixir_ and it looked amazing. Tweaking Calibre and hand-editing the CSS wasn't so fun, though.
- Michael Hartl's [Softcover](https://softcover.io) which was _stellar_ at formatting things and looked great but the styling choices were lacking. There were ways to edit things but I'm not a LaTex person. Installation was complex but doable... overall I enjoyed this one the most.
- A zillion others including **iAWriter, Bear, Ulysses, Pages/iBooksAuthor** and many others I'm forgetting.

I've written 5 books over the last 6 or so years and I'm currently writing 2 more (which I'll go into in a second). I swear to you I've tried just about everything.

When I wrote _[A Curious Moon](https://bigmachine.io/products/a-curious-moon/)_ I just went back to Word and decided to break the writing process up into two steps: **actual _writing_ and then _formatting_**. I knew this is what a lot of writers did, but my process of writing (constantly evolving, "living" ebooks) didn't lend itself to this. No matter - I can adapt.

It worked out pretty well, too. **I wrote everything in Word** and then hired an editor. Once editing was done I ported it to InDesign and spent weeks (literally) learning this massive tool.

It was worth it, I think, the book looks amazing...

![](https://bigmachine.io/img/gravity_assist.jpg)

The Problem is that it's **laborious** and just killed my inspiration. Making edits, for instance, means I have to use InDesign's ultra crap word editor to fix things, which isn't fun.

Things get really exciting when InDesign bumps versions and everything is _completely wrecked_ because they changed the way images are resolved (which happened)...

OK, enough complaining! Here's where I'm at today, with a process I really like.

## Writing in 2021

In my [last post](/2021/04/06/turning-a-blog-into-a-book/) I mentioned that I was writing a book with [Troy Hunt](https://troyhunt.com). It's a fun project in which I'm:

- Curating posts from his blog that I find interesting
- Curating comments from those posts and adding them at the end (anonymously)
- Pushing Troy to write retrospectives on these posts, giving some kind of backstory

Once again I decided to use Word and I wonder if that was the right decision. My thinking at the time was that Troy is a Windows person and I could use OneDrive to share the document with him so he could make comments.

There are problems with this, which include:

- The **document is huge**. We're up to 800+ pages with quite a lot of images. Syncing this thing real time is kind of a bugger.
- The **formatting looks horrendous** and trying to explain to Troy "yeah, nah mate I'll make it look good" is a bit hard. Especially when he replies "I already did that... have you seen my blog?" and I reply "yeah... ummm..."
- **Troy writes in Australian** and uses words like "flavour", "favourite" and "whilst". Word's spell checker doesn't like that and YES I've reset it to Aussie English but it doesn't seem to make a difference. Red squiggles are everywhere!

These are interesting problems to solve! For Troy, his blog is **a done thing** so my formatting woes are a bit ridiculous. I completely understand this, and I think I blew it by pulling things into Word first.

### Writing From Scratch: Ulysses

This might come as a shock, but I find Ulysses to be, hands-down, the best writing experience I've ever had. In 2018 [my choice](https://bigmachine.io/2021/04/06/turning-a-blog-into-a-book/) was [Scrivener](https://www.literatureandlatte.com/scrivener/overview):

> When it comes to assembling your thoughts and structuring your manuscript, there is nothing that beats Scrivener. The functionality it gives you is astounding, and you can write little snippets or big hunks - its all up to you.

> It successfully detaches the idea of text from presentation. You can kick up a manuscript and then compile it for various outputs such as epub, mobi, docx, and pdf. The compiler takes time to get used to, but once you do you can have a **serviceable** book.

This is still true, the keyword being "serviceable". Writing in Scrivener is an engineer's dream as it focuses to completely on the _process_ of writing. The aesthetics of it, however, **suck** and you end up with something... _serviceable_:

> By “serviceable” I mean the text will show on screen as well as the images, and if you’re lucky maybe some fonts will show up. I played with the compiler for days (literally), trying to get my epub layout flow the way I wanted. Line height, title leading, first paragraph non-indent… all of this is tricky or non-existent with Scrivener.

Ulysses, on the other hand, is **pure markdown**:

![](https://blog.bigmachine.io/img/shot_718.jpg)

When you come from Word and Scrivener, this is _amazing_. I can't tell you how many times I have to drop into "invisibles" to correct some janky weird formatting/layout issue in both Word and Scrivener. Things get so utterly ugly that I have to stop writing to fix it, which really makes the writing process suck.

With Ulysses, however, I just write in Markdown and I'm a happy person. It's more than just a Markdown editor - it's also a _writer's friend_ with some amazing support tools. The one I really like is the "Review" feature, which loads up your current page to [Language Tool](https://languagetool.org/). It takes the feedback (which is free) and shows you where corrections are suggested. There are also ways to annotate your text and leave yourself comments, which I also love.

When I'm ready to preview how things will look, there's a preview button right there that shows my markdown directly in epub. This is fine, if you're OK with producing something that's... "serviceable". But that's not what I want with Troy's book.

## Formatting for 2021

There are two apps that will format a book to near pixel-perfection for EPub and PDF that don't come from Adobe and require a master's degree:

- Apple Pages (which has absorbed iBooksAuthor)
- [Vellum](https://vellum.pub)

Apple Pages will open up a Word document, read the styling, and immediately show you a 99% perfect recreation of your Word document. From that point you can start polishing things up and off you go. It really is a great bit of software, but, oddly, it _sucks_ for writing. Not quite sure why.

The winner for me is Vellum. Check this out:

![](https://blog.bigmachine.io/img/shot_719.jpg)

I was able to send my text directly from Ulysses into Vellum and this is the formatting I saw. It is _perfect_. A downside with Vellum is that it doesn't support syntax highlighting which was one of my requirements :(. Another downside is that the export themes are _slightly_ customizable, but that's about it. I don't mind it - it keeps me from going nuts.

I'm OK with doing screenshots for code and then making sure the reader has a link to a GitHub repo with code in it - that's what I did for _A Curious Moon_ and it worked out fine. It also looks better and, let's be honest, no one is going to copy/paste code from an ebook - it's horrible!

The features in Vellum are tremendous and it's great for formatting a high-end ebook. It can't do all that InDesign does because InDesign is all about document designing. But I kind of like that - Vellum is focused on wonderful book formatting, putting the focus on your words and content, no more.

I think it will work really well for Troy's book - but **you tell me**.

## Live Streaming the Formatting Process

I'll be live streaming the formatting process with Troy on Monday, **April 12th at 2PM PDT**. We'll have a few chapters of our book open in Word and I'm going to pull them into Vellum to see what he thinks. If he doesn't like it, I'll pull the book into Apple Pages and see what I can put together there.

If he doesn't like _that_ I'll have to go back over to InDesign which isn't the worst thing in the world, but it's detailed enough that we'll have to do another full stream.

We'll also be discussing titles and cover ideas - so please join us! I would love to hear your thoughts. If you want to be updated on our progress [I created a mailing list](https://book.troyhunt.com) and we'll be sending out updates when they happen... maybe twice a week.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/book_2021.jpeg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/book_2021.jpeg" />
  </entry>
  <entry>
    <title>A Simpler Way to Azure</title>
    <link href="https://bigmachine.io/posts/a-simpler-way-to-azure" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.733Z</updated>
    <id>https://bigmachine.io/posts/a-simpler-way-to-azure</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/azx_init.jpg" alt="A Simpler Way to Azure" /></p>
**TL;DR:** [I made a fun CLI “wrapper”](https://github.com/robconery/azx) for working with web apps and Azure in a very Heroku way. It’s called “AZX” and you can read a full walkthrough and installation instructions from the GitHub repo.

One of my jobs at Microsoft is to help product teams understand how developers work in other ecosystems. In other words: _places other than the .NET/Azure world_. That’s a pretty big place, so I’ve decided to stay focused on an experience I love: [Heroku](https://heroku.com).

I’ve used Heroku for years and years and love the simplicity. You can do similar things nowadays with AWS, DigitalOcean, Netlify and yes, even Azure. The whole idea of `git push` deployment is pretty standard - in fact it’s kind of passé.

Either way, the experience that Heroku pioneered is still compelling. Once you install the CLI and login you simply:

- Create a project locally with `heroku create`
- Work on your app and when you’re ready
- Deploy your app using Git: `git push heroku main`

That’s it. Heroku will “guess” what stack it needs to use, provision a set of free resources for it, and before you know it you’re up and running.

## Scaling With You

Of course applications are much more complex than that. You’d likely want a database, a cache of some kind (like Redis), more capable logging and so on. You’re also going to quickly outgrow the free tier - and this, to me, is what’s super groovy about Heroku: _it’s there with you for all of this._

When you want to scale things up you can use `heroku ps:scale web=1` from your local dev directory and boom, you’re scaled. To add better logging you simply `heroku addons:create papertrail`.

Adding a database, such as PostgreSQL (because… why wouldn’t you?) you need to look up the plan name you want and then `heroku addons:create heroku-postgresql:<PLAN_NAME> --version=12`.

The fun part is that everything is configured and wired for you on the back end - there’s nothing you need to do aside from make sure you use “conventional” environment variables (like `DATABASE_URL`, `REDIS_URL` etc).

You can access all of these resources locally with the `heroku` tool. For instance, to read your logs you can `heroku addons:open papertrail` and there are your app logs!
Cool service - and it got me wondering how difficult would it be to do this with Azure?

## A Rite of Passage

I told Burke Holland my idea and he started laughing and quipped:

> I think that’s a rite of passage here at Microsoft - everyone wants to create a Heroku clone!

Fair enough! _Challenge accepted_.

To be honest I never thought this would see the light of day but my fellow friends in Cloud Advocacy pushed me to open it up, so here goes.

## A Localized Azure Experience

The idea with this project is straightforward: _let’s bring Azure to you instead of you to Azure_. The Azure CLI is outstanding but, if I’m honest, it’s aimed at IT people and not developers. The good news is that, with a little love and imagination, you can leverage the CLI and have a little magic happen. Let’s see…

I have a local Python application, the Tailwind Starter site that I’ve been working on and I want to get it up and live on Azure. I’ve installed `azx` using NPM:

```sh
    npm install -g @robconery/azx
```

I then head into my project directory and run `azx`:

```sh
    cd ~/Tailwind
    azx
```

This will display the help screen:

![](https://paper-attachments.dropbox.com/s_39654B13F38E173EF27924C19AFCEC4939F6BB36F87440CDAEC9E9D9A2568BF0_1637349323822_bip_17.png)

AZX is seeing that there’s no local settings file, which would be saved in `.azure` so it’s telling me what choices I have at this point and offering me a tip. My goal with this is to decrease the cognitive load that comes with something as complex as Azure (or any cloud service).

Here, we have 3 choices:

- `init` which creates our project
- `get` which will load an existing project (more on that in a minute) and
- `lookup` which provides supporting information like regions, runtime names and SKUs.

That’s it - and I love the sparseness of it. Let’s keep rolling by creating our project.

## Creating an Azure Project

An “Azure project” is something I made up - it’s not an Azure term. To create one, we just need to run `azx init`:

![](https://paper-attachments.dropbox.com/s_39654B13F38E173EF27924C19AFCEC4939F6BB36F87440CDAEC9E9D9A2568BF0_1637349603238_bip_18.png)

Running `azx init` does a couple of things, most notably:

- It creates a “Resource Group” on Azure for us, which is a virtual “bucket” for all the Azure stuff we’ll be creating for this project.
- It decides a name for us because naming is hard. You can override this if you want, but I like Heroku’s way of doing this - it keeps things simple.
- Our local project settings directory, `.azure` is created and in it is a JSON file with our project settings. We’ll see that in a minute.

Notice also that we’re given a tip about what to do next: `azx app create`. That wasn’t a choice before, but it is now:

![](https://paper-attachments.dropbox.com/s_39654B13F38E173EF27924C19AFCEC4939F6BB36F87440CDAEC9E9D9A2568BF0_1637349831023_bip_19.png)

Another example of keeping the cognitive load to a minimum - it just doesn’t make sense to show a command choice if you can’t run that command, which we couldn’t before. Now that we have a project, we can setup our application on Azure.

## Setting Up Our App Service

Each subcommand, just like the Azure CLI, has a help screen dedicated to it. If we run `azx app create` ` --``help ` we’ll see that we have only one command available to us: `azx app create <runtime>`. Once again, this is by design.

Following our tip, we can now create setup where our app is going to live on Azure. If we don’t know the runtime choice to enter we can ask the CLI by using `azx lookup runtimes` and we’ll see a list. In my case I need `azx app create python` because I’m using Django:

![](https://paper-attachments.dropbox.com/s_39654B13F38E173EF27924C19AFCEC4939F6BB36F87440CDAEC9E9D9A2568BF0_1637350082952_bip_20.png)

A bunch of goodness happened here:

- An App Service “Plan” was created and you’re told that this is basically a VM. It tried to create a free one as a logical first step but couldn’t because my account is limited to only 1 free (F1) App Service. You don’t care about any of that - so AZX just set it up for you. All of this can be overridden by specifying `--sku`.
- The location of the App Service, something you normally need to specify, was set to `West US` by default. You can override that too - but if you’re kicking the tires on Azure … why think about this stuff?
- A web application, on Azure, was created with local Git deployment - meaning you can `git push` just like with Heroku. This can be changed, easily, later on.
- Comprehensive logging was set up, which is not turned on by default.
- Deployment credentials were created for you using a random name and a GUID for a password. These credentials were added to your local Git repository under the `azure` remote.

That’s a lot of stuff you would have needed to understand before deploying your application. It’s a good idea to understand it at some point, but often if you’re coming to a service for the first time, it’s kind of nice if you don’t have to fiddle with every knob. In our case, a lot of this stuff is basic (like logging) and can just be turned on.
Notice that we’re given another tip - how to set up our database. We didn’t have an `azx db` choice before, but now we do because we have an application!

![](https://paper-attachments.dropbox.com/s_39654B13F38E173EF27924C19AFCEC4939F6BB36F87440CDAEC9E9D9A2568BF0_1637350578603_bip_21.png)

## Setting Up the Database

We have three choices (as of now) for databases: PostgreSQL (the correct choice), MySQL or Mongo DB (which is actually Cosmos DB using the Mongo DB driver). As you might expect, my database will be PostgreSQL.

To set that up, we can follow the tip that you see there at the top of the help screen. Notice that, just above the tip, we can see the basic description of our application. We see the name (cold-dust-76) and also that it’s a web application.

Let’s push on and create the database using `azx db create postgres`. If we wanted to know more about our choices we could use `azx db create` ` --``help ` and see that the only choice we have is to create a database with the specified engine.
Off we go…

![](https://paper-attachments.dropbox.com/s_39654B13F38E173EF27924C19AFCEC4939F6BB36F87440CDAEC9E9D9A2568BF0_1637350862052_bip_22.png)

This takes a few minutes to run and you’re told that straight off. On average, provisioning a PostgreSQL server takes about 3-4 minutes. Cosmos DB takes a bit longer - up to 6 minutes - but the nice thing is what comes next: _all the minutiae is handled for you._ You can, if you want, get up and go for a walk or write in your bullet journal - this time is given back to you!

So, what happened here? A lot of things that would normally take you 20 clicks in the portal, or more:

- A PostgreSQL server was created for you with a conventional name that you can override (a “-db” was added to the end of your project name).
- A firewall rule was added so your web app can see your database.
- Another firewall rule was added using your local IP address so that _you_ could see your database.
- Admin credentials were created for you, which is the way (in my mind) it should be. Once again, a semi-randomized name (like `admin-451`) was used with a GUID password. These credentials were then used to construct access credentials for your application (a `DATABASE_URL`) and those credentials were added to your Azure web app configuration settings for you.
- Those same credentials were added to a local `.env` file inside of the `.azure` directory.

We can now access our database using `azx db connect`:

![](https://paper-attachments.dropbox.com/s_39654B13F38E173EF27924C19AFCEC4939F6BB36F87440CDAEC9E9D9A2568BF0_1637351478083_bip_23.png)

**Note**: _this only works if you have the PostgreSQL binaries installed locally_.
From here we can, once again, follow the tip and set up our application database by sending in a SQL file. Given that we’re working with the local client binaries, we can redirect STDIN in the same way we might with `psql`:

![](https://paper-attachments.dropbox.com/s_39654B13F38E173EF27924C19AFCEC4939F6BB36F87440CDAEC9E9D9A2568BF0_1637351597738_bip_24.png)

We’re ready to go! At this point we have about 7 total minutes elapsed. But what do we do now?

One of my main goals with this tool is to have it _help you_ as much as possible, without doing too much or overloading you with concepts. The only thing you need to remember is to run `azx` when you don’t know what to do next:

![](https://paper-attachments.dropbox.com/s_39654B13F38E173EF27924C19AFCEC4939F6BB36F87440CDAEC9E9D9A2568BF0_1637351807989_bip_25.png)

Things have changed once again, and this is because we have a database to go with our application. We have a few tips at the top - the first of which is telling us how we can deploy our application using Git. We’ll talk about scaling and other things in just a minute.

## Deployment

We have Git deployment set up and an `azure` remote has been added for us with our deployment credentials embedded in the remote URL, all that’s left to do is `git push azure master` (or `main`):

![](https://paper-attachments.dropbox.com/s_39654B13F38E173EF27924C19AFCEC4939F6BB36F87440CDAEC9E9D9A2568BF0_1637352040705_bip_26.png)

Off we go. What you see here is Oryx, the build tool used by Azure’s App Services. It’s received our code and, using a post-receive hook, is setting things up for us. In this case it’s a Python environment but this also works with Node and Rails, etc.

It takes a minute - for Django it takes about 4 minutes for a full deployment to happen. This is due to the project settings and packages being setup completely on the first run.
After a few minutes we’re done and, hopefully, ready to go!

![](https://paper-attachments.dropbox.com/s_39654B13F38E173EF27924C19AFCEC4939F6BB36F87440CDAEC9E9D9A2568BF0_1637352629414_bip_27.png)

Ready to go… where exactly? What do we do now? Hopefully at this point you get the idea: _ask AZX:_

![](https://paper-attachments.dropbox.com/s_39654B13F38E173EF27924C19AFCEC4939F6BB36F87440CDAEC9E9D9A2568BF0_1637352846170_bip_28.png)

Our app menu has a bunch of new choices. We can change configuration settings on Azure, scale things, look at the logs (which we’ll do in a minute) and finally `open` it! That’s what I want to do…

**Note**: _if you’re running this inside WSL you’ll get an error due to access permissions to Windows executables. If that happens, you can use_ `*azx app get_settings*` _to see your app’s URL._

It takes a few seconds to load up on first run, but here we go!

![](https://paper-attachments.dropbox.com/s_39654B13F38E173EF27924C19AFCEC4939F6BB36F87440CDAEC9E9D9A2568BF0_1637353058177_bip_29.png)

We’re live and our site is happily pulling data from our database. We have a problem, however, in that our images aren’t showing up for some reason. Let’s troubleshoot.

## Troubleshooting and Moving On

This CLI has gone through a few iterations and I showed it once to executive types at Microsoft, which went pretty well, but there was one very important bit of feedback:

> Helping people use Azure is great, but we need to be sure we support them into the future and not back them into the corner

Makes perfect sense. To that end, I added as much as I could to help you out as you work with your application, specifically:

- The ability to scale your App Service up or down.
- The ability to scale your Database Server up or down.
- Rotating your database password (which also updates your web app).
- Viewing your application logs.

That last is what we need now because our images aren’t showing up. Let’s see what happened using `azx app logs`:

![](https://paper-attachments.dropbox.com/s_39654B13F38E173EF27924C19AFCEC4939F6BB36F87440CDAEC9E9D9A2568BF0_1637353465498_bip_30.png)

Doing this will `tail` the logs so you can see what’s happening realtime and, digging in, I shortly find out that I didn’t set my `STATIC_ROOT` properly so my images aren’t showing. I need to have that set for production… but how?

Let’s take a look at our web app configuration settings using `azx app get_settings`:

![](https://paper-attachments.dropbox.com/s_39654B13F38E173EF27924C19AFCEC4939F6BB36F87440CDAEC9E9D9A2568BF0_1637353683663_bip_31.png)

What we need to do is update these settings with a `STATIC_URL` that points to a CDN somewhere that stores our images. How do we write these settings? Let’s ask AZX!

![](https://paper-attachments.dropbox.com/s_39654B13F38E173EF27924C19AFCEC4939F6BB36F87440CDAEC9E9D9A2568BF0_1637353833516_bip_33.png)

As I mentioned (briefly) before, a `.env` file containing all of our application secrets lives inside of our `.azure` directory. We can update that file and then save it up to Azure. Here’s what it looks like:

![](https://paper-attachments.dropbox.com/s_39654B13F38E173EF27924C19AFCEC4939F6BB36F87440CDAEC9E9D9A2568BF0_1637353923099_bip_34.png)

All of these settings were created when our database was created and, yes, there’s a `.gitignore` file that was created as well to ensure that we don’t commit this to source control. All we need to do now is to add a setting in here for `STATIC_ROOT`:

![](https://paper-attachments.dropbox.com/s_39654B13F38E173EF27924C19AFCEC4939F6BB36F87440CDAEC9E9D9A2568BF0_1637354131179_bip_35.png)

Now we just need to use `azx app write_settings`:

![](https://paper-attachments.dropbox.com/s_39654B13F38E173EF27924C19AFCEC4939F6BB36F87440CDAEC9E9D9A2568BF0_1637354176096_bip_36.png)

Heading to the Azure portal to confirm:

![](https://paper-attachments.dropbox.com/s_39654B13F38E173EF27924C19AFCEC4939F6BB36F87440CDAEC9E9D9A2568BF0_1637354268783_bip_37.png)

Great! We just saved ourselves another 10 or so clicks, including restarting our App Service so these changes are picked up.

## There’s More To Do!

I’ve had tons of fun putting this project together. It was supposed to be a quick prototype but I figured it could also help someone too. There are a few more things I’d like to get done, including caching, deployment slots and more.

The main thing is that you can get to a solid point with your application and then jump over to the regular CLI or to the portal… the hard part being behind you.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/azx_init.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/azx_init.jpg" />
  </entry>
  <entry>
    <title>Deploying Node and Mongo to Azure Using AZX</title>
    <link href="https://bigmachine.io/posts/node-mongo-cosmosdb" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.733Z</updated>
    <id>https://bigmachine.io/posts/node-mongo-cosmosdb</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/azx-node-mongo-cosmos.jpg" alt="Deploying Node and Mongo to Azure Using AZX" /></p>
I’m going through a number of deployment stories with [my little CLI experiment, AZX](https://github.com/robconery/azx). I’ve been focused _solely_ on the non-MS developer experience, which means Python, Ruby, Node, etc. Up until now, I’ve been using Postgres as a database (as one should), but I also know that Mongo DB has a massive following - so I decided to focus on that for this latest experiment, specifically with respect to Node.

## Using Cosmos DB Instead of Mongo DB

Azure doesn’t do hosted/managed Mongo DB. Instead, you can work with Cosmos DB using the Mongo DB toolset. When I first learned about this I was confused - I never thought of Cosmos DB as a document store, but [it’s a thing](https://docs.microsoft.com/en-us/azure/cosmos-db/mongodb/mongodb-introduction):

> The Azure Cosmos DB API for MongoDB makes it easy to use Cosmos DB as if it were a MongoDB database. You can leverage your MongoDB experience and continue to use your favorite MongoDB drivers, SDKs, and tools by pointing your application to the API for MongoDB account's connection string.

Cosmos DB is kind of weird this way: _you can work with it using multiple APIs_. The most popular is the SQL API, which allows you to work with it using SQL in the same way you might with a relational database (though it's not relational, it's strictly JSON). There’s also a Cassandra API which makes Cosmos behave as if it were a column store and the Table API, which makes it behave like a key/value store. Wild stuff.

I was intrigued by this so I decided to push Cosmos DB a bit _while also_ working on a deployment scenario for AZX.

## My Node/Mongo App

The first step was to find a sample app that did more than a todo list. I couldn’t find one so I made what I consider to be [a reasonable “starter” site](https://github.com/robconery/node-mongo-start) - something I would use in the future. It has:

- **Tailwind** CSS with the [free starter kit from Creative Tim](https://demos.creative-tim.com/notus-js). Just gorgeous stuff.
- **Authentication** using OAuth (Google and Github) and Magic Links.
- **Email** capability using Nodemailer
- Site-wide **notification system** using flash and Toastr
- Basic modeling using **Mongoose**
- As few of my opinions as possible

Those last two points go together. If you know me, you know [I dislike ORMs](https://bigmachine.io/2015/02/21/its-time-to-get-over-that-stored-procedure-aversion-you-have/) tremendously and you also know that I haven’t, historically, been a big fan of Mongo DB. I do know that it’s used a lot and it’s improved a lot over the years… so I decided to get over myself and get to work.

![](https://paper-attachments.dropbox.com/s_3AAE70EFA8602A8C9CD5ED6DB9E53DAEFBD7B03A9AC4C298AF090CEEFEA60F18_1638473138607_bip_43.jpg)

## Provisioning Resources

As I mention, I’m using my little CLI experiment, [AZX](https://github.com/robconery/azx), to provision the Azure resources I’ll need for this. If you want to play along, you can:

```sh
npm install -g @robconery/azx
```

Step one is to intitialize the project:

```sh
azx init
```

![](https://paper-attachments.dropbox.com/s_3AAE70EFA8602A8C9CD5ED6DB9E53DAEFBD7B03A9AC4C298AF090CEEFEA60F18_1638474000174_bip_44.png)

So far, so good. A resource group was created for me (which I think of as a “project” with a generated name. A settings folder (`.azure`) was also created. The next step is to provision the App Service and friends:

```sh
azx app create node
```

![](https://paper-attachments.dropbox.com/s_3AAE70EFA8602A8C9CD5ED6DB9E53DAEFBD7B03A9AC4C298AF090CEEFEA60F18_1638474138030_bip_46.jpg)

I went over this output in detail in [my previous post](https://rob.conery.io/2021/11/19/a-simpler-way-to-azure/), but to quickly summarize:

- Many knobs were twiddled at Azure. A “plan” was created as well as a web application, sensible logging was turned on and deployment credentials created.
- A local Git remote called “azure” was created with your new deployment credentials and added to your local Git repo for you.
- AZX tried to use the free Linux tier (F1), which is a sensible first step, but as you can see my account isn’t eligible as I already have one of those, so the next level up, B1, was used.

The goal with AZX is to decrease the cognitive load when it comes to working with Azure _while not_ backing you into some kind of opinionated corner. You can change any and all of this if and when you need.
Right then… next step is to provision Cosmos DB:

```sh
azx db create mongo
```

![](https://paper-attachments.dropbox.com/s_3AAE70EFA8602A8C9CD5ED6DB9E53DAEFBD7B03A9AC4C298AF090CEEFEA60F18_1638474452085_bip_47.jpg)

Once again, AZX tried to opt in to the free tier of Cosmos DB ([yes, there is one](https://docs.microsoft.com/en-us/azure/cosmos-db/free-tier)) but since I already have one a regular Cosmos server was created for me.

AZX also queried Azure for the access keys and created the connection string for me, popping it locally into `.azure/.env` which, yes, is guarded by a `.gitignore` so it doesn’t get added to source control.
I can now connect to Cosmos DB using the Mongo DB client:

![](https://paper-attachments.dropbox.com/s_3AAE70EFA8602A8C9CD5ED6DB9E53DAEFBD7B03A9AC4C298AF090CEEFEA60F18_1638474983299_bip_50.jpg)

I think that’s pretty slick!

## Updating Azure Configuration

Every application will have some kind of environmental configuration, which usually includes a database connection string but can also contain, like mine does, 3rd party access keys.
Specifically: I’m using OAuth for authentication and I’m also using email, which means I need to have SMTP credentials for NodeMailer.

To make this easy, I can add these settings to my `.azure/.env` file:

![](https://paper-attachments.dropbox.com/s_3AAE70EFA8602A8C9CD5ED6DB9E53DAEFBD7B03A9AC4C298AF090CEEFEA60F18_1638474799098_bip_48.jpg)

The `DATABASE_URL` was already in there, so I didn’t need to change it. Now I can ask AZX to push these settings to Azure:

```sh
azx app write_settings
```

![](https://paper-attachments.dropbox.com/s_3AAE70EFA8602A8C9CD5ED6DB9E53DAEFBD7B03A9AC4C298AF090CEEFEA60F18_1638474901839_bip_49.jpg)

This command reached out to Azure, specifically my web application's configuration, and added all of the values in my .env file as configuration settings. Easy peasy!

All in all this process took about 5 minutes. We’re not quite there yet as we need to deploy our application, which I can do using Git directly:

![](https://paper-attachments.dropbox.com/s_3AAE70EFA8602A8C9CD5ED6DB9E53DAEFBD7B03A9AC4C298AF090CEEFEA60F18_1638475039740_bip_51.jpg)

My deployment credentials are stored in my remote connection string, so all I need do is use `git push azure master` (or main, depending) and up it goes.

This process takes about 2 minutes to complete, but when it’s done:

```sh
azx app open
```

And up pops a browser after a 30 second spin up:

![](https://paper-attachments.dropbox.com/s_3AAE70EFA8602A8C9CD5ED6DB9E53DAEFBD7B03A9AC4C298AF090CEEFEA60F18_1638475144411_bip_43.jpg)

I clocked just over 8 minutes total on this process, which I think is pretty neat. The big test, though, is whether I’ll be able to access Cosmos DB and login. Let’s try it!

I don’t have an account on the site, but I can create one by going to the registration page:

![](https://paper-attachments.dropbox.com/s_3AAE70EFA8602A8C9CD5ED6DB9E53DAEFBD7B03A9AC4C298AF090CEEFEA60F18_1638475264677_bip_53.jpg)

Once I click “register”, I get this notice:

![](https://paper-attachments.dropbox.com/s_3AAE70EFA8602A8C9CD5ED6DB9E53DAEFBD7B03A9AC4C298AF090CEEFEA60F18_1638475307097_bip_54.jpg)

It’s a little smushed in there because I collapsed the window - but you can see at the top there’s a blue box that confirms an email is on its way to me. This is [Toastr](https://johnpapa.net/toastr100beta/#:~:text=toastr%20is%20a%20simple%20JavaScript,toastr.), something my colleague John Papa helped create. It’s wired into `express-flash` and will pop open if there’s a message to be shown.

Checking my email, I have a link!

![](https://paper-attachments.dropbox.com/s_3AAE70EFA8602A8C9CD5ED6DB9E53DAEFBD7B03A9AC4C298AF090CEEFEA60F18_1638475493852_bip_55.jpg)

I know a few people who don’t like the whole magic link thing, but as a site owner I _really like it_. No storing passwords at all and, as a bonus, you get 2FA and email verification.
Clicking on that link I’m taken back to the site where I’m logged in:

![](https://paper-attachments.dropbox.com/s_3AAE70EFA8602A8C9CD5ED6DB9E53DAEFBD7B03A9AC4C298AF090CEEFEA60F18_1638475692916_bip_56.jpg)

I’m also shown a nice “welcome back” message - but I wasn’t quick enough with my screenshot to get it.

## So What's This All About, Anyway?

I like [this little starter app](https://github.com/robconery/node-mongo-start) - I’ve been having lots of fun with it and if you want to help build it out, please do. It’s likely a little rough around the edges as I made it in a short period of time, but I’ll keep improving it. I do think it can be useful for people learning Node who might need a leg up on their project.

What would be extra double-secret-probation fun is if you tried to deploy it to Azure to see if you could break something. Instructions are in the README.

Finally - there is a lot of interest internally at Microsoft regarding a more comprehensive developer-focused experience. That’s why I made AZX, as a thought experiment that I hoped would generate feedback. If you have some, please do [hit me up on Twitter](https://twitter.com/robconery).

It's not my intention to create a full-blown toolset here, **this is just a thought experiment**. I'm hoping to help the internal teams as much as I can, and that's about all.

Hope you have some fun!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/azx-node-mongo-cosmos.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/azx-node-mongo-cosmos.jpg" />
  </entry>
  <entry>
    <title>Yet Another Fork In The Road</title>
    <link href="https://bigmachine.io/posts/yet-another-fork-in-the-road" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.730Z</updated>
    <id>https://bigmachine.io/posts/yet-another-fork-in-the-road</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/splash.jpg" alt="Yet Another Fork In The Road" /></p>
I'm headed out on my own once again, and no, there's no drama here. Pluralsight has decided to venture into a realm of training that I'm just not qualified for (enterprise) so they were good enough to let me go off and do my own thing again.

I think that's pretty damn honorable of Aaron and gang. I wish them all the best... Now I get to play...

## Once Again, Trying Something Completely New

Pluralsight still owns the Tekpub brand, of course, so I can't just do that all over again - and honestly I don't think I want to. It was fun, to be sure, but there's a sense of "ending something" that I think should just... be. It balances the event in some way.

Over the last few months I've been trying some new ideas as well. For instance, I was extremely impressed by [the Discover Meteor](http://www.discovermeteor.com) tutorials. Such a complete experience, top to bottom. The only thing I wish they had was a video to accompany their amazing book.

I also like the idea of sprinkling in some kind of story or narrative. It's so easy to be cheesy, but at the same time I think dosing a boring technical tutorial with a bit of context is kind of fun.

[So that's what I made](http://www.redfour.io) for Elixir. I love that language and the vast, amazing OTP framework underneath it. Seriously once you start down OTP, "forever will it dominate your destiny". Erlang, Elixir - whatever - OTP is brilliant! I just can't help myself - I like to share this stuff.

## Didn't You Already Make That?

Sort of, but it needed to be updated and prettied up a bit. I love taking my time, refining things until they shine. I think customers feel that at an unconscious level - that you cared enough to sweat every pixel and font choice. The first Red:4 was fine - it looked ... OK but I always felt like I could have done more.

So I did - and I added a lot of content on the way..

![Red:4](/img/toc.jpg)

Yesterday I launched what I'd love to make more of: [an elegant tutorial site](http://www.redfour.io) that's looks as good as its content. This thing took me months to put together and, I'll admit, was one of the most frustrating experiences ever.

The first experience was fairly complete, but given some feedback and changes to Elixir, I decided to add:

 - Two chapters on debugging
 - A complete Github repository with bugs and refactoring that you need to fix
 - More OTP as well as troubleshooting OTP
 - Ecto and Mix Tasks

There are, of course, lots of fixes as proposed [by our intern pool](http://fleet.redfour.io).

## Getting It Right Takes Time

Once I realized that my time with Pluralsight was over, I began to freak out a bit. It's my only job! I had just received a royalty check (they send them out quarterly) so I was good for the next three months - that was a month and a half ago.

I don't think I ever intended to make a living off of this one site. The tutorial is on Elixir, which is kind of niche these days (but is also amazing seriously you should check it out), so the audience isn't all that huge. But I want to do other things just like this.

Little sites, one-off domains for a given topic like Node or Career stuff. I think people care about polished material.

## Available For Professional Video As Well

I've also started working with companies who want to do something a little different for their product videos - getting away from that god-awful ukulele/whistle music, making something relevant to the viewer. Currently I'm helping a company with their help and documentation tutorials - something I wish every company invested in.

So, if you need help that way let me know :). I have plenty of things to keep me busy, and I'm equally excited and terrified for what the next year will bring. *Stability, apparently, is not my thing.*]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/splash.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/splash.jpg" />
  </entry>
  <entry>
    <title>Writing A Book: The Imposter&apos;&apos;s Handbook</title>
    <link href="https://bigmachine.io/posts/imposters-handbook" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.730Z</updated>
    <id>https://bigmachine.io/posts/imposters-handbook</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/imposter_slide.png" alt="Writing A Book: The Imposter&apos;&apos;s Handbook" /></p>
## Update: The Book Is Available Now

I've setup a pre-release for the book [which you can buy now](http://bigmachine.io/products/the-imposters-handbook?utm_source=conery&utm_medium=blog&utm_campaign=announcement%20post). You'll receive all updates as they become available!

I was in the middle of a conversation with a friend the other week and we started debating some data access nonsense. This friend (whom shall remain anonymous) asked me a simple question:

> So you decided to go O(N^2) instead of just O(N+1) then?

I sort of laughed nervously, thinking that the definition would come. I swear it hurts to write this right now but... it's really what happened.

Yes, _I'm your nightmare_. The truth is I've known Big-O notation for quite some time, but the fact that I couldn't just call it up right then and there was ... quite frustrating.

I don't have a CS degree and the same can be said for a lot of my friends who write blog posts and get up in front of rooms of people to give talks on computer science stuff.

We could sit here all day and debate whether I'm truly qualified to do such things - but rather than do that I'll just throw in the towel and say **you're right**: _I really should know these things_.

My problem is that I a) don't have enought time and money to go back to school and b) I think I can dig in and figure a lot of stuff out using resources online (lectures, tutorials, etc) and books readily available for my Kindle/iPad.

**But that's no excuse**.

## The Inspiration

I don't know if this is nice of me, or "cool to do" in the online sense, but it's the truth so... whatever.

My inspiration for this book is an [anonymous comment](https://discourse.codinghorror.com/t/your-favorite-np-complete-cheat/256/10) left on [Jeff Atwood's blog](https://blog.codinghorror.com/your-favorite-np-complete-cheat/):

<blockquote>
<p>
For me, spreading ignorance (or bad information due to ignorance) is an issue.  If you are gonna talk about subject X, make sure you know subject X, well enough to talk about it. At least, make sure that you are not making gross errors about subject X. Is it really too much to ask?
</p>
<p>
I've been working as a professional programmer for years, and I've encountered many without basic scientific background. And that's FINE! Not everyone needs it, or wants it. But then these people read a blog post, and it sounds right, so they believe it. After all, they lack the knowledge to figure out which parts are true and which are don't. That's why they are reading the blog!
And they learn stuff that is WRONG. And then they are going to implement this stuff, and argue about it, and generally BELIEVE that anything having to do with CS is either unpractical or is easily enough learned in 30 minutes of reading.
</p>
<p>
And then I have to work with these people, and manage them! They have no grasp of how much they simply don't KNOW. And at some point, their knowledge simply ain't gonna cut it. And they are going to argue with me, or write me off as a user of fancy computer science jargon. I mean, it's just register allocation, right? How hard could it be? It's only BSP tree optimization, let's just check all the options!
</p>
<p>
So I am trying to combat this ignorance for practical, selfish reasons. Programmers need to understand the problems they work on. They need to understand when they are out of their depths, and its time to hit the books, or call someone who knows. Or reject that project, or bump up the cost and time estimate. At least map out the areas of your understanding, so you'll know when you're on treacherous ground.
</p>
<p>
The other reason is a matter of principle. Ignorance is pretty bad, and I reject mediocrity for its own sake.
</p>
<p>
In less fancy terms, if you are writing technical posts, get the technical details RIGHT!
</p>
</blockquote>

M1116: _you're absolutely right_. The main issue I have is that **I don't know what I don't know**. I set out to change that 6 months ago, and it's what I hope to share with you in a few months from now.

## The Imposter's Handbook

I'm 80% of the way through this effort, and it's quite literally changing my life. Every single chapter I write has completely altered my outlook on things and, most importantly: **made me see all too clearly just what I do not know**.

Here's a list of the chapters I have so far (in no particular order):

- **Computational** Theory: Finite State, Pushdown, and Turing Machines as well as Geek Trading cards and P/NP "stuff"
- **Algorithms**: bubble sort, heap sort, quick and merge sort, selection sort and binary search.
- **Big-O Notation**: what each common asymptotic complexity means and why you should care.
- **Linux Essentials**: how to get around and do basic things
- **Shell Scripting**: How you can accomplish various tasks using the command line, including tossing Grunt/Gulp and using Make
- **Software Design**: SOLID, Gang of Four patterns, etc
- **Programming Language Essentials**: Compilers, memory usage, garbage collection, runtimes, TIOBE analysis, a quick overview of what's popular vs. what pays
- **Data Structures**: Arrays, linked lists, heaps, hash tables and binary search trees... what they are and why you should care
- **Encryption**: ciphers, encryption algorithms and basic hashing including the security/insecurity of each approach

I might add some things or prune back but as I mention: I'm 80% there.

![](https://bigmachine.io/img/imp_1.png)

## Whatever Why Am I Telling You?

I thought if you had any ideas or requests you could let me know (do it on Twitter: http://twitter.com/robconery). These are things that I need to know more about, and writing/presenting things like this forces me to research them.

Which I think is critical: _I don't take this lightly_. One sure fire way that I'll learn this stuff is to have my ass on the line - and it really, really is with this effort.

![](https://blog.bigmachine.io/img/imp_2.png)

## Is This Supposed To Replace a Degree?

HELL. NO. My goal with this book is to show non-CS grads just how much they don't know. As I mention: the very act of writing this book has completely 100% no shit changed everything about how I approach programming. **Absolutely everything**.

I hope to fire up the non-CS grads out ther to go investigate subjects that interest them. To seek out the easily-accessible, **free** online courses from Harvard, Stanford, and other high-caliber schools. All links will be included in the book.

![](https://blog.bigmachine.io/img/imp_3.png)

## You Really Should Have a Chapter On X

Yeah, I probably should. [Do me a favor and let me know](http://twitter.com/robconery) if you have a request. If I have time, I'll add it in there.

Hopefully I'll have this thing ready to go in a month or so. It's taking me forever and it's mostly research - which is too damn fun.

## Want To Know More?

You can [sign up for the mailing list here](https://www.getdrip.com/forms/96263307/submissions/new)]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/imposter_slide.png" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/imposter_slide.png" />
  </entry>
  <entry>
    <title>Writing A Book Is Frustratingly Addictive</title>
    <link href="https://bigmachine.io/posts/writing-a-book-is-frustratingly-addictive" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.730Z</updated>
    <id>https://bigmachine.io/posts/writing-a-book-is-frustratingly-addictive</id>
    <summary type="text">I want to love this process... I ... want to ...</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/goldizalgo.jpg" alt="Writing A Book Is Frustratingly Addictive" /></p>
I really enjoyed writing [The Imposter's Handbook](https://bigmachine.io/products/the-imposters-handbook/), as well as making the videos. I can say that *now* because it's done with and the human mind has an amazing ability to cull things negative.

So, naturally, I'm doing it again.

I have a fun idea for my next little project. Once again it will be a book/video thing, but this time I'm going to let my creative side run free. It's the only way I can deal with the subject matter: *Node and ES6*.

Yes, I know. There a quite a few books out there, but none like what I have in mind. I'll write a bit more about it at another time (when the idea is a bit more shaped), for now I can tell you that it involves my fictional aerospace company (Red:4), the August 2017 eclipse, lasers and the moon.

I have a newsletter signup thingy at the bottom of the post if you want to sign up. I promise I won't spam you :).

A number of people have asked if there will be a "volume 2" to the Imposter's Handbook and yes, indeed, I am actively researching it. The first one took me 18 months to "finish", this one will likely be the same. There are a few things I want to do in the interim however...

## Goldilocks And The Three EBook Formats

Here's the thing: **formatting an ebook is like trying to give five cats a bath** (that's seriously the closest analogy I can come up with). Trying to have parity between formats is a pointless, wet, cold, and scratchy exercise... and no one is ever happy. Especially your cats.

There are three main formats you have to provide if you want to make *anyone* happy:

 - **epub**. An open standard used by quite a few ereaders. It's based on HTML, CSS and some XML and, just like the web technologies its built on, there are multiple "standards" you can build to (versions 2 or 3 are significantly different). You can create a book with pixel-perfect placement, or you can do a traditional "reflowable" book that is basically easier to read.
 - **PDF**. You know what PDF is. Read it anywhere on pretty much anything. You lose some of the bells and whistles that an ereader provides, but you can do fun things like "stamping" which imprints the name of the buyer on the book. It's the most usable and versatile format, which is likely why no one wants to use it for books.
 - **mobi**. This is the proprietary Kindle format and *it makes me want to scream every time I have to deal with it*. There are quite a few different Kindle formats out there, so when you build your ebook file for the Kindle (using Kindlegen from Amazon) it will *create a version for every Kindle out there*. The mobi size for The Imposter's Handbook was 1.5G. *Exciting times*.

I have spent months and months trying to figure this out. I've used all kinds of writing and conversion tools (which I'll describe below), but the closest I've come to parity between formats is...

 1. Create the best epub book you can, formatted as you like.
 1. Use [Calibre](https://calibre-ebook.com) to convert from epub to PDF/mobi. Give it the time it needs as you'll be twiddling all kinds of settings to get your margins *just so*.
 1. Set your mobi output to the Kindle Paperwhite (in Calibre). This (for me) covered most of my Kindle customers well enough, and the ones who had a different reader (the Fire, for instance) could use the PDF
 1. Use images for code samples.

That last point there? Number 4? That's a big deal. You see - no matter what you do, the fonts you choose will be obliterated by the ereader. These things want to show books the way *they* want to show them and your formatting be damned.

If you really care about the way the code reads, snap a screenshot. The downside is the book gets rather large... 

I'm getting ahead of myself. Let's talk about...

## The Actual Writing Process

I've been through every editor and writing tool you can think of. Please trust me on this, as I know you're gearing up to "have you tried X" and yes, I swear to you I have. Yes, them too. YES THAT ONE TOO.

It's quite sad how many editors I've tried... and given up on. Let's go through a small subset of my choices - the rest you can assume *simply sucked* for writing a book. It's hard! Books need a bit more than just regular old long-form writing tools. Yes you can *get by* with Word, but seriously it's a major pain in the ass to structure that many words and keep track of everything properly.

Anyway, here's what I've been using.

### iBooks Author

It's what I finally settled on and what I'm using for the next book. Overall it's giving me what I want: a great looking book, a nice writing experience and a reasonable structure. 

### Pros
The first is easy! **It's free**, but you have to have a Mac. You can create two kinds of epubs: standard reflowable and pixel-perfect "picture book" style. If you choose the reflowable option you can export your work in epub format, if you choose the pixel-perfect style you get PDF or `.ibooks` format.

Styling, layout, etc work like most editors you've probably used and the built in templates make it incredibly easy to get off the ground quickly.

If you have code examples, you can copy and "Paste and Retain Style" easily. With some small formatting tweaks you can have your code look *almost exact*, which is great.

It comes with a built in glossary, so if you need a quick reference for words and terms you got it. It also has a number of "widgets", like picture galleries and popups - you can even embed HTML! This is neat, but only people with iPads and sophisticated epub readers will be able to take advantage.

#### Cons
The format you write in is dictated by the export you need. If you choose the regular epub format, that's all you can export to, there is no PDF export for some stupid reason. If you choose to have a pretty, well-formatted epub 3 book, you can export to PDF but **without any TOC or navigatable outline**. This makes the PDF basically unusable.

The whole epub 3 thing is kind of a joke as other readers can read these epub 3 books just fine, but you can't export to them and Calibre won't convert them the way you want.

Fonts. There easy to use and set up, and you will be tempted to use all kinds of funky ones that will get blown away in the conversion process, or just scrubbed over completely when the reader loads your file. This is especially true for code! The best way around this is to use screenshots, but that inflates the size of your book dramatically and can also change the way your book reads. They also look crappy in "night mode".
 
### Scrivener
I love this tool, it's the best one I've found for writing. I've gotten to know it very, very well and messed with the compiler settings well into the night on multiple occasions. 

#### Pros
There is no tool out there that helps you organize your thoughts, do research and sketch scenes/chapters like Scrivener. It is quite simply the best.

Tons of helpful utilities, like a scratchpad, research folder, breaking big pages into little sections.

Easy to use to get up to speed.

Will export to PDF, mobi, reflowable epub, manuscript (for paper books) and a few other formats.

#### Cons
Formatting. My GOD this is so FRUSTRATING! Scrivener splits the writing process into two buckets: text and formatting. Which you would *think* would work well, but it doesn't because you get the choice to format your text during the writing process *and* during the compilation process, when all of your lovely text is assembled into whatever format you want to use.

Twiddling. So. Much. Twiddling! All kinds of settings and tweaking this, nudging that to get the layout to look *reasonable*. Not stellar, mind you, just *reasonable*. You can setup the compiler for each different format you need: epub, PDF or mobi. I put this as a "con" because you need to spend time with each setting to get it to show the way you want. **And it's never right**.

Styles. There are style presets you can use, "blockquote" for example. If you set some block quotes and then continue writing, but on page 200 decide "you know these look horrible let's change them", changing the style setting does *nothing*. You have to go back and tweak each one by hand. I have no idea why this decision was made.

Overall frustration. *I want to love this writing tool*. I've used it for years, but mostly for smaller things so formatting and other things didn't really bother me. It's so well done and the utilities you're given are incredibly well thought-out... but the rest of it makes me want to scream (and I have, on many occasions).

### Adobe InDesign
I gave this thing three solid days on two different occasions. Each time I kept thinking "wow this is amazingly great/horrible".

#### Pros
You can export to any format, reliably, and it looks great. There are tons of resources out there that show you how to create some interesting books, too. epub 2, 3, mobi, PDF - it's easy to use.

Drawing tools built in! That's pretty neat, especially if you're doing an epub 3 book and want things to look nice. [Take a look at this book](http://sunnibrown.com/doodlerevolution/) and you'll see what I mean. Sunni Brown (the author) used InDesign for it and was able to weave some interesting fonts together with her drawings and generated line sets - it works *really well*.

It's affordable. If you have an Adobe license you can add it in there (or maybe you already have it); if not it's only $24/month.

#### Cons
It's Adobe. You have to install their crappy crapware tool and deal with their emails. If you have Adobe stuff already this isn't going to ruin your day... I don't and I didn't enjoy the experience.

It's Adobe. Buttons, knobs, settings buried in settings within boxes on top of panels within other boxes and panels. The icons were indecipherable to me, but if you're an Adobe person already maybe you'll get it. Seriously: *you can't use this thing without taking a class first*. Not hyperbolic here, [it's the main reason I keep a Lynda sub](https://www.lynda.com/InDesign-training-tutorials/233-0.html) - learning tools like this one.

It's Adobe. Usability is out the window in terms of the act of writing. That's not what you do with InDesign, you *design* a book. Which is fine! Unless you're writing a book.

### Leanpub, Softcover, Gitbook, Static Sites (like Middleman) and Every Other Service You're Going To Suggest...
I used both Gitbook and Softcover for The Imposter's Handbook and they worked OK. For the most part it was nice to work in a text editor with Markdown.

#### Pros
Markdown is very simple to use, and Softcover and Leanpub have their own flavors of it so you can do some extra things. Softcover, for example, allows you to use LaTeX for math equations right in the markdown, which I did a lot.

Structure. It's nice the way these tools apply structure to your writing. Figures have numbers as do your chapters. 

Additional Services. If you want to publish and sell your book through them, these services will let you! This is a nice addon if you're doing this as a side thing. Leanpub is especially good at this.

Formatting. I used Softcover for the latest versions of The Imposter's Handbook and tweaking the CSS was pretty easy to do. I was also able to add my own formatting blocks. It wasn't easy, but it was doable.

#### Cons
Yak Shaving. Setting up Softcover involves Ruby, the Softcover gem and some additional programs. Softcover helps you with this by checking to see if you have what's required:

```
$ softcover check
Checking Softcover dependencies...
Checking for LaTeX...         Found
Checking for ImageMagick...   Found
Checking for Node.js...       Found
Checking for PhantomJS...     Found
Checking for Inkscape...      Found
Checking for Calibre...       Found
Checking for KindleGen...     Found
Checking for Java...          Found
Checking for EpubCheck...     Found
All dependencies satisfied.
```

These aren't small additions to your system. LaTeX is gigantic and there's a bug in the latest release of Inkscape which doesn't work with Softcover... and you only discover that when your math equations aren't showing up in PDF format.

Buggy. All of these tools work *for the most part*, until they don't. It could be for a number of reasons, but when you're using node and ruby to "build" a book, there *will be problems* because you're not using a unified toolset, rather a collection of smaller tools that will have dependencies that don't work right. I've come to a bit of a detente with Softcover: *it works, so I don't touch it*. This is after spending quite a few nights trying to figure out why equations weren't showing up at all in certain formats of the book. [The PDF version still has problems](https://github.com/imposters-handbook/feedback/issues/207) and to fix it, I need to debug Softcover. This is a pain in the ass.

Formatting. These toolsets were built by developers to create technical books, which is fine. If this is all you're creating then they might work fine for you. I like to make things look pretty and it's kind of hard. I got close with Middleman and the [Elixir book I wrote](https://bigmachine.io/products/take-off-with-elixir/), but just like Softcover I've had to deal with formatting issues.

### Ulysses, Bear, Ai Writer, And Every Other Writing App You'll Suggest
These things look GREAT, but once again fall completely short when it comes to structuring and formatting your book the way you want. I keep coming back to this: *these tools are great for certain things*, but not everything I need.

Yes, Goldilocks. That's me! And I'm OK with that. I want things to look and read a very certain way because technical books can be disastrously boring slogs; I want to create something more. 

For example: Ulysses allows you to embed code using their weird code block markdown syntax... but there's no highlighting. Which means it's pointless. Same with Bear and Ai. You also can't "Paste and Retain Style" because you're using markdown... frustration!

## Onward! Let's Write This Thing...

So that was the last 2 years of my life :). Figuring out how to write the book I want, realizing I probably want just a bit too much. 

For now, I'll stick with iBooks Author and hope for the best. I'll share what I'm doing as I go, and I'll send out email updates if you're interested. You can sign up here, if you want, and I promise I won't spam you:

<form class="ui form" action="https://www.getdrip.com/forms/17123094/submissions" method="post" data-drip-embedded-form="17123094">
  <h3 data-drip-attribute="headline">Node/ES6 Newsletter</h3>
  <div class="ui action input">
    <input type="text" name="fields[email]" value="" placeholder="Email address">
    <button class="ui teal right labeled icon button">
      <i class="envelope icon"></i>
      Sign Me Up!
    </button>
  </div>
</form>]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/goldizalgo.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/goldizalgo.jpg" />
  </entry>
  <entry>
    <title>Kicking The Tires On This Serverless Thing</title>
    <link href="https://bigmachine.io/posts/serverless-is-a-thing" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.730Z</updated>
    <id>https://bigmachine.io/posts/serverless-is-a-thing</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/firebase/cover_1.jpg" alt="Kicking The Tires On This Serverless Thing" /></p>
I just [released a video series](https://goo.gl/yCliXG) about building a serverless application with Firebase, and I thought I would write it up in a blog series as well. I think it's worth reading and understanding what I went through. This is Part 1 of that series.

## The Punchline

Before doing this I spent a few months with AWS Lambda (which you'll read about below); first with [The Serverless Framework](https://serverless.com/) (which I quite like - so jealous they got that domain!) and then with [ClaudiaJS](https://claudiajs.com/) which didn't quite fit what I wanted. Finally I rolled my own thing with some shell scripts and a zip file.

I ended up with a mess that cost more money than I wanted it to. It wasn't easy to figure out and more than once I had to rage-swim at the Y to get over the stress of the day.

I had better luck with Firebase. **I had a fun time and built something interesting**. At least *I* think it is. I had to approach the development process in a completely different way than what I was used to... but I like that kind of thing. I know others don't. The big thing to me, however, is that I was able to build something *that I would truly use*. In fact I'm using parts of it now in production (more on that in a future post).

This is a long story and will cover many posts. Not sure how many but I'll try to keep it focused. As you can clearly tell I like Firebase, a lot, and yes I encountered quite a few issues along the way but they were surmountable. I'll get into all that, I promise.

For now let's start at the beginning, when I first dug into the "serverless" thing.

## AWS, Firebase, Webtask.io...?

Let's start with [Webtask](https://webtask.io). I know a number of people over at Auth0 (who own and run Webtask.io) and I have a ton of respect for them. Everything you're about to read has been said to them in person, so don't think I'm being unfair - I think they would agree with me straight away.

If you [head over to the Webtask](https://webtask.io) site you'll see a simple page with a headline:

![](https://bigmachine.io/img/firebase/webtask.jpg)

It's a puzzling headline with a rather sparse lede. If you click on anything on this page (the green button or "learn more") you're asked to log in. Once you log in you're taken to this page:

![](https://blog.bigmachine.io/img/firebase/webtask_2.jpg)

It's gorgeous, as you can see, but it just adds to the confusion. What "more" am I supposed to learn here? How much does this cost? What language do I use? What... is happening?

I've built a few tasks with this tool and they worked great, but from there... I have no idea. If you [read their pricing](https://webtask.io/pricing) you'll become more confused, most likely. [The documentation](https://webtask.io/docs/101) looks promising but is, once again, more than a bit sparse. 

**Verdict**: Webtask is a neat idea but doesn't seem to be the thing I need. In fact I have no idea what it's supposed to be.

### AWS

Amazon introduced [AWS Lambda](https://webtask.io/docs/101) over a year ago and it made a big splash. The pitch was simple to understand:

>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume - there is no charge when your code is not running. With Lambda, you can run code for virtually any type of application or backend service - all with zero administration. Just upload your code and Lambda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically trigger from other AWS services or call it directly from any web or mobile app.

If you've worked with AWS before, you know that *nothing AWS is ever simple*. I went into this with a good dose of skepticism!

#### Step 1: Pick Your Function

After signing into the AWS console and clicking on "Lambda", you're sent to a splash screen with a button that says "Get Started". You're then taken here:

![](https://blog.bigmachine.io/img/firebase/aws_1.jpg)

Welcome to AWS. It might just be me, but I feel like working with AWS is like wandering around the Mall of America, hoping to find *that one store* that sells *that one thing* you're trying to find.

![](http://visitshakopee.org/img/2016/02/Mall-of-America-Attractions-Banner.jpg)

*Note: for foreign readers, the Mall of America is a gigantic structure in Minnesota that embodies everything that is not wonderful about the US*.

**AWS is conceptually deafening**. I am literally exhausted after working with it for a few hours and often I roll out of my chair onto the floor, weeping, as I can't remember what I just did nor how much it will cost me.

OK maybe a little hyperbolic. There's just a lot of moving parts is what I'm trying to say. So, where was I ...

#### Step 2: Pick Your Trigger

Right: after you pick you function (good luck with that) you're given this screen:

![](https://blog.bigmachine.io/img/firebase/aws_2.jpg)

Your function is executed based on some type of event. If you know AWS and you know what these services are, you're in luck! If not, say goodbye to the next few hours.

#### Step 3: Get Lost In The AWS Wilderness

We want to choose "API Gateway" for this so we can call our function from the outside world over HTTPS. And then...

![](https://blog.bigmachine.io/img/firebase/aws_3.jpg)

This is where I'm going to hit the fast-forward button. I remember swearing a lot and feeling lost at this point (not hyperbole). I mean: *I know what these things are*, but I don't know the implications fully. We're talking about security here, and staging environments! These aren't to be taken lightly.

I tried to figure it out myself for 2 solid days, and then I just gave up and went with a framework.

### Serverless Framework

The [Serverless Framework](https://serverless.com) is a Node project that automates most of the pain when dealing with AWS. The video on the site is reasonably informative but... clicking on the documentation link (from the home page) gives you a 404, which I think is funny. If you use the menu on the top you'll be OK.

![](https://blog.bigmachine.io/img/firebase/serverless_1.jpg)

So, in short, you use their CLI to generate a YML file and a Node file for your function code. You can use Python or Java with Lambda, but for now we'll stick with Node.

As with most HelloWorld demos, the initial steps look simple. I'm not interested in that, I want to see what complex looks like. Here ya go:

```yaml 
service: bigmachine-checkout

provider:
  name: aws
  runtime: nodejs4.3
  region: us-west-2
  stage: dev
  vpc:
    securityGroupIds:
      - "sg-derptyderp"
    subnetIds:
      - "subnet-herptyherp"
  iamRoleStatements: # permissions for all of your functions can be set here
    - Effect: Allow
      Action:
        - cloudwatch:DescribeAlarms
        - cloudwatch:GetMetricStatistics
        - ec2:DescribeAccountAttributes
        - ec2:DescribeAvailabilityZones
        - ec2:DescribeSecurityGroups
        - ec2:DescribeSubnets
        - ec2:DescribeVpcs
        - ec2:CreateNetworkInterface
        - ec2:DescribeNetworkInterfaces
        - ec2:DetachNetworkInterface
        - ec2:DeleteNetworkInterface
        - sns:ListSubscriptions
        - sns:ListTopics
        - sns:Publish
        - logs:DescribeLogStreams
        - logs:GetLogEvents
      Resource: "*"

functions:
  paypalBraintreeFinalize:
     handler: handler.paypalExpressFinalize
     events:
      - http:
          path: paypal/finalize
          method: post
          cors: true

  paypalBraintreeToken:
     handler: handler.paypalExpressToken
     events:
      - http:
          path: paypal/token
          method: get
          cors: true

  stripeCheckout:
    handler: handler.stripeCharge
    events:
     - http:
         path: charge/stripe
         method: post
         cors: true
```

All in all, *not that bad*. You can read through this and probably figure out that I have functions to handle a Stripe charge and some PayPal stuff. They're exposed via the API Gateway (which the framework does for you) and I'm able to wire up CORS easily.

Unfortunately I also want to **talk to a database that's not DynamoDB**. I like using PostgreSQL with [Compose.com](http://compose.com) which means that if I want my Lambda functions to talk to the outside world I need to set up a gateway, a VPC and all the lovely machinery that goes along with it.

The super silly thing is that I need to do this even if I use Amazon's RDS stuff - you have to have a VPC setup for security reasons. That aint cheap.

![](https://blog.bigmachine.io/img/firebase/aws_4.jpg)

VPCs and gateways are not cheap but, in the grand scheme of things, $50/month isn't so bad either. But if I'm going to pay that... why don't I just use Heroku?

#### Back Where I Started

It took me about 10 days to get things running properly. 2 of those days were spent being very, very frustrated trying to figure out all of the moving pieces and dealing with errors like this:

![](https://blog.bigmachine.io/img/firebase/aws_happiness.png)

This was the final capper for me:

```
servless --help

WARNING: You are running v1.7.0. v1.8.0 will include the following breaking changes:
  ...
```

This doesn't make me feel excited about the team behind this thing. Breaking things on a point release goes against the whole idea of semantic versioning. It wouldn't matter but the framework uses Node/NPM which by general agreement follows semver, so it seems a bit silly to ignore it.

A [bug report was filed](https://github.com/serverless/serverless/issues/3252) and completely ignored, aside from this email response:

>Breaking Changes - We're no longer following strict semver. You can always find the most recent list of breaking changes in the upcoming milestone or in the Serverless CLI.

So there's that then. Would you base a business on this framework? I decided it wasn't for me.

#### Rolling My Own

Once you understand the machinery that goes into AWS Lambda, rolling your own isn't too difficult. I took a weekend to dive in and figure out the bits that confused me prior, and eventually I had a few shell scripts rolled together that orchestrated most of what I needed.

If you're comfortable with scripting, know AWS and can tolerate some head-pounding... AWS Lambda isn't all that bad. A bit more expensive then I'd like but... not that bad.

**Verdict**: Every single time I've used AWS I've had to ditch a weekend or 2 remembering how things worked. It's a powerful system, but in many ways is overkill when compared to something like Heroku (or the services like Heroku). The problem is that Heroku can get expensive, fast.

Then there's what I've been doing for the last 5 years or so that's working great: [DigitalOcean](https://www.digitalocean.com/). It has been my go-to for so long; it takes a lot to justify moving away from them. I have my build scripts down cold and I can whip up a server during lunch, complete with SSL and multiple, managed Node processes. I have the same for Elixir too.

Honestly: serverless with AWS isn't buying me anything. I don't want to use DynamoDB (though I'm sure it's wonderful), I can use a message queue if I want to do things in a "small function" kind of way, and honestly it just takes too much space in my brain.

All of that said, yes I know that new frameworks are popping up daily, so maybe things will change a bit. For now, no AWS Lambda for me.

## And Then: Firebase

I remember reading [this post](https://firebase.googleblog.com/2017/03/introducing-cloud-functions-for-firebase.html) with a groan, thinking "not again"...

>Today we are excited to announce the beta launch of Cloud Functions for Firebase. It lets you write small pieces of JavaScript, deploy them to Google's Cloud infrastructure, and execute them in response to events from throughout the Firebase ecosystem.

I'll admit to a heavy amount of Magpie-ishness. I can't help it... it's just me and I've learned to let me be me... as opposed to you or anyone else.

The [first impression](https://firebase.google.com) I had when looking over the "new" Firebase site was one of immense relief. The page is elegant, focused, easy on the eyes and easy to understand. The console is wonderfully simple, too:

![](https://blog.bigmachine.io/img/firebase/firebase_1.jpg)

Clicking through each of the services made sense to me. But what about the functions? How simple would it be to get code up here? What about hosting? Can I SSL this stuff? Outbound networking? Can I use PostgreSQL instead of Firebase or, maybe, together with it?

I found out the answers to all of these questions was, in short: simple, simple, yes, yes and yes. After working with AWS, Firebase was a very welcome change.

I'll get to all of that in the next post.

---

## [See this series as a video](https://goo.gl/yCliXG)

Watch how I built a serverless ecommerce site using Firebase. Over 3 hours of tightly-edited video, getting into the weeds with Firebase. We'll use the realtime database, storage, auth, and yes, functions. I'll also integrate Drip for user management. I detest foo/bar/hello-world demos; I want to see what's really possible. That's what this video is.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/firebase/cover_1.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/firebase/cover_1.jpg" />
  </entry>
  <entry>
    <title>Should I Trust Firebase? Of Course Not!</title>
    <link href="https://bigmachine.io/posts/pants-on-firebase" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.730Z</updated>
    <id>https://bigmachine.io/posts/pants-on-firebase</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/firebase/pants_fire.jpg" alt="Should I Trust Firebase? Of Course Not!" /></p>
In the last post I discussed my initial foray into the serverless thing, and why going with other platforms (AWS Lambda, Webtask.io, etc) didn't make sense at the time.

I've received a number of comments since then, specifically about [this post](https://startupsventurecapital.com/firebase-costs-increased-by-7-000-81dc0a27271d) which details how one company had their costs jacked up by Firebase 7000%:

>Due to a change in how they report data usage, our monthly costs for Firebase, a SaaS provided by Google, has increased from $25 a month to what is now moving towards the $2,000 mark — with no changes to our actual data use. This change was made without warning.

The question was a simple one: *how can I possibly trust my business to a company that would do this sort of thing?* My answer, in most cases is simple: **you shouldn't**. Not now, not ever.

Let's dive into this.

## A Quick Summary Of This (and Every Other) Service Dissapointment

There are a number of problems discussed in the post referenced above, but they basically distill thus:

 - The startup (HomeAutomation) was doing IoT stuff and had a **query going off once a minute from all of their distributed devices**. The query was small (a flag check) and when the company checked their bandwidth usage they were within the limits of their $25/month plan which let them push 20 gigs of data per month (which is a hell of a lot).
 - **Firebase realized they had a bug in their bandwidth measurements**: they weren't accounting for the SSL payload. They fixed this in March/April of 2017. This fix caused HomeAutomation's bill to go up by 7000%.
 - **HomeAutomation could not fix this problem** without going offline and changing their app entirely. Given that it's IoT, this basically means going out of business.
 - **Firebase was not responsive** and it took a blog post to push them to (rightly) credit costs that were not foreseeable nor accountable. They have corrected it, but the customer service hit here is really, really bad.

No one looks good in this. Yes, Firebase did something dumb. I've been trying to find the right words to express my thoughts correctly, and I think I've hit on one:

> Duh

Firebase (and therefore Google) are Big Companies. They're in this to make money and if you're getting by on $25/month for a large distributed IoT system then yes, *something is probably wrong*.

I know that it sounds like I'm about to blame the developer for trying to plan for something that they had no actual metric to plan for it with. This is true, Firebase wasn't telling them accurate information, but as they say in Hawaii...

## Akamai!

I don't trust many people. I don't trust *any* businesses either. One company I used promised "unlimited bandwidth" until I soaked up 4Tb in a single month and they tried to charge me, saying "it was unreasonable for you to do that". Sure, whatever, lates.

I'm ready to jump ship with any service/platform on a moment's notice. I'm almost *too ready* as a matter of fact and routinely need to get pulled off the ceiling by peers. I'm SaaS trigger happy I guess. 

I could fill this entire post with the different companies that have let me down over the last 2 years, doing shady things and trying to overcharge me. Other's have [simply disappeared](https://twitter.com/userapp_io/status/770596364937932800) without telling people. While fun (for me), I don't want to derail this post too much.

Let me just summarize this way: *if something seems just too good to be true, expect a nasty surprise*. 

![Surprise!](/img/firebase/joker.jpg)


Paying $25/month pushing 10s of gigs of data over the wire fits that description.

## Coding For The Future

Pardon me while I climb the stairs to my Ivory Tower... ahh the view is nice from here! I can see the past in perfect 20/20 clarity...

When you write software applications it's usually a good idea to have a look at what other people have done before you. This situation could have been mitigated/avoided by embracing a simple sentiment that my friend [Rob Sullivan](http://datachomp.com) stated quite elegantly once in a talk he gave at NDC Oslo:

>Change hurts when you write bad code, doesn't it?

Developers in the previous decades have learned the hard way that hitching your wagon to *any* particular dependency, and betting your entire business on it *will always be a bad thing*. This goes for services, frameworks, and even the people themselves.

HomeAutomation seems to have just found this out, as this was one of their summary points:

> Always build your architecture in a way that will avoid becoming trapped into a specific service. Build your application in a way that swapping one service for another is as simple as possible.

**You have come correct** sir. Writing apps is indeed hard and takes some forethought.

## So, Should You Trust Firebase?

No. **Don't "trust" anything**. As a developer you're not allowed to "trust" any language, service, platform or other coder for that matter. Most of all: **you're not even allowed to "trust" yourself**. In short *trust has nothing to do with creating software*.

Firebase is a service that you can use with a reasonable amount of confidence. It can help you get your app out quickly and, hopefully, if you need to change something later on then you should be able to do that!

I know, I know: Ivory Tower Dev wagging Ivory Finger. But *hoooonnnneeessstlllyyyyyyyyyy* do we have to keep learning this lesson again and again?

At the very least - if your app is pushing 10s of gigs of data per day, don't you think it's reasonable to ask the question:

>"Are you sure we only have to pay $25/month"

This would scare me. It should scare you too! Here's the really scary part: *even if they built their app to handle a complete service change, they would have been in serious trouble anyway*.

Let's say they made it easy to move their system to AWS, for instance. They're in business to make money too, as it turns out, and if you're pushing 10's of gigs of data *per day* (or in HomeAutomation's case, 100s of gigs of data though they didn't know it at the time) you will have to pay for that. $2000/month is not unheard of for this.

I am going to be extremely curious what happens to this company in the next few months. No excuses for Firebase here either... this was a complete mess on all sides.

Which means you and I have to build defensive systems with interchangeable parts... because **change sucks** no matter what you do.

I've been using Firebase for 3 years total now, for very small things compared to HomeAutomation. I like what I've seen and so far I'm happy. This doesn't mean you will be, nor does it mean that I will be in a year or so.

I'm expecting that. You should be too.

---

## [See this series as a video](https://goo.gl/yCliXG)

Watch how I built a serverless ecommerce site using Firebase. Over 3 hours of tightly-edited video, getting into the weeds with Firebase. We'll use the realtime database, storage, auth, and yes, functions. I'll also integrate Drip for user management. I detest foo/bar/hello-world demos; I want to see what's really possible. That's what this video is.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/firebase/pants_fire.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/firebase/pants_fire.jpg" />
  </entry>
  <entry>
    <title>Using Recursion In Elixir To Break Your OO Brain</title>
    <link href="https://bigmachine.io/posts/using-recursion-in-elixir-to-break-your-oo-brain" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.729Z</updated>
    <id>https://bigmachine.io/posts/using-recursion-in-elixir-to-break-your-oo-brain</id>
    <summary type="text">I have to start out each post this way: I have no idea what I&apos;m doing, but dammit am I having fun. In the fist few posts I ham-handedly threw some code against the wall to see what would ... stick? Anyway It worked, but I realized (as I did with Ruby, wonderfully) that there just</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2015/09/recursion_title.jpg" alt="Using Recursion In Elixir To Break Your OO Brain" /></p>
I have to start out each post this way: *I have no idea what I'm doing*, but dammit am I having fun. In the fist few posts I ham-handedly threw some code against the wall to see what would ... stick? Anyway It worked, but I realized (as I did with Ruby, wonderfully) that *there just has to be a better way*.

I don't want to diminish that observation **because it's what I love about Ruby**. I always felt like I could level-up my Elegance in Ruby if I just took the time (and patience) to see what was possible.

With elixir it's the same.

## Refactoring, Loops, Recursion

If you read about loops in elixir or [Google Elixir for loop](https://www.google.com/search?client=safari&rls=en&q=Elixir+for+loop&ie=UTF-8&oe=UTF-8) the first hit you'll see is "Elixir Recursion". Which scared me.

You see, I suck as a programmer. As I mention - I really am a hack. **The fact that I understood recursion and can use it** is pretty damn outstanding incredible (on Elixir's end, not mine). OK enough blathering, let's see some code.

**The Problem**: I have this intensely fugly routine where I query a database and send the results back when a user registers:

```elixir
def register({email, password}) do
  {:ok, pid} = Membership.connect()
  sql = "select * from membership.register($1, $2);"

  case Postgrex.Connection.query(pid, sql, [email, password]) do
    {:ok, res} ->
      cols = res.columns
      [first_row | _] = res.rows
      [new_id, validation_token, auth_token, success, message] = first_row
      {:ok, %RegistrationResult{
        success: success,
        message: message,
        new_id: new_id,
        authentication_token: auth_token,
        validation_token: validation_token
    }}

    {:error, err} -> {:error, err}
  end
end
```

It's a start, but this function does way, way too much:

 - Queries a database
 - Transforms the results
 - Creates a struct and returns the results

Honestly I can live with this. But, like working with Ruby, I know there's a better way and **I know when I find that better way, I'll be better at Elixir**. So let's see what can happen.

## The Solution

I need to split this stuff out and I need a better way to cast the result properly. I hate the way this is all going together - so let's use some recursion and split things out into a specific module built to handle database results:

```elixir
defmodule Membership.DBResult do

  def map_single({:ok, res}) do
    cols = res.columns
    [first_row | _] = res.rows
    map_single {:cols_and_first, cols, first_row}
  end

  def map_single({:cols_and_first, cols, first_row}) do
    zipped = List.zip([cols,first_row])
    map_single {:zipped, zipped}
  end

  def map_single({:zipped, list}) do
    {:ok, Enum.into(list, %{})}
  end

  def map_single({:error, err}) do
    {:error, err}
  end

end
```

I'm sure this code still sucks, but I love how it's split out here. Notice that each function has the same name but has a different parameter signature? **Pattern Matching**, people. This is too fun.

So, basically an outside caller will simply do this:

```elixir
Membership.DBResult.map_single(query_result)
```

And Elixir will figure out how to match for you. From what I've read, Atoms do this for you and it's one of the idioms Elixir people use just for this reason. Each one of these functions has a different signature, and each one does a single thing. The first matches the `{:ok, ...}` tuple, which is the result from the query.

That function then calls itself, but with a different Atom at first position. That matches against the second function... and hopefully you can see the pattern here. Basically, what I'm trying to do is handle/transform the query result in one place. I'm sure there's probably a better way - but this takes a convoluted query result and transforms it really nicely.

## Pass The Pipe

There are two ways to use this new module. I can use it directly:

```elixir
Membership.DBResult.map_single(query_result)
```

Or do something just a bit more elegant by *piping* the result data through a pipeline:

```elixir
def new_application({email, password}) do
  {:ok, pid} = Postgrex.Connection.start_link(database: "bigmachine")
  sql = "select * from membership.register($1, $2);"
  Postgrex.Connection.query(pid, sql, [email, password])
    |> Membership.DBResult.map_single
    |> to_registration_result
end
```

I really love this. I'm running the query and then using the Elixir pipe operator `|>` to essentially "shove" the results into the next routine, which is the `map_single` stuff I wrote above. Finally, when I get it back I shove it into a `to_registration_result` function, which is this, here:

```elixir
def to_registration_result({:ok, res}) do
  {:ok, %Membership.RegistrationResult{
    success: res["success"],
    message: res["message"],
    new_id: res["new_id"],
    validation_token: res["validation_token"],
    authentication_token: res["authentication_token"]
  }}
end

def to_registration_result({:error, err}) do
  {:error, err}
end
```

Notice that I have two methods with the same name? This is, once again, **Pattern Matching** at its best. Elixir will call the function according to whatever signature is passed along. If there's an error, the second function will be called. Otherwise it will be the first, which then, finally, passes the result back.

As always, [you can see the code I'm writing up here, at Github](https://github.com/bigmachine-io/bigmachine-membership). I *know* that there is a ton of room for improvement - so if you have a thought please share.

<div class="ui items" style="padding-top:36px;border-top:1px solid #e5e5e5;">
  <div class="item">
    <div class="image">
      <a href="https://goo.gl/zvMHWK" target=_blank>
        <img src="/img/red4_product_slide.png">
      </a>
    </div>
    <div class="content">
      <a class="header" href="https://goo.gl/zvMHWK">Want to learn Elixir?</a>
      <div class="meta">
        <span>Learn how to build fast, fault-tolerant applications with Elixir.</span>
      </div>
      <div class="description">
        <p>
          This is not a traditional, boring tutorial. You'll get an ebook (epub or mobi) as well as 3 hours worth of tightly-edited,
          lovingly produced Elixir content. You'll learn Elixir <i> while doing Elixir</i>, helping me out at my new fictional job
          as development lead at Red:4 Aerospace.
        </p>
      </div>
    </div>
  </div>
</div>]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2015/09/recursion_title.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2015/09/recursion_title.jpg" />
  </entry>
  <entry>
    <title>Recursion, Not Recursion</title>
    <link href="https://bigmachine.io/posts/recursion-not-recursion" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.729Z</updated>
    <id>https://bigmachine.io/posts/recursion-not-recursion</id>
    <summary type="text">Had a great comment from my last post (about using Recursion):

![](https://blog.bigmachine.io/img/2015/09/Screen-Shot-2015-09-09-at-8.55.13-AM-1024x226.png)

I had to Google what &quot;acyclic call graph&quot; meant because I just couldn&apos;t remember :). I do remember what a call graph is..</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2015/09/Princess_Bride_That_Word.jpg" alt="Recursion, Not Recursion" /></p>
Had a great comment from [my last post](http://bigmachine.io/2015/09/04/using-recursion-in-elixir-to-break-your-oo-brain/) (about using Recursion):

![](https://bigmachine.io/img/2015/09/Screen-Shot-2015-09-09-at-8.55.13-AM-1024x226.png)

I had to Google what "acyclic call graph" meant because I just couldn't remember :). I *do* remember what a call graph is... but...

So I showed my brother, Mr. Computer Science Professor, who is learning Elixir with me for fun and profit and he said:

> Yep, he's right. Your use of recursion is interesting but it muddles things and it's not really recursive, even though you're using it that way.

Big brothers. Gah.

Here's the code in question:

```elixir
defmodule Membership.DBResult do
  def map_single({:ok, res}) do
    cols = res.columns
    [first_row | _] = res.rows
    map_single {:cols_and_first, cols, first_row}
  end

  def map_single({:cols_and_first, cols, first_row}) do
    zipped = List.zip([cols,first_row])
    map_single {:zipped, zipped}
  end

  def map_single({:zipped, list}) do
    {:ok, Enum.into(list, %{})}
  end

  def map_single({:error, err}) do
    {:error, err}
  end

end
```

He went on:

> A recursive routine typically performs some kind of operation that gets repeated. Here, you're doing three different things, abusing the notion of recursion.

And it hit me. Of course. As I mention in the very first post in my little Elixir excursion - I've used recursion in the past but certainly not regularly. It *did* solve a problem, but not appropriately.

Another lesson learned.

Anyway I refactored things a bit to a lot clearer and less of a muddle:

```elixir
defmodule Membership.DBResult do

  def get_first_result({:ok, res}) do
    cols = res.columns
    [first_row | _] = res.rows
    {:ok, cols, first_row }
  end

  def zip_columns_and_row({:ok, cols,row}) do
    {:ok, List.zip([cols,row])}
  end

  def create_map_from_list({:ok, list}) do
    {:ok, Enum.into(list, %{})}
  end

  def map_single({:ok, res}) do
    get_first_result({:ok, res})
      |> zip_columns_and_row
      |> create_map_from_list
  end
end
```

I need to add some error traps in here, of course, but this is so much clearer! Thanks **KMag** - appreciate the nudge :).


<div class="ui items" style="padding-top:36px;border-top:1px solid #e5e5e5;">
  <div class="item">
    <div class="image">
      <a href="https://goo.gl/zvMHWK" target=_blank>
        <img src="/img/red4_product_slide.png">
      </a>
    </div>
    <div class="content">
      <a class="header" href="https://goo.gl/zvMHWK">Want to learn Elixir?</a>
      <div class="meta">
        <span>Learn how to build fast, fault-tolerant applications with Elixir.</span>
      </div>
      <div class="description">
        <p>
          This is not a traditional, boring tutorial. You'll get an ebook (epub or mobi) as well as 3 hours worth of tightly-edited,
          lovingly produced Elixir content. You'll learn Elixir <i> while doing Elixir</i>, helping me out at my new fictional job
          as development lead at Red:4 Aerospace.
        </p>
      </div>
    </div>
  </div>
</div>]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2015/09/Princess_Bride_That_Word.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2015/09/Princess_Bride_That_Word.jpg" />
  </entry>
  <entry>
    <title>How To Learn a New Programming Language While Maintaining Your Day Job</title>
    <link href="https://bigmachine.io/posts/>-" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.729Z</updated>
    <id>https://bigmachine.io/posts/>-</id>
    <summary type="text">I don&apos;t typically write &quot;lifehack&quot; posts, but this question has come up repeatedly over the last few weeks:

&gt; How did you find time to learn Elixir? Must be nice to not work or have a family :p

First: I&apos;m certainly no expert in Elixir but I am finding my way through the languag</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2015/10/learn_programming.jpg" alt="How To Learn a New Programming Language While Maintaining Your Day Job" /></p>
I don't typically write "lifehack" posts, but this question has come up _repeatedly_ over the last few weeks:

> How did you find time to learn Elixir? Must be nice to not work or have a family :p

First: _I'm certainly no expert in Elixir_ but I am finding my way through the language. I pushed [my first two](https://hex.pm/packages/blackbook) [packages to Hex](https://hex.pm/packages/stripity_stripe) over the weekend and I'm having a really good time.

I'm also very married and a father of two, and [I'm happily working full time](http://www.pluralsight.com/author/rob-conery) (more than full time really) with Pluralsight. **But I also had ample time to dive into Elixir** and I thought I would share how I did it.

And I'll just come out with it right here: _A series of little wins_. I set myself up to win tiny little tasks which then led to more tiny Little Wins and the next thing I knew, I was thinking in Elixir. **A series of Little Wins**, I can't write that enough. More on this below.

This post will be a combination of "lifehack" stuff as well as tangible steps I took over the course of two weeks to become reasonably proficient in the language. Apply to whatever language you like. Elixir is the 7th programming language I've learned (and no I'm not counting HTML and CSS):

- Pascal
- vbscript/VBA
- SQL/PLPGSQL
- C#
- Ruby
- JavaScript

These are in no particular order and we could argue whether SQL is really a programming language but I've [programmed systems using PLPGSQL](https://github.com/robconery/pg_auth) so I say it is :p.

Anyway, onward.

## Give Yourself Permission

Make the decision, don't waffle. Don't say stupid shit to yourself, including:

- It's a fad
- I'm too old/young/fat
- There's no job market for it
- Something will come along and replace it later

I could go on, but you get the idea. These **are downers** and only serve to derail inspiration and make you sound toxic. Don't be toxic.

### It's a Fad (or: "I'm an Asshole")

People have said this to me about _every single language I learned_ save for Pascal (because it was the 80s) and SQL (because it's a workhorse language that's been around forever). **Everything is a fad in a long enough time scale** and when you say these words aloud **you sound like a complete asshole**. You might be right, the language you're referring to might go away in a few years - _you still sound like a toxic asshole_ so let be aware of how your words impact others - mostly yourself.

Assholes don't do anything but sit on the sidelines and call people names. Let them. You hitch your wagon to whatever star you like and be a better you. We're in an industry that is ever-changing, ever-moving. You need to move with it or you need to move aside.

### There's No Job Market/Use Case/What Can I Do With It?

You're opening a door here, not solving a problem. Even if nothing amounts from it you've stretched your brain some, exercised your ability to solve a problem and **that is always a win**. You simply cannot go wrong by trying to learn something.

When you do learn it you might find yourself creating something fun or solving a problem in a more elegant way that you can bring back to your day job. I did just this when I learned Ruby - my brain exploded with ideas and I created Subsonic and a number of other projects inspired by the language.

And you never know what could happen in a few years' time. Ruby and JavaScript didn't have much of a market years ago, neither did Java and .NET. Be ahead of the curve - or **define the damn curve yourself**.

Decide. Just... **Decide**. Now let's...

## Execute

Before I get into the nitty gritty details I need you to focus on the _importance of execution_. You can't go into this with the decision made and say "ho hum when I get the time... maybe this weekend". Nope. **You must execute** and it can literally be 10 minutes a day at lunch or 2 hours at night where you code instead of fucking off on Xbox.

What's coming next is how to set yourself up to win little tasks - building one win on top of another, creating a "cadence" the builds under you. But you won't do any of it unless you commit. I put "Elixir Time" on my calendar every day and treat as an appointment I need to keep, which underserves it because it's so much more.

This is time I'm investing in myself, in my career. Time that I will not question, I'll just execute.

Here are some simple steps to move beyond dreaming about it and actually doing it. They might sound silly, but each one is a demonstration of your commitment and, in a way, a Little Win:

- Follow the movers and shakers on Twitter
- [Sign up for the weekly newsletter](http://blog.plataformatec.com.br/2015/01/introducing-elixir-radar-the-weekly-email-newsletter-about-elixir/)
- Install Slack and subscribe to the language team
- Read and Post questions on Stack Overflow
- Write some blog posts (if you're into that kind of thing) about your explorations. This requires you to be fearless (see above).

And now, let's write some code.

## My Series of Little Wins

_I suck as a programmer_ so it's important for me to not give into my incompetence and lack of confidence. I Decided to learn elixir I gave myself time to Execute - now the question became _what, exactly, will I do?_.

Having a goal is nice, but it can also be destructive if you set it when you have no idea what's going on. **So skip the goal**, let's Execute and have a good time!

### Task 1: Creating a New Project

How much easier could it be? With C#/.NET it's File > New Project and you pick your flavor of project. With Ruby it can be a single file all the way up to a nicely structured set of directories. It's a little wild west in there, but it's doable. With Node it's `npm init my_project`.

With Elixir [it's spelled out with a quick Google search](http://elixir-lang.org/getting-started/mix-otp/introduction-to-mix.html):

```sh
mix new my_project

➜  Projects  mix new my_project
* creating README.md
* creating .gitignore
* creating mix.exs
* creating config
* creating config/config.exs
* creating lib
* creating lib/my_project.ex
* creating test
* creating test/test_helper.exs
* creating test/my_project_test.exs

Your mix project was created successfully.
You can use mix to compile it, test it, and more:

    cd my_project
    mix test

Run `mix help` for more commands.
```

That took 3 or 4 minutes of reading, 3 seconds to do. At this point we have a lot we can do, but the first thing to notice is the last line - where to find help. That says something about the project. We have a `README.md`, a `.gitignore`, a `/test` directory and a config setup. Nice!

We just learned a lot and executed a little win. We can now stop here and go play Xbox.

### Task 2: Writing a Test

We've created an initial project, now how do I write a test? This is where things can get kind of nuts because we haven't even dabbled in the language yet! Let's not get too out of control on this one - _we need a small win_ so make this easy.

There's a `test` directory so having a look inside we see two files: `my_project_test.exs` and `test_helper.exs`. This tells us something:

- The file extensions for Elixir are `.ex` and `.exs` as seen in our project setup. Something to write down later as a Little Win.
- The `my_project_test.exs` file is likely a throw away, which is great because we can hack it up
- The concept of Test Helpers is there for us. Nice!

Looking inside the test file we see this:

```sh
➜  my_project  cat test/my_project_test.exs
defmodule MyProjectTest do
  use ExUnit.Case

  test "the truth" do
    assert 1 + 1 == 2
  end
end
```

It's a somewhat familiar syntax and I can apply rules from other languages to it. The constructs are clean and feel very Ruby-ish to me. I like that (I like Ruby) and from here I think I can write a test of my own:

```
defmodule MyProjectTest do
  use ExUnit.Case

  test "the truth" do
    assert 1 + 1 == 2
  end

  test "my name" do
    assert "Rob" == "rob"
  end
end
```

Kind of a dumb test, but I want to watch it fail (hopefully). If it doesn't, I just learned something rather huge. Now I just need to figure out how to run a test.

I remember when I created the project it said "Run 'mix help' for more commands". Let's start there - if mix created the project for us, it will probably run the tests too:

```sh
➜  my_project  mix help
mix                   # Run the default task (current: mix run)
mix app.start         # Start all registered apps
mix archive           # List all archives
mix archive.build     # Archive this project into a .ez file
mix archive.install   # Install an archive locally
mix archive.uninstall # Uninstall archives
mix clean             # Delete generated application files
mix cmd               # Executes the given command
mix compile           # Compile source files
mix deps              # List dependencies and their status
mix deps.clean        # Remove the given dependencies' files
mix deps.compile      # Compile dependencies
mix deps.get          # Get all out of date dependencies
mix deps.unlock       # Unlock the given dependencies
mix deps.update       # Update the given dependencies
mix do                # Executes the tasks separated by comma
mix escript.build     # Builds an escript for the project
mix help              # Print help information for tasks
mix hex               # Prints Hex help information
mix hex.build         # Builds a new package version locally
mix hex.config        # Reads or updates Hex config
mix hex.docs          # Publishes docs for package
mix hex.info          # Prints Hex information
mix hex.key           # Hex API key tasks
mix hex.outdated      # Shows outdated Hex deps for the current project
mix hex.owner         # Hex package ownership tasks
mix hex.publish       # Publishes a new package version
mix hex.registry      # Hex registry tasks
mix hex.search        # Searches for package names
mix hex.user          # Hex user tasks
mix loadconfig        # Loads and persists the given configuration
mix local             # List local tasks
mix local.hex         # Install hex locally
mix local.rebar       # Install rebar locally
mix new               # Create a new Elixir project
mix phoenix.new       # Create a new Phoenix v1.0.0 application
mix run               # Run the given file or expression
mix test              # Run a project's tests
iex -S mix            # Start IEx and run the default task
```

Good grief. There's a lot in here - **stuff I should actively ignore right now** because I'll short-circuit, become overloaded and give right into my insecurities that I talked about above (\*I can't do this, it's too big, it's a fad\*\*).

Right there at the bottom is what I need - `mix test              # Run a project's tests`. Boom.

```sh
➜  my_project  mix test
Compiled lib/my_project.ex
Generated my_project app


  1) test my name (MyProjectTest)
     test/my_project_test.exs:7
     Assertion with == failed
     code: "Rob" == "rob"
     lhs:  "Rob"
     rhs:  "rob"
     stacktrace:
       test/my_project_test.exs:8

.

Finished in 0.02 seconds (0.02s on load, 0.00s on tests)
2 tests, 1 failures
```

Radical! I learned so, so much with this Little Win:

- Elixir is a compiled language
- Testing a project also compiles it
- Case sensitivity is built-in for strings (hooray!)
- The assertion failed in an informative way. `lhs` means "left hand side", `rhs` is "right hand side" and I can see what failed and how.

With these two wins I can now move on feeling pretty damn happy.

## Strings, Dates, Numbers - Getting a Footing

This part might seem obvious, and indeed it is. It also requires a book or some type of learning resource besides Google. For this kind of thing I tend to go [directly to Pragmatic Programmers](https://pragprog.com). Do a search on Elixir and find a great set of books. But there's one that stands out and it's amazing: [Dave Thomas's Programming Elixir](https://pragprog.com/book/elixir/programming-elixir). This book is simply wonderful and I can't recommend it enough.

Usually when I read programming books I like to read them in the order in which makes sense - for me it's learning the basic types, then the operators, data structures and so on. Here I found Dave's lead in to be so fun to read - I recommend it highly.

Elixir is a functional language so this book starts with that, which it should. For me, functions are made from primitive elements like strings, numbers, dates etc. so I like to know how those work. I read how those work, play around in my little project and write some more tests to see these things work.

Specifically strings - that's just me. I find that if a language has any warts it will be in how it handles strings, mutability, regex, and so on.

This is where you and I will branch off. I get my footing by understanding basic types - you might need to learn data structures and method calls. Either way slice it up so you can learn a little at a time. For me, it was:

- Concatenating a String
- Using basic Regex to run replacements and matches
- String syntactic sugar

With dates I found out (rather quickly) that the date/time story in Elixir is a bit limited and if you need to work with these things you'll be installing [a package called Timex](https://github.com/bitwalker/timex). I also found out that you can access stuff from Erlang directly (which a lot of people do). I bookmarked this as a thing to come back to (which I still haven't done).

## Data Structures and Databases

My first set of experiments and Little Wins took about a week, and the more I built up my wins, the harder it became to get focused. I fight this constantly - _I just want to jump to the finish line_ and build stuff. It takes a lot of effort to stop myself and stay focused.

I felt pretty good about the basics and how projects went together - now it was time to work with basic lists and data structures.

**And this is where my brain exploded**. Elixir is a functional language and there are aspects to it that are completely foreign to me (which is no surprise really). I tried as hard as I could to control the firehose of concepts coming at me... but eventually I just gave up and let it wash over me and I read the book all the way through.

Knowing I would pick it up again and read it quite a few times over.

At this point I stared at the mess of concepts on the floor and decided to pick one and dive into it. So I did - setting myself up for Little Wins here and there with each "higher" concept beyond the basics. Specifically:

- Using Recursion to iterate over a list and perform an operation
- Using List Comprehensions in a meaningful way
- Using the Pipe operator

The problem I typically have with this kind of thing is that _it's all so demo-y_. I need to grasp something real! So I did, and I modified my list:

- **Successfully Query PostgreSQL**
- Using Recursion to iterate over a list and perform an operation
- Using List Comprehensions in a meaningful way
- Using the Pipe operator

From there I was able to roll lists of data that meant something to me, using the the types and tests that I began to understand from before... the wins were piling up...

## An Ongoing Thing: Finding the Idioms

Here's a neat thing about Elixir - there are some voluntary idioms that people use (like the callback structures in Node) to make life easy. Here are a few:

- Required function parameters come first and are listed out. Optional parameters are passed using a Keyword List
- Function results typically have a tuple structure that indicates what happened - like `{:ok, return_data}` or `{:error, message}`
- Pattern Matching allows you to control the flow of your application in a very elegant way
- Writing smaller, more concise functions and piping them together makes programming a lot of fun

There are more idioms - and right now this is where I'm at. Call it "Level 5" if you will - and this is where the work begins. From here the Little Wins shift from learning to building, which is exactly what I've started to do.

I feel pretty good about the way things are going.

## Summary

Are you feeling anxious about this post? Do you want to tell me how _you wish you had time_ or that _that sounds fun but I need to make money_? I'll ask you why you work in a young, vibrant industry and expect things to stay the same for you.

Be an innovator. Make positive changes. You can do it - just give yourself the time and go for the Little Wins.

<div class="ui items" style="padding-top:36px;border-top:1px solid #e5e5e5;">
  <div class="item">
    <div class="image">
      <a href="https://goo.gl/zvMHWK" target=_blank>
        <img src="/img/red4_product_slide.png">
      </a>
    </div>
    <div class="content">
      <a class="header" href="https://goo.gl/zvMHWK">Want to learn Elixir?</a>
      <div class="meta">
        <span>Learn how to build fast, fault-tolerant applications with Elixir.</span>
      </div>
      <div class="description">
        <p>
          This is not a traditional, boring tutorial. You'll get an ebook (epub or mobi) as well as 3 hours worth of tightly-edited,
          lovingly produced Elixir content. You'll learn Elixir <i> while doing Elixir</i>, helping me out at my new fictional job
          as development lead at Red:4 Aerospace.
        </p>
      </div>
    </div>
  </div>
</div>]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2015/10/learn_programming.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2015/10/learn_programming.jpg" />
  </entry>
  <entry>
    <title>Thinking About Function Signatures in Elixir</title>
    <link href="https://bigmachine.io/posts/thinking-about-function-signatures-in-elixir" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.729Z</updated>
    <id>https://bigmachine.io/posts/thinking-about-function-signatures-in-elixir</id>
    <summary type="text">One of the things I&apos;ve had to adjust to is how I want to structure function calls in Elixir. This is forced upon you by Pattern Matching and is a Very Good Thing. Deciding on these patterns early on can really be helpful.

Consider this function:



This is how you might think ab</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2015/10/pipeline_builders.jpg" alt="Thinking About Function Signatures in Elixir" /></p>
One of the things I've had to adjust to is how I want to structure function calls in Elixir. This is forced upon you by [Pattern Matching](http://elixir-lang.org/getting-started/pattern-matching.html) and is a Very Good Thing. Deciding on these patterns early on can really be helpful.

Consider this function:

```
def charge_customer(id, amount, description, card, currency \\ "USD") do
  #  ...
end
```

This is how you might *think* about writing an Elixir function - what's required, what can be defaulted. But then I get that twitch that I used to get with Ruby all the time: *there has to be something more elegant*.

## Taking a Step Back

The method call above is a bit too long and is also a bit "wobbly" for lack of better words. The first thing to consider here is *how will this function be used?*. I think it will be something like this:

```
def process_checkout(args) do
  verify_cart(args.cart)
    |> charge_customer
    |> create_invoice
    |> debit_inventory
    |> empty_cart
    |> send_email
end
```

The `|>` operator simply chains the calls together, sending the result of one function into another. Also - this is *semi-pseudo-code*, there would likely be transactions involved here... anyway...

Consider what is going to be passed to the `charge_customer` function - it will be the result of `verify_cart` which, likely, will be the cart so that's good. But as you scan down the list... you start to realize that passing information along will require some greater thought. And lighter functions with simple parameter structures.

## Just Tell Me What You Need!

One way to do this is to only pass a single argument along (an *arrity of 1* in Elixir-speak: `/1`). You can do this by using tuples:

```
{:ok, cart}
```

By passing a "qualifier" in the first tuple position, you can setup Pattern Matching in a much nicer way (which I'll get to later). But this only gets us half way. Our `charge_customer` function needs a lot more than just a cart - it needs some kind of payment method as well (the description, amount and currency can be pulled from the cart).

We can do this by tweaking the parameter list thus:

```
def charge_customer({:ok, cart}, card) do
  #  ...
end
```

This looks a little strange, but it's doing two things:

 - By setting the first parameter to `{:ok, cart}` it's making sure that whatever function is calling to it is delivering back a good result
 - The second parameter, card, is required

Now we can add an additional definition to handle an error:

```
#just pass along the error through the chain
def charge_customer({:error, err}), do: {:error, err}
def charge_customer({:ok, cart}, card) do
  #  ...
end
```

Now we have two solid pattern matches, which is good. Our function structure is more flexible than before, but there's more we can do.

## Building In Flexibility With a Keyword List

Pattern matching is key to writing flexible code that you can massage later on. For instance - if we're using a gateway like Stripe we might want to pass a card token along, rather than the card information itself. Or we might be using Paypal's Express Checkout and have a Paypal token in there.

What we need is a more flexible structure - and we can do this (and flex pattern matching) using a [Keyword List](http://elixir-lang.org/getting-started/maps-and-dicts.html) as our second argument:

```
def charge_customer({:error, err}), do: {:error, err}
def charge_customer({:ok, cart}, [card: card]) do
  #  ...
end
def charge_customer({:ok, cart}, [token: token]) do
  #  ...
end
def charge_customer({:ok, cart}, [paypal: paypal]) do
  #  ...
end
```

Each one of these methods will match based the payment type. But how would this work in a Pipeline? Like this:

```
def process_checkout(args) do
  verify_cart(args.cart)
    |> charge_customer(token: "cx_339393939")
    |> create_invoice
    |> debit_inventory
    |> empty_cart
    |> send_email
end
```

There are two really neat things happening here. The first is that you can "inject" arguments into a piped function call, and whatever you add will be placed at the end of the parameter list. So `verify_cart` will return `{:ok, cart}` that will then get passed to `charge_customer`, then the token will be passed in second position (that's a keyword list with some syntactic sugar, braces removed).

In the real world the payment information would be passed in through the arguments, and you would probably pass on `args.payment` or the like.

## The End Result

Thinking about pattern matching and functional "interop" if you will leads you naturally towards keeping things flexible and light. As I was writing out the little libraries I wrote over the weekend, I started to focus less on writing individual functions and more on entire modules, together.

I found that adhering to the `{tuple}, options` argument structure worked really well for me, but as I keep saying *I am just learning this stuff* and if you have found better patterns, sound off!

<div class="ui items" style="padding-top:36px;border-top:1px solid #e5e5e5;">
  <div class="item">
    <div class="image">
      <a href="https://goo.gl/zvMHWK" target=_blank>
        <img src="/img/red4_product_slide.png">
      </a>
    </div>
    <div class="content">
      <a class="header" href="https://goo.gl/zvMHWK">Want to learn Elixir?</a>
      <div class="meta">
        <span>Learn how to build fast, fault-tolerant applications with Elixir.</span>
      </div>
      <div class="description">
        <p>
          This is not a traditional, boring tutorial. You'll get an ebook (epub or mobi) as well as 3 hours worth of tightly-edited,
          lovingly produced Elixir content. You'll learn Elixir <i> while doing Elixir</i>, helping me out at my new fictional job
          as development lead at Red:4 Aerospace.
        </p>
      </div>
    </div>
  </div>
</div>]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2015/10/pipeline_builders.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2015/10/pipeline_builders.jpg" />
  </entry>
  <entry>
    <title>Learn Elixir The Fun Way</title>
    <link href="https://bigmachine.io/posts/learn-elixir-while-having-fun" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.729Z</updated>
    <id>https://bigmachine.io/posts/learn-elixir-while-having-fun</id>
    <summary type="text">About 3 years ago I had an idea for creating a different kind of tutorial. Something that would combine the problem-solving of a video game, the immersion of a sci-fi story and the joy of learning something new. A tutorial you wanted to finish in the same way you want to finish a</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2016/01/splash.jpg" alt="Learn Elixir The Fun Way" /></p>
About 3 years ago I had an idea for creating a different kind of tutorial. Something that would combine the problem-solving of a video game, the immersion of a sci-fi story and the joy of learning something new. A tutorial you *wanted to finish* in the same way you want to finish a good book or fun video game.

[I finally got around to doing just that](https://goo.gl/zvMHWK)... *sort of*. Let me explain.

## Not Quite a Book, Not Quite a Tutorial

This is an Elixir tutorial. It's a downloadable application which uses the [Electron Shell](http://electron.atom.io) (from Github - the thing that powers Slack and the Atom text editor). It's divided into chapters like a book is, but you are actively involved in the content - it's not something you just sit back and experience.

I wrapped it around the idea that you work for me at a startup (Red:4) and I'm on the hot seat trying to get some demos ready. We've been asked to help the Science Team with some basic calculations they need for the probe they're about to launch. We have a serious case of NIH here at Red:4.

This is where the "fun problem solving thing" starts. Rather than write `def foo` and `bar = x`, I make you:

 - Calculate escape velocity for each of the planets
 - Calculate orbital acceleration and term for our prototype orbiter
 - Create a solar flare warning system using real flare data
 - Store that solar flare information in Mnesia (the Erlang DB) as well as PostgreSQL
 - Refactor my rushed attempt at creating a planet library
 - Blow up PostgreSQL using OTP


<img src="http://bigmachine.io/img/2016/01/calcs_1.png" alt="calcs_1" width="400" height="401" class="aligncenter size-full wp-image-740" />

That last item is the fun part. We play around with asynchronous patterns and the OTP library for Erlang - carefully managing a barrage of queries so PostgreSQL doesn't get DDoS'd... and then unleashing the hounds.

## An Elixir Expert, Are You?

Hardly. I'm not an expert in anything - I'm just a rabidly obsessive person who likes to figure things out. Yes you likely read about new things here on my blog and you should expect nothing less! Occasionally, however, something sticks.

Elixir is that something for me. When I learn something new I want to tell everyone about it - that's just me. I like the way [Derek Sivers](http://sivers.org) puts it:

> Right after you learn something, teach it to someone! Do it before you forget what *not knowing it was like*

I've woven this idea into my little Elixir tutorial thingy and tried to infuse the joy I felt learning this language, OTP, and the Erlang VM. There's still so much to learn - and that's the exciting part.

But excitement only gets you so far - you also have to be sure what you've created isn't garbage :). To that end I asked [José Valim](http://twitter.com/josevalim) and [Johnny Winn](https://twitter.com/johnny_rugger) (curator of [The Elixir Fountain](https://twitter.com/elixirfountain)) to do the tech review for me.

Anyway - I had a ton of fun creating this thing and I'd love to do more like it. I still do videos for Pluralsight - this is just a fun side venture.

You can [read more about it here](https://goo.gl/zvMHWK/) or if you want to buy it right now you can do that too! Here's a small introduction video:

<div class="embed-responsive embed-responsive-16by9">
<iframe src="https://player.vimeo.com/video/149825791" width="800" height="455" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div>]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2016/01/splash.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2016/01/splash.jpg" />
  </entry>
  <entry>
    <title>JSONB and PostgreSQL: Work Faster By Ditching Migrations</title>
    <link href="https://bigmachine.io/posts/jsonb-and-postgresql" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.729Z</updated>
    <id>https://bigmachine.io/posts/jsonb-and-postgresql</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/pooh-jsonb.jpg" alt="JSONB and PostgreSQL: Work Faster By Ditching Migrations" /></p>
Migrations are a simple mechanism whereby you script out some change commands for your ORM, and that ORM then builds your database for you. To me, this is pure insanity. I dislike ORMs (accept, of course, for [LLBLGenPro](https://www.llblgen.com), which is astoundingly good). Trusting your ORM to build a proper database is ... kind of weird to me. SQL is terser, more expressive and (as it turns out) just right for the job.

## What About Change Management?

That's a question I get asked constantly when I start ranting on ORMs, favoring SQL instead. The answer I always give remains the same:

> Know your SQL, use change scripts, get a reliable DIFFing tool

This answer usually elicits some laughs. I can see why - migrations are really, really powerful when you're getting your database off the ground. When your Majestic Monstrosity moves past it's 3rd or 4th year of life, however, those 40 or so migration files start to look rather ridiculous.

Creating, altering, dropping, reloading a few times over. Kinda like...

<img src="/img/airport_queue.jpg" class="img-responsive" />

I suppose you can deal with this during one of the [Majestic Rewrites](https://twitter.com/dhh/status/695272044024487936) - "rebasing" your migrations if you will. That's some lovely extra work isn't it :).

I think there's an easier way.

## Lean On JSONB

As I'm sure you're aware, PostgreSQL supports binary JSON (like MongoDB does). This means you can treat your lovely relational engine like a document database. You just need [a data tool](https://github.com/robconery/massive-js) that [supports JSONB at the top level](https://github.com/robconery/moebius).

A document is, basically, "schema-less". This means that you can save whatever JSON structure you want, and change it as you need. **Ideal for getting your app off the ground**. It's OK if you're a big fan of relational systems! This is something to work toward if you want.

In fact, I'll go so far as to suggest that *normalization is optimization*. Well sort of. It's optimization in terms of data structure and "good database design" - not always for speed (joins can be costly). For large transactional systems you'll probably want some tight rules in there with constraints, checks, indexes - all the nutritious stuff an growing database needs.

But not right away. Write the code, design the experience - *nail the idea first*.

I'm a big believer that applications take on a life of their own and, after a while, start to write themselves. It's a birthing process where you get inspired, write some code and watch it work, become even more inspired which makes you write more ... and boom! Here's a new baby app for you.

Want a sure-fire way to drain that enthusiasm? **Add in some needless friction in the form of migrations**. Not only do you need to think through the normalization rules (does that foreign key ensure against nulls?) - you also get to wrestle with the migration tool, it's syntax, and version conflicts in the database.

Oh yeah did you run the migration on the test database too?

Let it go. Use JSONB to freely design your storage and when the time is right, **normalize that shit**. Or don't - it's up to you.

## Preaching What I Practice

Here's why I'm writing this post: I'm [building up a groovy eCommerce store with Elixir](http://bigmachine.io/category/redfour/) that I plan to use in the wild. I have changed direction, refined, tweaked, prodded and tossed entire aspects of the application into the bin. I have thought about data access perhaps three times - yet I'm storing everything I need, exactly as I need it.

This could change. It will change... and that's my point. *I'm letting it change without my needing to micro-manage it*. I'm letting it grow, die, and grow again without having to orchestrate each step becasue if I did... I would have given up.

It's why I'm writing this post. I'm having fun and thought I would share :).]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/pooh-jsonb.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/pooh-jsonb.jpg" />
  </entry>
  <entry>
    <title>Red4 Store Part 5: Fun With Phoenix, OTP, and Agents</title>
    <link href="https://bigmachine.io/posts/more-adventures-with-otp" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.729Z</updated>
    <id>https://bigmachine.io/posts/more-adventures-with-otp</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/max_smart.jpg" alt="Red4 Store Part 5: Fun With Phoenix, OTP, and Agents" /></p>
*Before I get to the meat of this post, [the code for what I've written so far is up here](http://github.com/robconery/peach). The main bits are in `/apps/peach`.*

<hr>

## Stumbling a Bit

I've had a hard time over the last few weeks trying to figure out what's going on with this little app. It's been quite fun diving into OTP and the neat little abstractions Elixir provides on top of Erlang and OTP. I have found working with Phoenix, however, to be a bit challenging. Crawling up into the Phoenix abstractions and trying to understand them (so I can figure out if I can remove them) has been somewhat ... not fun.

I know the some will think I'm being "negative" but... well I like to be honest about things. I do try to give an educated opinion - not just be an ass for the fun of it... promise.

The conversations I've had about this over the last few weeks have basically gone like this:

> Me: Phoenix is rather convoluted and a bit heavy. I can see why people compare it to Rails

> Phoenix person: Phoenix **is not Rails** and it's not heavy.

> Me: Well that's my impression because I'm having to rip out quite a lot that I don't want or need. Just your saying so isn't really convincing me.

> Phoenix person: You'll need all of that eventually. Phoenix isn't heavy.

> Me: I'd rather opt-in to what I need. Namely Channels, Ecto, Brunch, Views... and

> Phoenix person: Those are all optional (aside from Channels). You can remove them by ...

> Me: Yes - this is my point. I have the kitchen sink here when all I need is a small bucket

> Phoenix person: You can have your small bucket. Phoenix is supposed to be beginner-friendly so just follow this procedures to strip Phoenix down

> Me: I thought Phoenix wasn't Rails? This conversation is draining (get it... you gave me the kitchen sink...)

> Phoenix person: /mute

This is the problem with **opt-out by default**: you (by definition) start out with way more than you need; all of which you need to understand before you do anything. If you don't completely understand the toolset you're given (by default), you end up building crap, which is precisely what happend to me (repeatedly) with Rails.

When you complain about this stuff, you're told you shouldn't use a tool you don't understand completely and by the way in the next version we're adding some really great new features everyone will love.

I think I'll coin a term here:

<div class="well">
  <p>
  <u><b>Technical Entrapment</b></u><br>
  <i>When a framework front-loads tools and abstractions to the point that technical debt and a rewrite within a year is guaranteed.</i>
  </p>
</div>

There's a lot I'm not using with Phoenix: namely Ecto and Brunch (yes, I did start off with `--no-ecto` and `--no-brunch` which I think should be the default)- and I'm not using migrations or models (I don't care for ORMs much and find them odd in a functional landscape). Because I'm not using models, I can't use the generators (which I guess I wouldn't use anyway).

I don't need real-time channels (although the option is nice to have if I want it later) and the View abstraction is something I don't care to use as I can just reference UI helper stuff directly from modules (I want to keep formatters etc centralized). Controllers don't make sense without Models and Views so there goes the whole MVC idea and I'm not putting the bulk of my application within the web project (I'm using an umbrella project to keep things initially separated).

If you're wondering why I'm using Phoenix at all - I think that's a really good question. I suppose, in part, because *I do think things will get more complicated* and I probably *will* opt for a Controller here and a View there. *Maybe* - I just don't know yet. I kind of want to figure that out as I go.

The primary reason for this is that I will likely offload a number of things to external services to start out with (like [Auth0 for user stuff](http://auth0.com) and [Keen.io](http://keen.io) for reporting. Mailjet will probably handle mailers too) so that just leaves a small core. I might change my mind and bring some of that stuff in-house in the future, so the option to build out is a good one to have.

I don't think I'm a unique snowflake here, but I do feel like **I'm somehow going against the grain** and "being Rob, doing things differently" (yes, that was said to me to). I guess I don't see this as *being different*. I see it as rather normal.

As my friend [Scott Hanselman](http://hanselman.com) said once:

> It's Comp Sci baby

But learning is fun so I suppose I should be grateful that I get to learn more things. So let's be positive.

I do like the Router, the integration with the rendering engine, and *I love* the way Phoenix just sits nicely with Plug. The `pipeline` is a neat idea and very clear. It would be great if I could just start here, by default. Then when I need to, I can add in controllers etc.

Anyway: I will spend some more time with Phoenix. The team has insisted that once I get into more complexity, the abstractions and structure will make sense. I believe them - they're smart people. Smarter than me, I'm sure.

Let's get to the OTP stuff.

## Simplifying with Agents

In [part 3 of this little series](http://bigmachine.io/2016/02/17/red4-store-part-3/) I dropped into using `GenServer` and OTP so I could kick up a `Session` for each customer to come along. This would be a standalone process running in memory that existed *only* to store information about the customer's shopping session (adding items, removing, etc).

I loved the approach - it felt correct. I loved how it simplified data access (saving the `Session` entirely when needed) and flexed the power of the Erlang VM. And then something occurred to me...

### What if I Event Sourced this?

I hate throwing jargon and terms around - but taking a step back and squinting at our little `Session` GenServer ... we might have [a classic Event Source](http://martinfowler.com/eaaDev/EventSourcing.html)

> The fundamental idea of Event Sourcing is that of ensuring every change to the state of an application is captured in an event object, and that these event objects are themselves stored in the sequence they were applied for the same lifetime as the application state itself.

If you consider the `Session` to be an event (which it kind of isn't... more below) and our GenServer process to be an "app" in Fowler's mind - it makes good sense.

So I decided to test this out.

The first thing I did was think more in terms of a meaningful result to this event. Shopping *is literally* an event, but I think what we would want to trap is *the result of that event*, which would be one of two things:

 - A sale
 - No sale

They both have value and meaning - but let's be positive and focus on the first thing. If I was to store a gigantic blob of data that clearly showed the result of a customer shopping in our store - what would that look like?

Here's what I came up with:

```elixir
defmodule Peach.Accounting.SalesOrder do

  defstruct [
     store_id: nil,
     customer_id: nil,
     status: "open",
     customer_name: nil,
     customer_email: nil,
     address: %{street: nil, street2: nil, city: nil, state: nil, zip: nil, country: nil},
     id: nil,
     key: nil,
     landing: "/",
     message: nil,
     ip: "127.0.0.1",
     items: [],
     history: [],
     invoice: nil,
     payment: nil,
     summary: %{item_count: 0, total: 0.00, subtotal: 0.0},
     logs: [%{entry: "order Created", date: now_iso}],
     discounts: [],
     deliverables: []
   ]

   #...
```

Relational data fans - look away. This will probably make you want to scream... I suppose if you're a document DB fan you'll probably want to scream as well... but stay with me.

This is a rather large struct that defines all kinds of entries where I can tack on data as the customer goes along:

 - "cart" items are tracked in `items[]`
 - payment info (the payment itself and the response) can be tracked in `payment`
 - address information is captured, and the generated `invoice` document as well
 - timestamped logs show what happened when

In short: *everything* is tracked, right here, within a context that makes sense. A **Single Point of Authority** if you will that I can use as the source of all kinds of data later on (sales reports, marketing, etc).

As the customer does things in the store (including check out), I slowly fill out this struct, saving it *en-mass* each time as a JSON blob in Postgres. When a checkout happens, I create the invoice, tack on the payment stuff and reset the status - nothing else.

So, so simple.

## Hand Meet Glove: Using an Agent To Track All of This

When you create a `GenServer` in Elixir you're following a formalized OTP pattern that allows you to keep some form of state on a process - that's their whole reason for existence (and you can Supervise them as well).

These can be long-lived (like our `Catalog`) or short-lived (like a fulfillment process). For semi-complex processes that need to "accrete state" if you will, [an Agent is perfect](http://elixir-lang.org/docs/stable/elixir/Agent.html):

> Often in Elixir there is a need to share or store state that must be accessed from different processes or by the same process at different points in time ... The Agent module provides a basic server implementation that allows state to be retrieved and updated via a simple API.

An Agent is just a GenServer with a few more abstractions, which is nice because writing all of that `handle_call` code can be a bit tiresome. Moreover, updating state is super simple!

The first thing is to change the call from `GenServer.start_link` to `Agent.start_link`:

```elixir
defmodule Peach.Sales do

  import Peach.Util
  alias Peach.Sales.CartItem
  alias Peach.Db.Postgres, as: Db
  alias Peach.Accounting.SalesOrder
  import Plug.Conn

  def start_link(%{key: key} = args) when is_binary(key) do
    order = Db.find_or_create_order(args)
    Agent.start_link fn -> order end, name: {:global, {:order, key}}
  end

  #...
```

*A few notes: I've changed the name of the store to Peach, which is a name I chose at random. I've altered things quite a bit, as you can tell (like working directly with Plug), and I'll go more into this in later posts.*

With this code I'm solving quite a few problems:

 - I'm registering the Agent in the global container, using the `{:order, "SOME STRING"}` tuple (which was [recommended by Ricardo Garcia Vega](http://rob.conery.io/2016/02/20/red4-store-part-4/) - thank you!). This solved a problem [that cropped up in my last post](http://rob.conery.io/2016/02/20/red4-store-part-4/) - using Atoms as names for my process, which you shouldn't do.

 - I'm now able to strip out about 1/2 my code and simplify it with `Agent.get_and_update`

## GenServer Fatigue

When working with `GenServer` you typically abstract the api for your callers, and it looks something like this:

```elixir
defmodule Peach.Sales do
  use GenServer

  #... initialization stuff
  def start_link(args), do: GenServer.start_link(__MODULE__,args)

  def init(args), do: args

  # public API
  def select_item(pid, item),  do: GenServer.call(pid, {:select_item, item})

  def remove_item(pid, sku: sku),  do: GenServer.call(pid, {:remove_item, sku: sku})

  def change_item(pid, sku: sku),  do: GenServer.call(pid, {:change_item, sku: sku})

  # internal GenServer bits
  def handle_call({:select_item, item}, _sender, session) do

  end
  def handle_call({:remove_item, sku: sku}, _sender, session) do

  end
  def handle_call({:change_item, sku: sku}, _sender, session) do

  end

  # privates
end
```

For every public API call you have a corresponding `handle_x` call that is responding to `GenServer`. You can simplify this by using `Agent`:

```elixir
defmodule Peach.Sales do

  #... initialization stuff
  def start_link(args), do: GenServer.start_link(__MODULE__,args)

  # public API
  def select_item(pid, item)  do
    Agent.get_and_update pid, fn(state) ->
      #do something with the state
      {state, state} #first item is the result, second is the new state
    end
  end

  def remove_item(pid, item)  do
    Agent.get_and_update pid, fn(state) ->
      #do something with the state
      {state, state} #first item is the result, second is the new state
    end
  end

  def change_item(pid, item)  do
    Agent.get_and_update pid, fn(state) ->
      #do something with the state
      {state, state} #first item is the result, second is the new state
    end
  end
  # privates
end
```

So much cleaner. But we can improve this even more! Given that storing state is common in each routine, we can centralize it:

```elixir
defmodule Peach.Sales do

  #... initialization stuff
  def start_link(args), do: GenServer.start_link(__MODULE__,args)

  #just return the current state
  def current(pid), do: Agent.get(pid, &(&1))

  # public API
  def select_item(pid, item),  do: current |> do_something |> save
  def change_item(pid, item),  do: current |> do_something |> save
  def remove_item(pid, item),  do: current |> do_something |> save


  # privates
  defp save(state) do
    Agent.get_and_update pid, fn(_state) ->
      #save to the DB
      {state, state}
    end
  end
end
```

This looks so much cleaner doesn't it? The only way it could be better is if we were working with `Plug.Conn`... and which is what I'll do next time!

<div class="ui items" style="padding-top:36px;border-top:1px solid #e5e5e5;">
  <div class="item">
    <div class="image">
      <a href="https://goo.gl/zvMHWK" target=_blank>
        <img src="/img/red4_product_slide.png">
      </a>
    </div>
    <div class="content">
      <a class="header" href="https://goo.gl/zvMHWK">Want to learn Elixir?</a>
      <div class="meta">
        <span>Learn how to build fast, fault-tolerant applications with Elixir.</span>
      </div>
      <div class="description">
        <p>
          This is not a traditional, boring tutorial. You'll get an ebook (epub or mobi) as well as 3 hours worth of tightly-edited,
          lovingly produced Elixir content. You'll learn Elixir <i> while doing Elixir</i>, helping me out at my new fictional job
          as development lead at Red:4 Aerospace.
        </p>
      </div>
    </div>
  </div>
</div>]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/max_smart.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/max_smart.jpg" />
  </entry>
  <entry>
    <title>PostgreSQL Document API Part 2: Full Text Search and Bulk Save</title>
    <link href="https://bigmachine.io/posts/postgresql-document-api-part-2-full-text-search-and-bulk-save" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.728Z</updated>
    <id>https://bigmachine.io/posts/postgresql-document-api-part-2-full-text-search-and-bulk-save</id>
    <summary type="text">In part 1 of this series I setup a nice save function, as well as another function to create an opinionated document storage table on the fly.
This works well and does what&apos;s needed, but we can do so much more. Specifically: I want Full Text Indexing on the fly and bulk saves wit</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2015/08/pg_doc_search.jpg" alt="PostgreSQL Document API Part 2: Full Text Search and Bulk Save" /></p>
In [part 1 of this series](http://bigmachine.io/2015/08/20/designing-a-postgresql-document-api/) I setup a nice save function, as well as another function to create an opinionated document storage table on the fly.
This works well and does what's needed, but we can do so much more. Specifically: *I want Full Text Indexing on the fly and bulk saves within a transaction*.

Let's do it.

## Full Text Search

Our document table has a search field in it that's of type `tsvector`, which is indexed using the GIN index for speed. I want to update that field whenever I save the document, and I don't want to have a lot of API noise when doing it.

So I'll use some convention.

Typically, when creating a Full Text Index, you'll store fields with fairly specific names. Things like:

 - First or Last name, maybe email
 - a Title or Description of something
 - Address Information

I'd like to scan my document when it gets saved to see if it has any keys I might want indexed, and then I want to save them in my `search` field. I can do that with a function which I'll call `update_search`:

```sql
create function update_search(tbl varchar, id int)
returns boolean
as $$
  //get the record
  var found = plv8.execute("select body from " + tbl + " where id=$1",id)[0];
  if(found){
    var doc = JSON.parse(found.body);
    var searchFields = ["name","email","first","first_name",
                       "last","last_name","description","title",
                       "street", "city", "state", "zip", ];
    var searchVals = [];
    for(var key in doc){
      if(searchFields.indexOf(key.toLowerCase()) > -1){
        searchVals.push(doc[key]);
      }
    };

    if(searchVals.length > 0){
      var updateSql = "update " + tbl + " set search = to_tsvector($1) where id =$2";
      plv8.execute(updateSql, searchVals.join(" "), id);
    }
    return true;
  }else{
    return false;
  }

$$ language plv8;
```

Again, I'm using JavaScript to do this (PLV8) and I'm pulling out a document based on ID. I'm then looping over the keys to see if there are any that I might want to store, if there are, I'm pushing to an array.

If we have any hits in that array I'm concatenating the values and saving in the `search` field of the document using `to_tsvector`, which is a built-in Postgres function that takes loose text and turns it into indexable values.

And that's it! Running this, we get the following:

<a href="http://rob.conery.io/img/2015/08/update_search.png"><img src="http://rob.conery.io/img/2015/08/update_search-1024x383.png" alt="update_search" width="1024" height="383" class="alignnone size-large wp-image-530" /></a>

That's perfect - now I can just pop this into my `save_document` function right at the end, and it gets called transactionally whenever I save something:

```sql
create function save_document(tbl varchar, doc_string jsonb)
returns jsonb
as $$
  var doc = JSON.parse(doc_string);
  var result = null;
  var id = doc.id;
  var exists = plv8.execute("select table_name from information_schema.tables where table_name = $1", tbl)[0];

  if(!exists){
    plv8.execute("select create_document_table('" + tbl + "');");
  }

  if(id){
    result = plv8.execute("update " + tbl + " set body=$1, updated_at = now() where id=$2 returning *;",doc_string,id);
  }else{
    result = plv8.execute("insert into " + tbl + "(body) values($1) returning *;", doc_string);
    id = result[0].id;
    doc.id = id;
    result = plv8.execute("update " + tbl + " set body=$1 where id=$2 returning *",JSON.stringify(doc),id);
  }

  //run the search indexer
  plv8.execute("perform update_search($1, $2)", tbl,id);
  return result[0] ? result[0].body : null;

$$ language plv8;
```

## Bulk Saves

Right now I can pass in a single document to `save_document`, but I'd also like to be able to pass in an Array. I can do this by checking the type of the argument, and then pulling things out in a loop:

```sql
create function save_document(tbl varchar, doc_string jsonb)
returns jsonb
as $$
  var doc = JSON.parse(doc_string);

  var exists = plv8.execute("select table_name from information_schema.tables where table_name = $1", tbl)[0];
  if(!exists){
    plv8.execute("select create_document_table('" + tbl + "');");
  }

  //function that executes our SQL statement
  var executeSql = function(theDoc){
    var result = null;
    var id = theDoc.id;
    var toSave = JSON.stringify(theDoc);

    if(id){
      result=plv8.execute("update " + tbl + " set body=$1, updated_at = now() where id=$2 returning *;",toSave, id);
    }else{
      result=plv8.execute("insert into " + tbl + "(body) values($1) returning *;", toSave);

      id = result[0].id;
      //put the id back on the document
      theDoc.id = id;
      //resave it
      result = plv8.execute("update " + tbl + " set body=$1 where id=$2 returning *;",JSON.stringify(theDoc),id);
    }
    plv8.execute("select update_search($1,$2)", tbl, id);
    return result ? result[0].body : null;
  }
  var out = null;

  //was an array passed in?
  if(doc instanceof Array){
    for(var i = 0; i &lt; doc.length;i++){
      executeSql(doc[i]);
    }
    //just report back how many documents were saved
    out = JSON.stringify({count : i, success : true});
  }else{
    out = executeSql(doc);
  }
  return out;
$$ language plv8;
```

The nice thing about working with JavaScript here is that the logic required for this kind of routine is fairly straightforward (as opposed to using PLPGSQL). I've pulled out the actual save routine into its own function - this is JavaScript after all - so I can avoid duplication.

Then I check to see if the passed-in argument is an Array. If it is, I loop over it and call `executeSql`, returning a rollup of what happened.

If it's not an Array, I just execute things as I have been, returning the entire document. The result:

<a href="http://rob.conery.io/img/2015/08/bulk_save.png"><img src="http://rob.conery.io/img/2015/08/bulk_save-1024x413.png" alt="bulk_save" width="1024" height="413" class="alignnone size-large wp-image-527" /></a>

Nice! The best thing about this is that **it happens in a transaction**. I love that!

## Node Weirdness

If only this could work perfectly from Node! I've tried in both .NET and Node and, with .NET, things just work (oddly) using the Npgsql library. With Node, not so much.

Long story short: *the node_pg driver does some weird serialization when it sees an object or array coming in as a parameter*. Consider the following:

```js
var pg = require("pg");
var run = function (sql, params, next) {
  pg.connect(args.connectionString, function (err, db, done) {
    //throw if there's a connection error
    assert.ok(err === null, err);

    db.query(sql, params, function (err, result) {
      //we have the results, release the connection
      done();
      pg.end();
      if(err){
        next(err,null);
      }else{
        next(null, result.rows);
      }
    });
  });
};

run("select * from save_document($1, $2)", ['customer_docs', {name : "Larry"}], function(err,res){
  //works just fine
}
```

This is fairly typical Node/PG code. At the bottom, the run function is set to call my `save_document` function and pass along some data. When PG sees the object come in, it will serialize it to a string and the save will happen fine.

If I send an array, however...

```js
run("select * from save_document($1, $2)",
         ['customer_docs', [{name : "Larry"}, {name : "Susie"}],
         function(err,res){
  //crashes hard
}
```

I'll get an error back saying that I have invalid JSON. The error message (from Postgres) will say it's due to this poorly formatted JSON:

```
{"{name : "Larry"}, ...}
```

Which ... yeah is hideous. I've tried to figure out what's going on, but basically it looks like the node_pg driver is stripping out the outer array - perhaps calling Underscores `flatten` method? I don't know. To get around this you need to change your call to this:

```js
run("select * from save_document($1, $2)",
         ['customer_docs', JSON.stringify([{name : "Larry"}, {name : "Susie"}]),
         function(err,res){
  //Works fine
}
```

## Onward!

This save routine is pretty slick, and it makes me happy. In the next post I'll tackle the finders, and also create a Full Text Search function.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2015/08/pg_doc_search.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2015/08/pg_doc_search.jpg" />
  </entry>
  <entry>
    <title>PostgreSQL Document API Part 3: Finding Things</title>
    <link href="https://bigmachine.io/posts/postgresql-document-api-part-3-finding-things" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.728Z</updated>
    <id>https://bigmachine.io/posts/postgresql-document-api-part-3-finding-things</id>
    <summary type="text">In parts 1 and 2 of this little series I showed various ways to save a document and then update its search field. I also showed how to do a Bulk Saves of many documents transactionally. In this post I&apos;ll explore options for running queries.

A Better Way To Find Documents

In par</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2015/08/pg_press.jpg" alt="PostgreSQL Document API Part 3: Finding Things" /></p>
In parts 1 and 2 of this little series I showed various ways to [save a document](http://bigmachine.io/2015/08/20/designing-a-postgresql-document-api/) and then [update its search field](http://rob.conery.io/2015/08/21/postgresql-document-api-part-2-full-text-search-and-bulk-save/). I also showed how to do a Bulk Saves of many documents transactionally. In this post I'll explore options for running queries.

## A Better Way To Find Documents

In part 1 we designed a bit of an opinionated table, which looks like this:

```sql
create table my_docs(
  id serial primary key,
  body jsonb not null,
  search tsvector,
  created_at timestamptz not null default now(),
  updated_at timestamptz not null default now()
)
```

Since we have control over how the data is stored, we can write our own functions to pull that data out in various fun ways! The hard stuff is behind us (saving, updating, etc) - let's have some fun.

## Pulling A Document By ID

Every document has an `id` field associated with that's managed entirely by the `save_document` function. This is still postgres so every row needs a primary key - and we're planting that key in the document itself. I've set mine up to be an integer key, but you can also do [a Twitter Snowflake `bigint`](http://rob.conery.io/2014/05/29/a-better-id-generator-for-postgresql/) if you want; for now: we go with a serial int.

The function for this is pretty straightforward:

```sql
create function find_document(tbl varchar, id int, out jsonb)
as $$
  //find by the id of the row
  var result = plv8.execute("select * from " + tbl + " where id=$1;",id);
  return result[0] ? result[0].body : null;

$$ language plv8;

select * from find_document('customers',20);
```

*BTW: my syntax highlighter is completely thrown by the SQL/JS stuff, sorry for the weird formatting here*

This is the simplest possible function - it takes the name of the table and the ID you're looking for and does the fastest query possible (which we like!): **a search by primary key**. Speed: *we like it*.

Now lets add one for a *containment* query. For this I want to enter some criteria and have it return the first match to me. This is only valid if *I also order the results*, so I'll do that too and default the `ORDER BY` parameter to be the ID:

```sql
create function find_document(
  tbl varchar,
  criteria varchar,
  orderby varchar default 'id'
)
returns jsonb
as $$
  var valid = JSON.parse(criteria); //this will throw if it invalid
  var results = plv8.execute("select body from " +
                tbl +
                " where body @> $1 order by body ->> '" +
                orderby + "' limit 1;",criteria);
  return results[0] ? results[0].body : null
$$ language plv8;

select * from find_document('customers','{"last": "Conery"}', 'first');
```

There's a bit more to this, and you'll have weird behavior depending on the driver you use. The first thing to notice is that I'm overloading `find_document` because Postgres allows this. This means that the only difference between our first function, which finds by id, and this function is the argument list.

For the Npgsql driver this is no problem. For the node_pg driver, it's a big one. Because I'm defaulting the `orderby` parameter, some confusion creeps in when selecting which function to run. As far as I can tell, the node_pg driver doesn't worry about the types of function arguments, **only the amount of them**. So, if you try to run the "find by id" function above, our second function here will fire.

Again: Npgsql (the .NET driver) doesn't have this issue. So if you have problems just rename one of the functions, or take off the default for the parameter.

Another thing to notice is that I specified the `criteria` parameter as `varcher`. I did this because, while technically incorrect, it makes the API a bit nicer. If I specified it as `jsonb` you would have to run the query thus:

```sql
select * from find_document('customers','{"last": "Conery"}'::jsonb, 'first');
```

Not a huge deal really, since we'll be using this API mostly from code (which I'll go into in the next post).

## Filtering

Let's do the same thing now, but for multiple document returns:

```sql
create function filter_documents(  
  tbl varchar,
  criteria varchar,
  orderby varchar default 'id'
)
returns setof jsonb
as $$
  var valid = JSON.parse(criteria);//this will throw if it invalid
  var results = plv8.execute("select body from " +
                tbl +
                " where body @> $1 order by body ->> '" +
                orderby +
                "'",criteria);

  var out = [];
  for(var i = 0;i &lt; results.length; i++){
    out.push(results[i].body);
  }
  return out;
$$ language plv8;

select * from find_document('customer_docs','{"last": "Conery"}');
```

This one is a bit funkier. My result here is a `setof jsonb`, which means I need to return a bunch of rows of `jsonb`. It's not directly clear how you do this with PLV8, and there may be a better way than I'm doing it - but this is what I found that works.

Once I get the results (which are rows from our document table), I need to loop over that set and push the `jsonb` body field into an array, which I then return.

This works because the `body` field is `jsonb` which, essentially, is text. It's not a JavaScript object because, if it was, I'd get an error (the old [Object object] parsing silliness).

## SQL Injection

Many of you will notice the `orderby` parameter here is concatenated directly in. If you let your clients write SQL in your database then **yes**, there's a problem. But, hopefully, you'll be executing this function from a driver that will parameterize your queries for you so that, something like this:

```js
db.filter("customers", {
  last : "Conery",
  orderBy : "a';DROP TABLE test; SELECT * FROM users WHERE 't' = 't"
}, function(err, res){
  console.log(err);
  console.log(res);
});
```

... won't work. Why not? Because ideally you're making the call like this:

```sql
select * from filter_documents($1, $2, $3);
```

If not, you get what you deserve :).

## Full Text Query

Let's finish this up with a Full Text search on our docs, shall we. This one's my favorite:

```sql
create function search_documents(tbl varchar, query varchar)
returns setof jsonb
as $$
  var sql = "select body, ts_rank_cd(search,to_tsquery($1)) as rank from " +
             tbl +
            " where search @@ to_tsquery($1) " +
            " order by rank desc;"

  var results = plv8.execute(sql,query);
  var out = [];
  for(var i = 0; i &lt; results.length; i++){
    out.push(results[i].body);
  }
  return out;
$$ language plv8;

select * from search_documents('customers', 'jolene');
```

This one is straightforward if you know how Full Text Indexing works with Postgres. Here, we're simply querying the `search` field (which is GIN indexed for speed), which we've updated on every save. This query is lightning fast, and very easy to use.

## Flexing Indexes

In the two functions that take criteria (find and filter), I'm using the [Containment operator](http://www.postgresql.org/docs/current/static/datatype-json.html) (scroll down to section 8.14.3). It's the little `@>` symbol.

This operator is specific to `jsonb` and allows us to use the GIN index we put on the `body` field. This index looks like this:

```sql
create index idx_customers on customers using GIN(body jsonb_path_ops);
```

The special sauce here is the `jsonb_path_ops`. This tells the indexer to optimize for `jsonb` containment operations (basically: *does this bit of jsonb exist in this other bit of jsonb*). This means the index is faster and smaller.

Now, see this is where I would link to a bunch of benchmarks and articles about how PostgreSQL walks all over MongoDB etc when it comes to writes/reads. But that's entirely misleading.

## Read and Write Speed

If you put a single Postgres server up against a single MongoDB server, MongoDB will look rather silly and Postgres will smoke it on almost every metric. This is because that is how Postgres was designed - a "scale up" database, basically.

If you optimize MongoDB and add some servers to handle the load, things start to balance out but then you also get to deal with a horizontal system that [might not do what you think it's supposed to do](https://aphyr.com/posts/322-call-me-maybe-mongodb-stale-reads). This whole thing is highly arguable, of course, but I it's worth pointing out that:

 - Indexing in Postgres slows things down. So if it's slamming write performance you want, you might want to tweak the index to target only what you want indexed (by specifying `(body -> my_field)` in the GIN specification.
 - If you query on something very often (like email), just replicate it to its own column and throw a `UNIQUE` on it. You can handle the synchronization in code or your own function.

In the next post I'll dive into ways you can call this stuff from code!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2015/08/pg_press.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2015/08/pg_press.jpg" />
  </entry>
  <entry>
    <title>PostgreSQL Document API Part 4: Complex Queries</title>
    <link href="https://bigmachine.io/posts/postgresql-document-api-part-4-complex-queries" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.728Z</updated>
    <id>https://bigmachine.io/posts/postgresql-document-api-part-4-complex-queries</id>
    <summary type="text">Storing documents in PostgreSQL is a little easier, now that we have some solid save routines, a way to run Full Text searches, and some basic Find and Filter routines.

This is only half the story, of course. Rudimentary finds might serve application needs, but they&apos;ll never wor</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2015/09/pg_querying.jpg" alt="PostgreSQL Document API Part 4: Complex Queries" /></p>
Storing documents in PostgreSQL is a little easier, now that we have some [solid save routines](http://bigmachine.io/2015/08/20/designing-a-postgresql-document-api/), a way to run [Full Text searches](http://rob.conery.io/2015/08/21/postgresql-document-api-part-2-full-text-search-and-bulk-save/), and some basic [Find and Filter routines](http://rob.conery.io/2015/08/25/postgresql-document-api-part-3-finding-things/).

This is only half the story, of course. Rudimentary finds might serve application needs, but they'll never work over the long term, where we need to ask some deep questions.

## The Source Document

Document storage is a huge subject. How you store a document (and what you store), for me, resolves itself into three areas:

 - Document/Domain Model. Kind of a developer point of view on this, but if you're a DDD fan, this makes sense.
 - The Real World. Invoices, Purchase Lists, Sales Orders - businesses run on these things, let's reflect that.
 - Transaction/Process Result/Event Source. Basically, when "something happens" to your application you track everything that went into that event and store it.

I tend to favor the latter. I'm an information hoarder and when things happen I want to know what/why/when to the *nth* degree.

Here's what I've done in the past to store information about people buying something from Tekpub. This is a document design that I was about to put into production using RethinkDB but never got there (due to the Pluralsight sale).

```json
{
  "id": 1,
  "items": [
    {
      "sku": "ALBUM-108",
      "grams": "0",
      "price": 1317,
      "taxes": [],
      "vendor": "Iron Maiden",
      "taxable": true,
      "quantity": 1,
      "discounts": [],
      "gift_card": false,
      "fulfillment": "download",
      "requires_shipping": false
    }
  ],
  "notes": [],
  "source": "Web",
  "status": "complete",
  "payment": {
    //...
  },
  "customer": {
    //...
  },
  "referral": {
    //...
  },
  "discounts": [],
  "started_at": "2015-02-18T03:07:33.037Z",
  "completed_at": "2015-02-18T03:07:33.037Z",
  "billing_address": {
    //...
  },
  "shipping_address": {
    //...
  },
  "processor_response": {
    //...
  }
}
```

This is a **huge document**. I *love* huge documents! This document is the exact result of all the information moving around during the checkout process:

 - Customer addresses (billing, shipping)
 - Payment info and what was bought
 - How they got here and basic info that happened along the way (in the form of notes)
 - The exact response from the processor (which itself is a big document)

I want this document to be a standalone, actionable item that *requires no other document* to be complete. In other words - from here I want to be able to:

 - Fulfill the order
 - Run some reports
 - Notify the customer of changes, fulfillment, etc
 - Take further steps if required (refunding, voiding)

This document is complete unto itself, and it's lovely!

OK, enough of that, let's right some reports.

## Shaping The Data: The Fact Table

When running analytics it's important to remember two things:

 - **Never run these things on a live system**
 - Denormalization is the norm

Running huge queries over joined tables takes forever, and it amounts to nothing anyway. You should be running reports on *historical* data that doesn't change (or changes very, very little) over time. Denormalizing helps with speed, and speed is your friend with reports.

Given that, we need to use some PostgreSQL goodness to shape our document into a **Sales Fact Table**. A "Fact" table is simply a denormalized bit of data that represents a *fact* in your system - the smallest digestible bit of information about a *thing*.

For us, that *thing* is a sale and we want our fact to look something like this:

<a href="http://rob.conery.io/img/2015/09/fact_result.png"><img src="http://rob.conery.io/img/2015/09/fact_result.png" alt="fact_result" width="755" height="320" class="alignnone size-full wp-image-548" /></a>

*I'm using the [Chinook sample database](https://chinookdatabase.codeplex.com) with some randomized sales data in there that I generated with [Faker](https://github.com/marak/Faker.js/)*.

Each of these records is a single *fact* that I want to rollup on, and all the *dimension* information I want to roll it up with is included (time, vendor). I can add more (category, etc) but this will do for now.

This data is tabular, which means we needed to transform it from the document above. Not an easy task - but much simpler since we're using PostgreSQL:

```sql
with items as (
  select body -> 'id' as invoice_id,
  (body ->> 'completed_at')::timestamptz as date,
  jsonb_array_elements(body -> 'items') as sale_items
  from sales
), fact as (
  select invoice_id,
  date_part('quarter', date) as quarter,
  date_part('year', date) as year,
  date_part('month', date) as month,
  date_part('day', date) as day,
  x.*
  from items, jsonb_to_record(sale_items) as x(
    sku varchar(50),
    vendor varchar(255),
    price int,
    quantity int
  )
)

select * from fact;
```

This is a set of Common Table Expression (CTEs), chained together *in a functional* way (more on this below). If you've never used CTEs they can look a little weird... until you squint and you realize you're just chaining things together with a name.

In the first query above, I'm pulling the `id` of the sale out and calling it `invoice_id`, and then pulling out the timestamp and converting it to a `timestamptz`. Simple stuff for the most part.

The thing that's a bit tricky here is `jsonb_array_elements` - this is pulling the items array out of the document and creating a record for each item. So, if we had only a single sales document in our database with three items and we ran this query:

```sql
select body -> 'id' as invoice_id,
(body ->> 'completed_at')::timestamptz as date,
jsonb_array_elements(body -> 'items') as sale_items
from sales
```

Instead of one record representing the sale, we'd get 3:

<a href="http://rob.conery.io/img/2015/09/jsonb_array_elements.png"><img src="http://rob.conery.io/img/2015/09/jsonb_array_elements.png" alt="jsonb_array_elements" width="518" height="108" class="alignnone size-full wp-image-549" /></a>

Now that we've "elevated" the items, we need to "spread them out" into their own columns. This is where the next bit of trickery comes in with `jsonb_to_record`. We can use this function along with a type definition on the fly:

```sql
select * from jsonb_to_record(
  '{"name" : "Rob", "occupation": "Hazard"}'
) as (
  name varchar(50),
  occupation varchar(255)
)
```

In this simple example I'm converting some `jsonb` into a table - I just have to tell PostgreSQL how to do it. That's what we're doing in the second CTE ("fact") above. We're also using `date_part` to transform dates.

This give us a fact table, which we can save to a view if we like:

```sql
create view sales_fact as
-- the query above
```

You might be wondering if this query is *dog slow*. In fact it's quite fast. This isn't supposed to be some kind of benchmark or anything - just a relative result to show you that this query, is in fact, rather speedy. I have 1000 test documents in my database, running this query on all the documents comes back in about a 10th of a second:

<a href="http://rob.conery.io/img/2015/09/fact_speed.png"><img src="http://rob.conery.io/img/2015/09/fact_speed.png" alt="fact_speed" width="723" height="432" class="alignnone size-full wp-image-550" /></a>

PostgreSQL. Cool stuff.

Now we're ready for some rollups!

## Sales Reports

From here it's mostly gravy. You just rollup on what you like, and if you forget something add it to your view - best part is you don't have to worry about joins! It's just transformation of data, *which is really fast*.

Let's see our top 5 sellers:

```sql
select sku,
  sum(quantity) as sales_count,
  sum((price * quantity)/100)::money as sales_total
from sales_fact
group by sku
order by salesCount desc
limit 5
```

This one comes back in 0.12 seconds. Pretty fast for a 1000 records.


## CTEs and Functional Querying

One of the things I really like about RethinkDB is its query language, ReQL. It's inspired by Haskell (according to the team) and is all about *composition* (emphasis mine):

> To grok ReQL, it helps to understand functional programming. Functional programming falls into the declarative paradigm in which the programmer aims to describe the value he wishes to compute rather than prescribe the steps necessary to compute this value. Database query languages typically aim for the declarative ideal since this style gives the query execution engine the most freedom to choose the optimal execution plan. But while SQL achieves this using special keywords and specific declarative syntax, ReQL is able to **express arbitrarily complex operations through functional composition**.

As you've seen above, we can approximate this by using CTEs chained together, each transforming the data in a specific way.

## Summary

There's a lot more I could write - but let's just wrap this up by saying that you can do *everything other document systems can do* and a whole lot more. [PostgreSQL's querying abilities are very powerful](http://rob.conery.io/2015/02/24/embracing-sql-in-postgres/) - there's very little you can't do and, as you've seen, the ability to transform your documents into a tabular structure greatly helps things.

And with that this little series of posts is complete.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2015/09/pg_querying.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2015/09/pg_querying.jpg" />
  </entry>
  <entry>
    <title>Hello Elixir. Wow.</title>
    <link href="https://bigmachine.io/posts/hello-elixir-wow" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.728Z</updated>
    <id>https://bigmachine.io/posts/hello-elixir-wow</id>
    <summary type="text">I don&apos;t know anything about elixir but I very much want to learn it. I like learning new things - I feel it&apos;s required for our industry. It&apos;s easy to feel a twinge of snark about this, I feel that too, every day. But every now and again something comes along and I just find mysel</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2015/09/elixir_1.jpg" alt="Hello Elixir. Wow." /></p>
I don't know anything about elixir but I very much want to learn it. I like learning new things - I feel it's required for our industry. It's easy to feel a twinge of snark about this, I feel that too, every day. But every now and again something comes along and I just find myself getting pulled right in.

I know nothing about Elixir short of 6 chapters I read in a book over the weekend and some goofing around today. If you want to learn along with me, hurrah! Let's go.

## Why Is Elixir Exciting?

For me it's both a positive and a negative reason. I remember the joy I felt when Ruby/Rails hit the scene, dragging me away from .NET. Five or so years later, Node came along and made life so much simpler - moving away from the monolithic web monster that's so easy to create with Rails.

But Node is JavaScript, and to be honest my programming skills have eroded **tremendously** since I started doing Node full time over the last 4 years or so. I found this out when I tried to write some C# a few weeks ago. Ugh.

C# introduced me to some very interesting programming concepts, which I further exercised with Ruby and then flat-out ditched while working with JavaScript. Sigh.

Elixir is exciting to me because it's fast, fast, my god **it's fast** and has many of the nice facilities that Ruby does - with the power of Erlang behind it.

I like this.

## Can I Build Something Right Now?

For most languages that you're learning the answer is a flat "no". Getting set up takes some time (Java, C#, Haskell, etc). With Elixir you just install it and you can write some code:

```
brew update &amp;&amp; brew install elixir
```

_This is installing elixir using Homebrew on a Mac. [other installation options are here](http://elixir-lang.org/install.html)_

Within 30 seconds you have Elixir (and Erlang) on your machine. You can see this right now:

```
iex
```

That opens up the Elixir REPL. Type something:

```
iex> x = "Rob"
Rob
iex> x
Rob
```

You just wrote some Elixir. Nicely done! For me that's the first victory - 0 to code in under 5 minutes. Nice!

## Mix, NPM For Elixir

Sort of. Mix is a "project and build" manager for Elixir. It's a little bit of Rake, a little NPM - kind of in-between. Mix will create a new project for you - which you can think of as both a package and an executable (this is a command line command, not an iex command):

```
mix new hello_elixir
* creating README.md
* creating .gitignore
* creating mix.exs
* creating config
* creating config/config.exs
* creating lib
* creating lib/hello_elixir.ex
* creating test
* creating test/test_helper.exs
* creating test/hello_elixir_test.exs
```

This is so, so nice. It's created a tight little package structure for us, with a README, a .gitignore, a test directory (ready for tests!) and what's obviously an entry point with a `mix.exs` file. Even a central place for "config stuff"!

This is brilliant. It's also heavily commented so you can read through and have a basic understanding of what's going on.

## Something Useful

If you want to skip right ahead - go for it. [I put all the code on Github](https://github.com/bigmachine-io/bigmachine-membership) and will be tweaking it over the coming weeks.

So I could step through and do some silly crap where I output "Hello World" - but I'd rather do something more fun. **How about we connect to a database and execute a query**. Great idea - let's create a new Mix project called "Membership":

```
mix new membership
* creating README.md
* creating .gitignore
* creating mix.exs
* creating config
* creating config/config.exs
* creating lib
* creating lib/membership.ex
* creating test
* creating test/test_helper.exs
* creating test/membership_test.exs
```

I want to create a harness around my [pg_auth](https://github.com/robconery/pg-auth) project. It's a set of Postgres tables, functions, etc for handling membership in your application. I want to see how hard it is to execute a query, and WTF is going on here.

Yes I could read the book more, _but I much prefer to actually do something_, learning as I go.

The first thing to find out is whether there's a PostgreSQL driver. Elixir has been around for a while, so I'm fairly certain there is one, and [yes indeed there is](https://github.com/ericmj/postgrex).

Now, I need to figure out how to get that project into my new project. The instructions are right on the main page of the Postgrex project: so open up `mix.exs` and edit the dependencies as well as which applications we'll be using inside our own:

```elixir
def application do
  [applications: [:logger, :postgrex]]
end

#...

defp deps do
  [{:postgrex, "~> 0.9.1"}]
end
```

The bottom directive says that we want the `postgrex` package, the top directive says we want to "mount it" in our app... I think. Still a bit hazy on what exactly is going on here.

The next step is to make sure the dependencies are installed:

```
mix deps.get
```

This goes out to "hex.pm" - kind of like npmjs.org - it's where the package bits for Elixir are stored online. It will look for the `postgrex` package (version 0.9.1) and download it - creating a new directory called `deps` in our project.

Have a look in there (after you run this command). _It's the source_. Which I think is quite nice. It is, essentially, NPM's "node_modules" approach - grab the source and stick it in your project so you can review etc. This _seems_ to be a single-level dependency graph, unlike node_modules, which recursively grabs the entire internet just in case.

Anyway - we have what we need here - access to a database.

## The Membership Module

Let's write some code. I have a function called "membership.register" that I want to call, and I want to return the results somehow. For this I'll crack open `membership_test.ex` and have a look at the test structure:

```elixir
defmodule MembershipTest do
  use ExUnit.Case

  test "the truth" do
    assert 1 + 1 == 2
  end
end
```

Gloriously sparse. It's not hard to figure out what's going on here - especially if you know Ruby. We're defining a module with a `do` block (yay!), bringing in some help from the ExUnit test library, and then writing a wonderfully terse test.

The fun Ruby vibes are setting in. This is exciting.

Let's write our test and watch as everything explodes:

```elixir
defmodule MembershipTest do
  use ExUnit.Case

  test "Registration succeeds with valid credentials" do
    {:ok, res} = Membership.register({"test@test.com", "password"})
    assert res.success,res.message
  end

end
```

This is stretching what I know about Elixir - but basically I have a tuple on the left there, that's being "matched" with the result of the `Membership.register/1` on the right.

We're now in the new language weeds. Here's what I know so far about this.

Every function in Elixir works on the concept of "pattern matching". You don't just call a function or "send a message" as you do in Ruby (though those ideas work too). You try to match things on both the left and right side of the assignment.

On the left I have a tuple with the first element being an "atom". You can think of this exactly as a Ruby symbol. The second element is any kind of data coming from the function result. This is a different kind of thing for me, but I think I get it and I'm going to punt on talking about that more because, simply, I'm just not sure what I can really do with it.

OK, let's write our `register` routine:

```elixir
defmodule RegistrationResult do
    defstruct success: false, message: nil, new_id: 0
end

defmodule Membership do

  def register({email, password}) do
    {:ok, pid} = Postgrex.Connection.start_link(database: "bigmachine")
    sql = "select * from membership.register($1, $2);"
    {:ok, res} = Postgrex.Connection.query(pid, sql, [email, password])
    [record | last] = res.rows
    [new_id, validation_token, auth_token, success, message] = record
    {:ok, %RegistrationResult{success: success, message: message, new_id: new_id}}
  end

end
```

This is going to look very, very strange to you if you've never seen Elixir. It might also look incredibly strange to you even if you have! _I'm very new to this stuff_.

In the first lines up there I'm defining a struct to hold my result. I don't need to do this, but it's a nice way to package up results. I could use a `Dict`, `Hash`, or `Map` (or even a tuple) - but I like working with dot notation and you can do that with a struct.

In the register function I'm accepting a tuple (which is a good thing to do - a single argument that is flexible and expandable) and then opening the connection, matching it against a tuple to hold the `pid`.

That `pid` is not an operating system pid, it's Process pid built into elixir which is pretty mind-twisty. It's like having a little message queue all your own right inside your runtime. I don't know nearly enough to talk more about it, but I'm excited to get there.

OK, the next lines are pretty clear - I create a SQL string, pass the params, and execute using a pattern match on `{:ok, res}` where `res` is a struct for handling results.

Now that I have the result, I need to peel off the "head" of the rows array. This is where I think my code is pretty damn messy and I'm sure there's some better way - but for the sake of learning and trying to get something done I hacked the crap out of it.

The deal is that you can ask for the "head" and "tail" of a list by using this notation:

```elixir
[h | t] = [1,2,3,4]
```

Here, `h` will equal "1" and `t` will equal "[2,3,4]". You might be thinking "WTF? Why?" like I did, then you get to see some recursive action and it will blow your mind. Again: I need to know more about that before I start writing anything.

So, the record I want is the first element of the return array - that means I need to pattern match against a list of variables, which will hold those values. Weird notation, but I think it's pretty interesting.

Finally I kick up a new struct and assign the results, making sure to pass the `:ok` atom first thing.

## Testing This Properly

This took me a while to figure out. I wanted to write a test that executed the `register` function once so I could write a set of asserts on the result - I don't want to call `register` multiple times.

I tried my Ruby approach, using module-level variables and assigning them in the `before` block, which didn't work. Turns out it's much simpler:

```elixir
defmodule MembershipTest do
  use ExUnit.Case

  setup do
    {:ok, pid} = Postgrex.Connection.start_link(database: "bigmachine")
    Postgrex.Connection.query!(pid, "delete from membership.users", [])
    {:ok, res} = Membership.register({"test@test.com", "password"})
    {:ok, [res: res]}
  end

  test "Registration with valid credentials", %{res: res} do
    assert res.success,res.message
  end

end
```

There's a setup "macro" (don't know what those are yet) that does what you expect. I open a connection, drop the users, and then run my registration function.

This is where things get neat. Every `setup` block can pass along a "context" to each test. It's just the result of `setup` - which is the very last line. The result has to be in the form of something like this:

```
{:ok}
{:ok, [key: value, another_key: another_value]} #a Dict
```

As you can see, Elixir has the convention of returning an atom in the fist place of a tuple to "define" what the tuple represents. If things are all good, `:ok` is returned. You can use anything in the first position - like `:person`, `:refund`, or `:err`. It's simply a convention - just like Node's callback structure `(err, res, next)`.

Where I got into trouble was trying to send back a raw result (my `RegistrationResult ` struct) from the `register` function - I kept getting an error about my `RegistrationResult`:

```
** (Protocol.UndefinedError) protocol Enumerable not implemented for %RegistrationResult
```

I got this error because the `setup` routine was trying to treat it like a `Dict` (dictionary) - which is enumerable (that's what `Enum` is, an enumeration module).

OK anyway I finally figured out I could send the result directly to my tests by using this:

```elixir
    {:ok, res} = Membership.register({"test@test.com", "password"})
    {:ok, [res: res]}
```

That meant I could structure my test this way:

```elixir
  test "Registration with valid credentials", %{res: res} do
    assert res.success,res.message
  end
```

And it worked. **My god it worked**.

## Why I Put Wow In The Title

Elixir is intimidating to me. I suck as a programmer and really, I'm kind of a hack. But in about 3 hours I was able to take what I read in a book, connect to a database **without a framework** and execute something in a rather elegant way.

Wow. This made me quite happy today. We'll see about tomorrow.

<div class="ui items" style="padding-top:36px;border-top:1px solid #e5e5e5;">
  <div class="item">
    <div class="image">
      <a href="https://goo.gl/zvMHWK" target=_blank>
        <img src="/img/red4_product_slide.png">
      </a>
    </div>
    <div class="content">
      <a class="header" href="https://goo.gl/zvMHWK">Want to learn Elixir?</a>
      <div class="meta">
        <span>Learn how to build fast, fault-tolerant applications with Elixir.</span>
      </div>
      <div class="description">
        <p>
          This is not a traditional, boring tutorial. You'll get an ebook (epub or mobi) as well as 3 hours worth of tightly-edited,
          lovingly produced Elixir content. You'll learn Elixir <i> while doing Elixir</i>, helping me out at my new fictional job
          as development lead at Red:4 Aerospace.
        </p>
      </div>
    </div>
  </div>
</div>]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2015/09/elixir_1.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2015/09/elixir_1.jpg" />
  </entry>
  <entry>
    <title>Using Elixir&apos;s Pattern Matching And Case Statement To Handle Errors</title>
    <link href="https://bigmachine.io/posts/using-elixirs-pattern-matching-and-case-statement-to-handle-errors" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.728Z</updated>
    <id>https://bigmachine.io/posts/using-elixirs-pattern-matching-and-case-statement-to-handle-errors</id>
    <summary type="text">I don&apos;t really know what I&apos;m doing. I&apos;m trying to learn Elixir and I&apos;m having so much fun doing it that I thought I would share what I&apos;m learning. So ... here goes. The code for the stuff I&apos;m writing is up at Github - feel free to drop over.

In my last post, Ayende Rahien mad a </summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2015/09/learn_elixir.jpg" alt="Using Elixir&apos;s Pattern Matching And Case Statement To Handle Errors" /></p>
I don't really know what I'm doing. I'm trying to learn Elixir and I'm having so much fun doing it that I thought I would share what I'm learning. So ... here goes. The code for the stuff I'm writing is [up at Github](https://github.com/bigmachine-io/bigmachine-membership) - feel free to drop over.

[In my last post](http://bigmachine.io/2015/09/03/hello-elixir-wow/), Ayende Rahien mad a great comment:

<a href="http://rob.conery.io/img/2015/09/ayende_comment.png"><img src="http://rob.conery.io/img/2015/09/ayende_comment.png" alt="ayende_comment" width="895" height="482" class="alignnone size-full wp-image-566" /></a>

Excellent question (of course)! At the time I was just hacking things and wanted to see a result - I didn't worry too much about errors.

That said, let's see how we can handle this better. And please, if you know of a better please share in the comments!

## Case And Pattern Matching

A better way to execute the query for our call would be to use `case`:

```elixir
def register({email, password}) do
  {:ok, pid} = Membership.connect()
  sql = "select * from membership.register($1, $2);"

  case Postgrex.Connection.query(pid, sql, [email, password]) do
    {:ok, res} ->
      cols = res.columns
      [first_row | _] = res.rows
      [new_id, validation_token, auth_token, success, message] = first_row
      {:ok, %RegistrationResult{
        success: success,
        message: message,
        new_id: new_id,
        authentication_token: auth_token,
        validation_token: validation_token
    }}

    {:error, err} -> {:error, err}
  end
end
```

Case statements are pretty standard for programming languages, and this one essentially operates the same way. Here, we're evaluating the return of the query and matching against that return. If all is OK, the `Postgrex` driver will match against `{:ok, %Postgrex.Result}`, otherwise it will match against `{:error, %Postgrex.Error}`.

This matching thing is really head-twisty. The nice part is, however, that we only need to think about the atoms (`:ok` and `:error`) here, Elixir will see that the second argument is a variable and will bind to it - making the match work out.

Crazy stuff.

So, back to the `case` statement. If a match is made on `:ok`, I'll return the struct I was returning before using an anonymous function (the `->` operator). If things aren't OK and we match on `:error`, I'll just pass the error on.

## Using This

I can now use this in my test for a cleaner error response if something goes wrong, or just handle it as needed, again, using a `case` statement:

```elixir
setup do
  {:ok, pid} = Postgrex.Connection.start_link(database: "bigmachine")

  case Membership.Registration.register({"test@test.com", "password"}) do
     {:ok, res} -> {:ok, [res: res]}
     {:error, err} -> raise err
  end

end

test "Registration with valid credentials", %{res: res} do
  assert res.success,res.message
end
```

This `statement` evaluates what comes back from `register` and if it's OK, returns a tuple that gets set as the test context, which I use down below.

If things aren't OK, I raise. I could log here as well, or do other things - for now raising works.

Also - I should mention if you haven't figured it out that the last line in any function is the return value, just like in Ruby.

Case statements return values - you can bind a variable if you like or you can have them as the last operation, as I'm doing here.

<div class="ui items" style="padding-top:36px;border-top:1px solid #e5e5e5;">
  <div class="item">
    <div class="image">
      <a href="https://goo.gl/zvMHWK" target=_blank>
        <img src="/img/red4_product_slide.png">
      </a>
    </div>
    <div class="content">
      <a class="header" href="https://goo.gl/zvMHWK">Want to learn Elixir?</a>
      <div class="meta">
        <span>Learn how to build fast, fault-tolerant applications with Elixir.</span>
      </div>
      <div class="description">
        <p>
          This is not a traditional, boring tutorial. You'll get an ebook (epub or mobi) as well as 3 hours worth of tightly-edited,
          lovingly produced Elixir content. You'll learn Elixir <i> while doing Elixir</i>, helping me out at my new fictional job
          as development lead at Red:4 Aerospace.
        </p>
      </div>
    </div>
  </div>
</div>]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2015/09/learn_elixir.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2015/09/learn_elixir.jpg" />
  </entry>
  <entry>
    <title>Inserting And Using A New Record In Postgres</title>
    <link href="https://bigmachine.io/posts/inserting-using-new-record-postgres" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.727Z</updated>
    <id>https://bigmachine.io/posts/inserting-using-new-record-postgres</id>
    <summary type="text">A Problem Postgres Can Solve Easily

Let&apos;s say you need to insert a record into a table and then use that record immediately to push data into another table. This happens a lot with parent/child relationships (think about users and roles - add a user, add to a role, and log it - </summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2015/02/ctes_rock.jpg" alt="Inserting And Using A New Record In Postgres" /></p>
## A Problem Postgres Can Solve Easily

Let's say you need to insert a record into a table and then use that record immediately to push data into *another table*. This happens a lot with parent/child relationships (think about users and roles - add a user, add to a role, and log it - 3 queries!) and, typically, developers will shove that into the app layer and let Rails/Node/ASP.NE T execute multiple queries.

But you don't have to! Just know you some SQL and you're good to go.

## Common Table Expressions

These are not new and have been around for a while - and yes SQL Server fans you can do what you're about to see in SQL Server, however you won't have the syntactic niceties.

First, let's create some tables in a demo database:

```sql
create table things(
  id serial primary key,
  name varchar(50)
);

create table thing_logs(
  id serial primary key,
  thing_id int not null references things(id),
  entry varchar(50)
);
```

In the example above you can see the use of `serial`, a great shortcut for creating a primary key. This is example is pretty stupid, but hopefully it gets the point across.

Let's insert some data with a common table expression. I'll insert the record, pull the record out, and insert the new values into the logs table:

```sql
with inserted as (
  insert into things(name)
  values('Rob') returning *
)
insert into thing_logs(thing_id,entry)
select inserted.id, 'New thing created: ' || inserted.name from inserted;
```

If you've never used them, Common Table Expressions (CTEs) can look quite foreign at first. Basically, they create a table on the fly that you can then query against. Here I'm using Postgres syntactic shortcut for returning the inserted record: `returning *`. If I only wanted the id I could use `returning id`.

I wrap in a `WITH inserted` and I can use it in a subsequent query (I can call it whatever I like). In this case I drop an entry into the `thing_logs` table.

But why would I do this?

Two reasons: *performance* and *transactions*. In the above query performance doesn't really matter, but as your database grows you'll want to streamline the connections to the database and make them as reasonable as you can.

Transactions are something many developers just don't think about until they realize they're not trapping all the data they should. In this case we're only executing two writes - but in the real world you might have 5 or 6. Perhaps when a user is created. 

You can use CTEs for this as well (UPDATE: I originally wrapped this with a BEGIN/COMMIT but CTEs, even chained like this, encompass a transaction):

```sql
with new_user as(
  insert into users(email,created_at, password)
  values('test@test.com',now(),'hashed_password')
  returning id
), role_assignment as(
  insert into users_roles(user_id, role_id)
  select new_user.id, 10 from new_user
  returning user_id
), log_entry as(
  insert into user_logs(user_id, entry)
  select role_assignment.user_id, 'Added to system' from role_assigment
  returning user_id
)
--return the new record with the assigned role for niceties
select users.email, users.created_at, roles.name
from users 
inner join users_roles on users.id = users_roles.user_id
inner join roles on roles.id = users_roles.role_id
where users.id=(select user_id from log_entry);
```

This example shows a *killer feature* of CTEs - they're chainable. I've also wrapped everything inside a `BEGIN` and `COMMIT` so if there's an error here for whatever reason, nothing gets written.

At the end there I'm returning the new `users` record along with the role assignment -but you can return whatever your application needs data-wise. This is why we SQL people!

Of course there are different ways of doing this (functions come to mind) but sometimes the syntactic niceties of Postgres make writing queries like this quite fun.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2015/02/ctes_rock.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2015/02/ctes_rock.jpg" />
  </entry>
  <entry>
    <title>It&apos;s Time To Get Over That Stored Procedure Aversion You Have</title>
    <link href="https://bigmachine.io/posts/its-time-to-get-over-that-stored-procedure-aversion-you-have" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.727Z</updated>
    <id>https://bigmachine.io/posts/its-time-to-get-over-that-stored-procedure-aversion-you-have</id>
    <summary type="text">There is a lot of opinion about stored procedures out there that are just... </summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2015/02/sp_pain3.jpg" alt="It&apos;s Time To Get Over That Stored Procedure Aversion You Have" /></p>
In the .NET world (and beyond), data access is a cluster-fucked echo chamber of half-assed rocket engineering and cargo cultism based on decade-old cathedralized thinking and corporate naval -gazing.

_(Cracks Knuckles...)_

The thought vacuum you encounter when discussing databases and data access is (most of the time) a full-speed face grind. Reminds me of the time in college when I woke up in the middle of the night at a friend's house trying to find the bathroom after a few beers and walking nose-first into the wall: _empty, painful, dark_.

_Damn I am sick of this conversation._

The mental paralysis surrounding Stored Procedures and Functions in your database is hurting you. These things exist for a reason: your database and all of its amazing amazingness is there to _help you_. To **love you** and your data! You need to let this love flow.

Let's move on from this medieval database thinking. Let's embrace SQL and Stored Procedures for what they are, and what they do: _kick ass_.

## Business Logic

Developers don't want Stored Procedures and Functions in their database because _Business Logic in database bad_. This is what they've read in blogs, read on Twitter, heard from a rockstar developer at a conference. This is what _someone else told them_ and it's simply gone too far. **Stored Procedures are not inherently bad** - you've been told this because someone thinks you're not capable of thinking for yourself.

Think on this: **what is Business Logic**? I've had this conversation often with fellow developers and I can say that it's a bit like asking "[so ... what is REST anyway](http://bigmachine.io/2012/02/28/someone-save-us-from-rest/)".

To me, I think you take all the code you write for displaying things to a user, shoving things into a database, caching, configuration and optimization and scrape it away. What you have left is your Business Logic (more or less). At least it's _what you think is Business Logic_. It's really not. Not entirely.

It's your lovely Domain Model that is bent to serve Entity Framework or ActiveRecord or [whatever ORM you're using]. It's the thing you wanted to create, that you tried to create using all of the lovely OO principles you know but eventually had to corrupt and abandon because ORMs are hell and for some reason you can't see just what that ORM has done to your codebase and _OH MY GOD GET THAT STORED PROCEDURE AWAY FROM ME_.

I once read a post by a Rails fanatic deep in the heat of Rails fever describe referential integrity and database normalization as "business logic". Evidently ActiveRecord/ActiveSupport has everything you need to protect your data! At the time I was getting to know and love Rails, and I remember thinking _this person is trying to scare me away from Rails_. My next thought was _I need a beer and a hammer_ OH MY GOD.

You build applications to generate data. That data is your value, that value brings in the money. There is nothing else and if you think safeguards for this data belong anywhere else other than cuddled up next to your data with a machine gun and flamethrower you're out of your fucking mind.

That ORM you're using is destroying your ability to clearly see, feel and love your data and how it is stored.

This mentality: that the database is there to "just hold data" is confoundingly irritating. I say this because I have confoundingly irritated myself by falling into this thinking (with Rails and other platforms) - letting ORMs handle my data needs. ORMs aren't there to protect your data, they exist to ruin your life and destroy what good feelings you have left about your career - that's about it.

## Business Logic In Your Database

There are good reasons to exclude Business Logic from your database, including:

- SQL is not intelligent enough to describe a business processes. You can do it, but it's extremely crap-filled.
- A database is for handling data, not executing higher-level logic
- It's difficult to test SQL

All of these are very valid and I agree with each of them. So let's not do that OK? Now that we agree - **onward**.

## Data Logic vs. Business Logic

Consider the application you're writing right now. It very likely has the concept of a User and you want to remember who that User is. If not, let's just pretend OK?

In Rails if you need membership for your site (logins, passwords, etc) you have a mess of 3rd party gems to use. They're so good at what they do it's ridiculous to write your own - something I've said to many developers who have tried to write their own membership system: "you're prone to make the same mistakes and end up in the same place as Devise. You're not a unique snowflake, just use it".

This is because Devise (and libraries like it) do pretty much the same thing: register users, authenticate them, role management, etc. and they do it well. The people who built Devise (and libraries like it) encountered every problem you will encounter building your own, and they've done a pretty good job.

Membership is what you call a **horizontal concern**. In every application you write with users you want to remember, you'll need this kind of thing and it doesn't change.

So: _is this business logic_? I don't think so. That's a bit of a bold statement but it has a bold bit of reasoning: _if every application needs it, why is it unique Business Logic to your app?_. Answer: **it's not**.

I bring this up because there are decisions and executions that are unique to your business that belong in code. Once those decisions are reached, they need to be remembered. **This is your data logic** - the remembering, the writing it to disk after all decisions have been reached!

Let's see an example of what I mean.

## A Database Membership System

One day I decided to test-drive some crazy ideas regarding databases and membership systems and I fired up Postgres to see if I could emulate Devise with a set of functions. My thinking was this: **if I'm going to let a gem do this for me, why not a set of functions I can debug?**. In other words: **if all of this is hidden from me in a gem, why can't it be hidden from me in the database?**.

I can write the code that validates inputs, makes basic decisions about who can enter when, why, etc (even charging money) but when the time comes - I'll need to write their record to the system. Again: **Data Logic**.

The first step was to add hashing to my database (this is psql in action below):

```sql
 create database crazytalk;
 \c crazytalk;
 create extension pgcrypto;
```

[pgcrypto](http://www.postgresql.org/docs/8.3/static/pgcrypto.html) is a set of hashing and encryption functions that allow you to do things like hash a password using blowfish (bcrypt) - the same hashing algorithm that Devise uses.

Long story short, I created it over the course of 3 weeks - a set of 12 different functions and 9 tables. I love it, dearly.

The main reason is it goes well, well beyond registering and logging people in. It handles the notion of sessions, logging and activity, notes, roles, and a [kickass way of handling id generation](http://rob.conery.io/2014/05/29/a-better-id-generator-for-postgresql/).

I still have a few things to do with it - but my goal was satisfied: **can I create a full database implementation of a membership system without wanting to throw up?**. And yes, I can. But that's me, [I love Postgres](http://rob.conery.io/category/postgres/) and I love thinking about all the data I'll need in the years to come **and how I'll protect it**.

No it doesn't send emails or any crazy crap like that - _it just moves data around_ efficiently, transactionally. Just like Devise, but better.

## Stored Procedures, Functions, Whatever Get Over It

The main thing I came away with is just how powerful these functions can be. And with power comes the lure of abuse, of course, but let's just have a think on it.

When a new User registers into your system, what happens? Do you know? In mine a number of things go off:

- The user record is written
- A log entry is created
- A note is added describing when/how/where
- A role is assigned
- A mailer is prepared and the mailer record attached to the user (a Welcome! email)
- A validation token is created and two separate logins (auth token and local user/password)

If I was to use an ORM, this would be a lot of writes. If I was careless (or using Rails - take your pick) I wouldn't put this into a transaction, which it very much should be.

Do you think the above is Business Logic? I don't - it's simple execution of writing some data which is quite easily facilitated by Postgres:

```sql
CREATE OR REPLACE FUNCTION register(login varchar(50), email varchar(50), password varchar(50), ip inet)
returns TABLE (
  new_id bigint,
  message varchar(255),
  email varchar(255),
  email_validation_token varchar(36)
)
AS
$$
DECLARE
  new_id bigint;
  message varchar(255);
  hashedpw varchar(255);
  validation_token varchar(36);
BEGIN

  --hash the password using pgcrypto
  SELECT crypt(password, gen_salt('bf', 10)) into hashedpw;

  --create a random string for the
  select substring(md5(random()::text),0, 36) into validation_token;

  --create the member. Email has a unique constraint so this will
  --throw. You could wrap this in an IF if you like too
  insert into members(email, created_at, email_validation_token)
  VALUES(email, now(), validation_token) returning id into new_id;

  --set the return message
  select 'Successfully registered' into message;

  --add login bits to logins
  insert into logins(member_id, provider, provider_key, provider_token)
  values(new_id, 'local',email,hashedpw);

  --add auth token to logins
  insert into logins(member_id, provider, provider_key, provider_token)
  values(new_id, 'token',null,validation_token);

  -- add them to the members role which is 99
  insert into members_roles(member_id, role_id)
  VALUES(new_id, 99);

  --add log entry
  insert into logs(subject,entry,member_id, ip, created_at)
  values('registration','Added to system, set role to User',new_id, ip, now());

  --return out what happened here with relevant data
  return query
  select new_id, message, new_email, success, validation_token;

END
$$ LANGUAGE plpgsql;
```

This is a Stored Procedure. I wrote it, and I love every part of it. It writes the data I need after I've decided the User is good to go - _after my app has made the decision to let them in_. No additional logic or decisions here, just a bunch of writes executed in a transaction - precisely what a Stored Procedure is good for.

My goal with this exercise is not programmatic purity. **My goal is data correctness and accuracy** which I hope is your goal too. That should be everyone's goal! Your app is there to produce data, no one gives a damn about your code but you.

## Of Course You Can Take This Too Far

That's the argument I hear most often: _it's a slippery slope to keep dumping logic in your database!_. Sigh.

Once you start writing code you're surrounded by slippery slopes falling away from Slick Demo Mountain - I don't care what the platform or tool is. The only thing you can do is to study up and know what tools do what, and what they're good for and how they're could hurt in the end.

postgres as it turns out, is an excellent transactional data engine (and yes, SQL Server as well). What it's not so good at is providing a descriptive language that I can screw myself into a corner with (PLPGSQL is brutal).

Why do I need to monkey with an ORM when I can let Postgres elegantly handle this for me with SQL that's pretty damn simple to understand? Are `SELECT` statements and parameters really that scary?

I don't understand the mentality of spending a twice the time to learn an ORM rather than learning the SQL of your current database system. You _think_ you're moving faster in the beginning, but as time goes on debugging that ORM will change your mind... and then you try to rip it out and end up writing a blog post late at night ranting about ORMs and then...

I want to vault every ORM into the heart of the sun or, preferably, go back in time and smash all the computers responsible for their genesis. **It's 2015, let's wake the fuck up to the power of SQL and our relational systems.**]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2015/02/sp_pain3.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2015/02/sp_pain3.jpg" />
  </entry>
  <entry>
    <title>Embracing SQL In Postgres</title>
    <link href="https://bigmachine.io/posts/embracing-sql-in-postgres" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.727Z</updated>
    <id>https://bigmachine.io/posts/embracing-sql-in-postgres</id>
    <summary type="text">One thing that drives me absolutely over the cliff is how ORMs try so hard (and fail) to abstract the power and expressiveness of SQL. Before I write further let me say that &lt;a href=&quot;https://twitter.com/fransbouma&quot;&gt;Frans Bouma&lt;/a&gt; reminded me yesterday there&apos;s a difference betwee</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2015/02/jordan_sql.jpg" alt="Embracing SQL In Postgres" /></p>
One thing that drives me absolutely over the cliff is how ORMs try so hard (and fail) to abstract the power and expressiveness of SQL. Before I write further let me say that <a href="https://twitter.com/fransbouma">Frans Bouma</a> reminded me yesterday there's a difference between ORMs and the people that use them. They're just tools (the ORMs) - and I agree with that in the same way I agree that crappy fast food doesn't make people fat - it's the people that eat too much of it.

Instead of ripping ORMs apart again - I'd like to be positive and tell you <em>just why</em> I have stopped using their whack-ass OO abstraction on top of my databases :). In short: <em>it's because SQL can expertly help you express the value of your application in terms of the data</em>. That's really the only way you're going to know whether your app is any good: <strong>by the data it generates</strong>.

So give it a little of your time - it's fun once you get rolling with the basics and how your favorite DB engine accentuates the SQL standard. Let's see some examples (by the way all of what I'm using below is <a href="http://www.postgresql.org/docs/9.4/static/functions.html">detailed here in the Postgres docs</a> - have a read, there's a lot of stuff you can learn - my examples below barely even scratch the surface).

<h2>Postgres Built-in Fun</h2>

Right from the start: <em>Postgres sugary SQL syntax is really, really fun</em>. SQL is an ANSI standardized language - this means you can roughly expect to have the same rules from one system to the next (which means you can't expect it at all).

Postgres follows the standards almost to the letter - but it goes beyond with some very fun additions. Let's take a look!

<h3>Regex</h3>

At some point you might need to run a rather complicated string matching algorithm. Many databases (<a href="https://msdn.microsoft.com/en-us/magazine/cc163473.aspx">including SQL Server</a> - sorry for the MSDN link) allow you to use Regex patterning through a function or some other construct. With Postgres it works in a lovely, simple way (using PSQL for this with the old Tekpub database):

<pre><code class="sql">select sku,title from products where title ~* 'master';
    sku     |              title
------------+---------------------------------
 aspnet4    | Mastering ASP.NET 4.0
 wp7        | Mastering Windows Phone 7
 hg         | Mastering Mercurial
 linq       | Mastering Linq
 git        | Mastering Git
 ef         | Mastering Entity Framework 4.0
 ag         | Mastering Silverlight 4.0
 jquery     | Mastering jQuery
 csharp4    | Mastering C# 4.0 with Jon Skeet
 nhibernate | Mastering NHibernate 2
(10 rows)
</code></pre>

The <code>~_</code> operator says "here comes a POSIX regex pattern that's case insensitive". You can make it case sensitive by omitting the <code>_</code>.

Regex can be a pain to work with but if you wanted you could ramp this query up by using Postgres' built-in Full Text indexing:

<pre><code class="sql">select products.sku,
products.title
from products
where to_tsvector(title) @@ to_tsquery('Mastering');
    sku     |              title
------------+---------------------------------
 aspnet4    | Mastering ASP.NET 4.0
 wp7        | Mastering Windows Phone 7
 hg         | Mastering Mercurial
 linq       | Mastering Linq
 git        | Mastering Git
 ef         | Mastering Entity Framework 4.0
 ag         | Mastering Silverlight 4.0
 jquery     | Mastering jQuery
 csharp4    | Mastering C# 4.0 with Jon Skeet
 nhibernate | Mastering NHibernate 2
(10 rows)
</code></pre>

This is a bit more complicated. Postgres has a built-in data type specifically for the use of Full Text indexing - <code>tsvector</code>. You can even have this as a column on a table if you like, which is great as it's not hidden away in some binary index somewhere.

I'm converting my title on the fly to <code>tsvector</code> using the <code>to_tsvector()</code> function. This tokenizes and prepares the string for searching. I'm then shoving this into the <code>to_tsquery()</code> function. This is a query built from the term "Mastering". The <code>@@</code> bits simply say "return true if the <code>tsvector</code> field matches the <code>tsquery</code>". The syntax is a bit wonky but it works really well and is quite fast.

You can use the <code>concat</code> function to push strings together for use on additional fields too:

<pre><code class="sql">select products.sku,
products.title
from products
where to_tsvector(concat(title,' ',description)) @@ to_tsquery('Mastering');
    sku     |              title
------------+---------------------------------
 aspnet4    | Mastering ASP.NET 4.0
 wp7        | Mastering Windows Phone 7
 hg         | Mastering Mercurial
 linq       | Mastering Linq
 git        | Mastering Git
 ef         | Mastering Entity Framework 4.0
 ag         | Mastering Silverlight 4.0
 jquery     | Mastering jQuery
 csharp4    | Mastering C# 4.0 with Jon Skeet
 nhibernate | Mastering NHibernate 2
(10 rows)
</code></pre>

This combines <code>title</code> and <code>description</code> into one field and allows you to search them both at the same time using the power of <a href="http://www.postgresql.org/docs/9.4/static/textsearch.html">a kick-ass full text search engine</a>. I could spend multiple posts on this - for now just know you can do it inline.

<h3>Generating a Series</h3>

One really fun function that's built in is <code>generate_series()</code> - it outputs a sequence that you can use in your queries for any reason:

<pre><code class="sql">select * from generate_series(1,10);
 generate_series
-----------------
               1
               2
               3
               4
               5
               6
               7
               8
               9
              10
</code></pre>

If sequential things aren't what you want, you can order by another great function - <code>random()</code>:

<pre><code class="sql">select * from generate_series(1,10,2)
order by random();
 generate_series
-----------------
               3
               5
               7
               1
               9
(5 rows)
</code></pre>

Here I've added an additional argument to tell it to skip by 2.

It also works with dates:

<pre><code class="sql">select * from generate_series(
         '2014-01-01'::timestamp,
         '2014-12-01'::timestamp,
         '42 days');

   generate_series
---------------------
 2014-01-01 00:00:00
 2014-02-12 00:00:00
 2014-03-26 00:00:00
 2014-05-07 00:00:00
 2014-06-18 00:00:00
 2014-07-30 00:00:00
 2014-09-10 00:00:00
 2014-10-22 00:00:00
(8 rows)
</code></pre>

Here I'm telling it to output the dates in 2014 in 42 day intervals. You can do this backwards to, you just have to use a negative interval.

Why is this useful? You can alias this function and plug in the number from the series generation into whatever calculation you want:

## <pre><code class="sql">select x as first_of_the_month from generate_series('2014-01-01'::timestamp,'2014-12-01'::timestamp,'1 month') as f(x); first_of_the_month

2014-01-01 00:00:00
2014-02-01 00:00:00
2014-03-01 00:00:00
2014-04-01 00:00:00
2014-05-01 00:00:00
2014-06-01 00:00:00
2014-07-01 00:00:00
2014-08-01 00:00:00
2014-09-01 00:00:00
2014-10-01 00:00:00
2014-11-01 00:00:00
2014-12-01 00:00:00
(12 rows)
</code></pre>

Aliasing functions like this allows you to use the resulting row inline with your SQL call. This kind of thing is nice for analytics and spot-checks on your data. Also, notice the <code>month</code> specification? That's an interval in Postgres - something you'll use a lot with data stuff. Speaking of dates...

<h3>Date Math Fun</h3>

Intervals are brilliant shortcuts for working with dates in Postgres. For instance, if you want to know the date 1 week from today...

<pre><code class="sql">select '1 week' + now() as a_week_from_now;
        a_week_from_now
-------------------------------
 2015-03-03 10:08:12.156656+01
(1 row)

</code></pre>

Postgres sees <code>now()</code> as a <code>timestamp</code> and uses the <code>+</code> operator to infer the string '1 week' as an interval. Brilliant. But do you notice the result <code>2015-03-03 10:08:12.156656+01</code>? This is a very interesting thing!

It's telling me the current date and time all the way down to milliseconds... and also the timezone (+1 as I'm currently in Italy).

If you've ever had to wrestle with dates and UTC - well it's a major pain. Postgres has a built-in <code>timestamptz</code> data type - timestamp with time zone - that will account for this when doing date calculations.

This is really fun to play with. For instance I can ask Postgres what time it is in California:

<pre><code class="sql">SELECT now() AT TIME ZONE 'PDT' as cali_time;
         cali_time
----------------------------
 2015-02-24 02:16:57.884518
(1 row)
</code></pre>

2am - best not call Jon Galloway and tell him his SQL Server is on fire. This returns an <code>interval</code> - the difference between two timestamps (edited).

How many hours behind me is Jon? Let's see...

<pre><code class="sql">select now() - now() at time zone 'PDT' as cali_diff;
 cali_diff
-----------
 08:00:00
(1 row)
</code></pre>

Notice the return value is a <code>timestamp</code> of 8 hours, not an integer. Why is this important? Time is a relative thing and it's incredibly important to know <em>which time zone</em> your server is in when you calculate things based on time.

For instance - in my Tekpub database I recorded when orders were placed. If 20 orders came in during that "End of the Year Sale" my accountant would very much like to know if they came in before, or after, midnight on January 1st, 2013. My server is in New York, my business is registered in Hawaii...

This is important stuff and Postgres handles this and many other date functions quite nicely.

<h3>Aggregates</h3>

Working with rollups and aggregates in Postgres can be tedious precisely because it's so very, very standards-compliant. This always leads to having to be sure that whatever you GROUP BY is in your SELECT clause.

Meaning, if I want to look at sales for the month, grouped by week I'd need to run a query like this:

<pre><code class="sql">select sku, sum(price),
date_part('month',created_at) from invoice_items
group by sku,date_part('month',created_at)
having date_part('month',created_at) = 9
</code></pre>

That's a bit extreme and a bit of a PITA to write (and remember the syntax!). Let's use a better SQL feature in Postgres: <em>windowing functions</em>:

<pre><code class="sql">select distinct sku, sum(price) OVER (PARTITION BY sku)
from invoice_items
where date_part('month',created_at) = 9
</code></pre>

Same data, less noise (windowing functions are also available in SQL Server). Here I'm doing set-based calculations by specifying I want to run a <code>SUM</code> over a partition of data for a given row. If I didn't specify <code>DISTINCT</code> here the query would have spit out all sales as if it we just a normal <code>SELECT</code> query.

The nice thing about using windowing functions is that I can pair aggregates together:

<pre><code class="sql">select distinct sku, sum(price) OVER (PARTITION BY sku) as revenue,
count(1) OVER (PARTITION BY sku) as sales_count
from invoice_items
where date_part('month',created_at) = 9
</code></pre>

This gives me a monthly sales count per sku as well as revenue. I can also output total sales for the month in the very next column:

<pre><code class="sql">select distinct sku,
sum(price) OVER (PARTITION BY sku) as revenue,
count(1) OVER (PARTITION BY sku) as sales_count,
sum(price) OVER (PARTITION by 0) as sales_total
from invoice_items
where date_part('month',created_at) = 9
</code></pre>

I'm using <code>PARTITION BY 0</code> here as a way of saying "just use the entire set as the partition" - this will rollup all sales for September.

... and combine this with the power of <a href="http://bigmachine.io/2015/02/09/inserting-using-new-record-postgres/">a Common Table Expression</a> I can run some interesting calcs:

<pre><code class="sql">with september_sales as (
    select distinct sku,
    sum(price) OVER (PARTITION BY sku) as revenue,
    count(1) OVER (PARTITION BY sku) as sales_count,
    sum(price) OVER (PARTITION by 0) as sales_total
    from invoice_items
    where date_part('month',created_at) = 9
)

select sku,
    revenue::money,
    sales_count,
    sales_total::money,
    trunc((revenue/sales_total * 100),4) as percentage
from september_sales
</code></pre>

In the final select I'm casting <code>revenue</code> and <code>sales_total</code> as <code>money</code> - which means it will be formatted nicely with a currency symbol.

A pretty comprehensive sales query - I get a total per sku, a sales count and a percentage of monthly sales with (what I promise becomes) fairly straightforward SQL.

I'm using <code>trunc</code> in the CTE here to round to 4 significant digits as the percentages can be quite long.

<h3>Strings</h3>

I showed you some fun with Regex above, but there is more you can do with strings in Postgres. Consider this query, which I used quite often (again, the Tekpub database):

<pre><code class="sql">select products.sku,
    products.title,
    downloads.list_order,
    downloads.title  as episode
from products
inner join downloads on downloads.product_id = products.id
order by products.sku, downloads.list_order;
</code></pre>

This fetched all of my videos and their individual episodes (I called them downloads). I would use this query on display pages, which worked fine.

But what if I just wanted an episode summary? I could use some aggregate functions to this. The simplest first - just a comma-separated string of titles:

<pre><code class="sql">select products.sku,
    products.title,
    string_agg(downloads.title, ', ') as downloads
from products
inner join downloads on downloads.product_id = products.id
group by products.sku, products.title
order by products.sku
</code></pre>

<code>string_agg</code> works like String.join() in your favorite language. But we can do one better, let's concatenate and send things down in an array for the client:

<pre><code class="sql">select products.sku,
    products.title,
    array_agg(concat(downloads.list_order,') ',downloads.title)) as downloads
from products
inner join downloads on downloads.product_id = products.id
group by products.sku, products.title
order by products.sku
</code></pre>

Here I'm using <code>array_agg</code> to pull in the <code>list_order</code> and <code>title</code> from the joined downloads table and output them inline as an array. I'm using the <code>concat</code> function to concatenate a pretty title using the <code>list_order</code> as well.

If you're using Node, this will come back to you as an array you can iterate over.

If you're using Node, you'll probably want to have this JSON'd out, however:

<pre><code class="sql">select products.sku,
    products.title,
    json_agg(downloads) as downloads
from products
inner join downloads on downloads.product_id = products.id
group by products.sku, products.title
order by products.sku
</code></pre>

Here I'm shoving the related downloads bits (aka the "Child" records) into a field that I can easily consume on the client - an array of JSON.

<h2>Summary</h2>

If you don't know SQL very well - particularly how your favorite database engine implements and enhances it - take this week to get to know it better. It's so very powerful for working the gold of your application: <em>your data</em>.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2015/02/jordan_sql.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2015/02/jordan_sql.jpg" />
  </entry>
  <entry>
    <title>Document Storage Gymnastics with Postgres</title>
    <link href="https://bigmachine.io/posts/document-storage-gymnastics-in-postgres" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.727Z</updated>
    <id>https://bigmachine.io/posts/document-storage-gymnastics-in-postgres</id>
    <summary type="text">With the release of Postgres 9.4 came the additional datatype . This is binary JSON, the same type of thing that MongoDB uses for internal storage. Postgres has had the  data type for a while, but  allows you to do something lovely: document indexing and specialized queries.

Mon</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2015/03/pg_parkour.jpg" alt="Document Storage Gymnastics with Postgres" /></p>
With the release of Postgres 9.4 came the additional datatype `jsonb`. This is binary JSON, the same type of thing that MongoDB uses for internal storage. Postgres has had the `json` data type for a while, but `jsonb` allows you to do something lovely: **document indexing** and specialized queries.

Mongo is currently rewriting it's storage engine so what I'm about to say might not could be viewed as arguable, however: [Postgres blows MongoDB away in terms of speed and space on disk](http://blogs.enterprisedb.com/2014/09/24/postgres-outperforms-mongodb-and-ushers-in-new-developer-reality/). 

But benchmarks like the one above can be argued forever - specifically that Mongo excels at "scale out" and if you scale out enough you can turn the tables. Postgres tends to "scale up" so... yeah. But, for single-node deployments Postgres is pretty much the clear winner.

Just take from this that we have a pretty hot bit of competition! Oh and **Postgres also has a full, standards-based, ACID-compliant engine that you can use too!**.

With that complete, let's play with `jsonb`.

## Turning Records Into JSON

I'm playing with the Tekpub database again. Here I want to push the invoices into a document structure, and I can do this with a Common Table Express (CTE) and Windowing function, with some built-in JSON functions in Postgres:

```sql
with invoice_docs as (
  select invoices.*,
  json_agg(invoice_items) over (partition by invoices.id) as items
  from invoices 
  inner join invoice_items on invoice_items.invoice_id = invoices.id 
)
select row_to_json(invoice_docs) from invoice_docs;
```

I don't strictly need to use a CTE here, it just makes things nice and readable. I'm using `json_agg` to wrap up the joined invoice_items and parse them into a JSON array. Then I'm using `row_to_json` to squeeze the result into a full JSON document.

Simple stuff... but it's kind of slow. I have 40,000 or so records here and it takes about 7 seconds to complete on my machine - so this isn't something you'll want to run all the time.

I'll create a table to store this stuff in:

```sql
create table invoice_documents(
  id serial primary key,
  body jsonb
)
```

Since I'm using `jsonb` I'll want to index these documents so I can have some fast queries:

```sql
create index idx_invoice_docs on invoice_docs using GIN(body jsonb_path_ops)
```

I'm using the specialized `GIN` index for this. [Postgres has a number of different index types](http://www.postgresql.org/docs/9.4/static/indexes-types.html) that are worth getting to know - `GIN` however is a special index for values with multiple keys (arrays, json, xml, and tokenized text like you find in the `tsvector` search field). The `jsonb_path_ops` argument simply says "index all the keys" - this can be wasteful in big documents, however.

Now let's pump those documents in:

```sql
with invoice_docs as (
  select invoices.*,
  json_agg(invoice_items) over (partition by invoices.id) as items
  from invoices 
  inner join invoice_items on invoice_items.invoice_id = invoices.id 
)
insert into invoice_docs(body)
select row_to_json(invoice_docs) from invoice_docs;
```

We know have a document database. Let's play with it!

## Querying Our Documents

There are a number of ways you can query `jsonb` and it takes a bit to get used the syntactic sugar Postgres gives you. Let's have a look:

```sql
select body -> 'id' as id,
(body ->> 'amount_due')::money as price, 
(body->> 'created_at')::timestamptz as created_at
from invoice_docs;

  id   |   price   |          created_at
-------+-----------+-------------------------------
 1     |    $18.00 | 2012-08-15 03:59:17.762872+02
 2     |    $12.00 | 2012-08-15 04:02:44.965786+02
 3     |    $12.00 | 2012-08-15 04:06:32.67401+02
 4     |    $12.00 | 2012-08-18 01:39:31.938155+02
 5     |   $200.00 | 2012-09-20 00:53:17.858894+02
 10    |    $28.00 | 2012-07-28 07:28:21.922092+02
```

There is a lot going on here. First, the syntax for selecting a document value is a bit whacky: `body -> 'key'` (the parens are optional). The `->` operator says "give me jsonb" and Postgres does what it can, inferring what type is held there.

For `amount_due` and `created_at` we get something completely different. You can see this if we rerun the query:

```sql
select pg_typeof(body -> 'amount_due') as one_arrow,
pg_typeof(body ->> 'amount_due') as two_arrows
from invoice_docs limit 1;
 one_arrow | two_arrows
-----------+------------
 jsonb     | text
(1 row)
```

Here I'm using `pg_typeof` to know what types I'm dealing with - this is massively important when you're dealing with these functions, as you'll see. The `->` operator returns `jsonb` (which PG will do it's best to interpret for you), the `->>` operator returns the JSON text itself.

## Using Where with JSONB

Let's find a particular invoice - ones where I bought something from myself:

```sql
select body -> 'id' as id,
(body ->> 'email') as email,
(body ->> 'amount_due')::money as price,
(body->> 'created_at')::timestamptz as created_at
from invoice_docs
where body @> '{"email" : "rob@wekeroad.com"}';

  id   |      email       |  price  |          created_at
-------+------------------+---------+-------------------------------
 197   | rob@wekeroad.com | $200.00 | 2012-10-10 03:20:46.412831+02
 15562 | rob@wekeroad.com |  $18.00 | 2011-12-16 09:07:26.04932+01
 18589 | rob@wekeroad.com |  $28.00 | 2012-05-25 08:13:27.211351+02
 19135 | rob@wekeroad.com |   $0.00 | 2012-10-14 19:16:47.198226+02
(4 rows)
```

This syntax: `where body @> '{"email" : "rob@wekeroad.com"}'` is really not fun to write, but it works pretty well. The `@>` is "has the following key/value pair". You can also check for basic existence:

```sql
select body -> 'id' as id,
(body ->> 'email') as email,
(body ->> 'amount_due')::money as price, 
(body->> 'created_at')::timestamptz as created_at
from invoice_docs
where (body -> 'email') ?  'rob@wekeroad.com';

  id   |      email       |  price  |          created_at
-------+------------------+---------+-------------------------------
 197   | rob@wekeroad.com | $200.00 | 2012-10-10 03:20:46.412831+02
 15562 | rob@wekeroad.com |  $18.00 | 2011-12-16 09:07:26.04932+01
 18589 | rob@wekeroad.com |  $28.00 | 2012-05-25 08:13:27.211351+02
 19135 | rob@wekeroad.com |   $0.00 | 2012-10-14 19:16:47.198226+02
(4 rows)
```

Same thing, a little easier to write. So what's the difference? The latter query here is just checking for the existence of a value for a given `jsonb` element in the document - *it's kind of a dumb query* if you will. We can see this if we use `EXPLAIN`:

```sql
explain select body -> 'id' as id,
(body ->> 'email') as email,
(body ->> 'amount_due')::money as price,
(body->> 'created_at')::timestamptz as created_at
from invoice_docs
where (body -> 'email') ?  'rob@wekeroad.com';
                            QUERY PLAN
-------------------------------------------------------------------
 Seq Scan on invoice_docs  (cost=0.00..5754.99 rows=32 width=1206)
   Filter: ((body -> 'email'::text) ? 'rob@wekeroad.com'::text)
```

A Sequential Scan is not what you want - the query executes over each row in your database and evaluates each row individually. Here you can see that for each row, a Filter is being applied. It's a simple one and you won't see a problem on small datasets, but when things get big, your DBA will kill you for this.

Now let's do the same with our `@>` operator:

```sql
explain select body -> 'id' as id,
(body ->> 'email') as email,
(body ->> 'amount_due')::money as price,
(body->> 'created_at')::timestamptz as created_at
from invoice_docs
where body @> '{"email" : "rob@wekeroad.com"}';

                                   QUERY PLAN
---------------------------------------------------------------------------------
 Bitmap Heap Scan on invoice_docs  (cost=16.25..137.81 rows=32 width=1206)
   Recheck Cond: (body @> '{"email": "rob@wekeroad.com"}'::jsonb)
   ->  Bitmap Index Scan on idx_invoice_docs  (cost=0.00..16.24 rows=32 width=0)
         Index Cond: (body @> '{"email": "rob@wekeroad.com"}'::jsonb)
```

Ahhhh... much better. Remember the `GIN` index I created? The first query was ignoring it, but the one using `@>` uses it fully. If you only remember ONE THING from this article, remember this: **favor `@>` queries for indexed tables**.

The great thing about this is that it *indexes the whole document*. This means we can do deep queries like this flexing the same index:

```sql
select body -> 'number' as knockouts                                       
from invoice_docs                                                                   where body @> '{"items" : [{"sku" : "knockout"}] }' limit 1;
      
      knockouts
----------------------
 "someinvoicenumber"
(1 row)
```

By now hopefully you're seeing that querying a `jsonb` document, while different from typical SQL, uses the same SQL structures you (hopefully) already know. For instance I'm setting a `limit` here as well as aliasing the column. This is a big advantage over other document databases.

## Document Gymnastics

Astute readers will be wondering what good it is to put these things into documents when you might need to do some kind of rollup on them. Too true.

Specifically: we might need to query the items buried inside our invoice document so we can get some sales intelligence. 

Hopefully you're not running analytics on your live system - you really should be exporting your data into something good at crunching numbers like Excel or Numbers or whatever spreadsheet you use.

So let's how you can export this goodness from our `invoice_docs` table. First, you can grab the items directly using a select as we've been doing:

```sql
select body -> 'items'
from invoice_docs;

[{"id": 1, "sku": "ft_speaker"...}]
[{"id": 2, "sku": "knockout", ...}]
```

What data type does it return? Let's find out:

```sql
select pg_typeof(body -> 'items') 
from invoice_docs limit 1                ;
 pg_typeof
-----------
 jsonb
(1 row)
```

It's `jsonb` - or more specifically an array of JSON items. This is important. We need to be sure we're using `jsonb` exclusively so we can a) take advantage of indexing an b) use some amazing built-in functions.

So, the first thing we need to do is unwrap the result from an array into a simple `jsonb` object:

```sql
select jsonb_array_elements(body -> 'items') as items
from invoice_docs limit 2

{"id": 1, "sku": "ft_speaker"...}
{"id": 2, "sku": "knockout"...}
```

I'm using `jsonb_array_elements` to do this for me, and it returns a `jsonb` object that I can play with. Now I can wrap the select query above using a CTE:

```sql
with unwrapped as (
	select jsonb_array_elements(body -> 'items') as items
	from invoice_docs
), invoice_lines as (
	select x.* from unwrapped, jsonb_to_record(items) as 
	x(
		id int,
		sku varchar(50), 
		name varchar(255),
		price decimal(10,2)
	)
)
select * from invoice_lines limit 5;

id |    sku     |                 name                 | price
----+------------+--------------------------------------+--------
  1 | ft_speaker | The Art of Speaking: Scott Hanselman |  18.00
  2 | knockout   | Practical KnockoutJS                 |  12.00
  3 | knockout   | Practical KnockoutJS                 |  12.00
  4 | knockout   | Practical KnockoutJS                 |  12.00
  5 | yearly     | Tekpub Annual Subscription           | 200.00
(5 rows)
```

OK, I just hit you with a lot of stuff. Let's walk through this.

The first query in the CTE is using `jsonb_array_elements` to unwrap the items from an array into a simple `jsonb` document. The second query is using `jsonb_to_record` which is a bread and butter function that takes a `jsonb` object and turns it into a record.

The only way this will work, however is if I give Postgres a column definition list. I do this by aliasing the function (here it's `x`) and defining a column list. 

If this is all new to you, use it a few times - it starts to make sense. The function `jsonb_to_record` returns a `record` and that record needs to be reported as something - and you can do that using plain old SQL.

You can [read more about various JSON functions here](http://www.postgresql.org/docs/9.4/static/functions-json.html) - perhaps you can improve what I did above.

## Analytical Output

The last thing I'll do here is create an extraction query for export. In the analytics world this is the second thing you need to do: *shape your query for export*. The first is to make sure it's correct - but let's assume I've done that already.

I want to export this data in denormalized fashion for use in [what's called a "fact" table](http://en.wikipedia.org/wiki/Fact_table). This means I'll need to add a few columns of derived data. I can use an additional CTE to do this:

```sql
with unwrapped as (
	select jsonb_array_elements(body -> 'items') as items
	from invoice_docs
), invoice_lines as (
	select x.* from unwrapped, jsonb_to_record(items) as 
	x(
		id int,
		sku varchar(50), 
		name varchar(255),
		price decimal(10,2),
		quantity int,
		created_at timestamptz
	)
), fact_ready as (
  select sku,
	price::money,
	(quantity * price)::money as line_total,
	date_part('year', created_at) as year,
	date_part('month', created_at) as month,
	date_part('quarter', created_at) as quarter
  from invoice_lines
)
select * from fact_ready limit 5;

    sku     |  price  | line_total | year | month | quarter
------------+---------+------------+------+-------+---------
 ft_speaker |  $18.00 |     $18.00 | 2012 |     8 |       3
 knockout   |  $12.00 |     $12.00 | 2012 |     8 |       3
 knockout   |  $12.00 |     $12.00 | 2012 |     8 |       3
 knockout   |  $12.00 |     $12.00 | 2012 |     8 |       3
 yearly     | $200.00 |    $200.00 | 2012 |     9 |       3
(5 rows)
```

*(I've limited this to 5 records for readability)*.

Not bad for using a document! As I mention above - **we certainly don't have to do things this way**, but it's a fun mental exercise to see what's possible with Postgres and tweaked SQL.

## Full Text Search

Let's flex our CTEs for one last thing - finding invoices that involve [Scott Hanselman](http://hanselman.com). I could do this in a number of ways but since we're having so much fun with `jsonb` let's see what kind of silliness we can do on the fly.

Like creating a Full Text index:

```sql
with unwrapped as (
	select jsonb_array_elements(body -> 'items') as items
	from invoice_docs
), invoice_lines as (
	select x.* from unwrapped, jsonb_to_record(items) as 
	x(
		id int,
		sku varchar(50), 
		name varchar(255),
		price decimal(10,2),
		quantity int,
		created_at timestamptz
	)
), searchable as (
	select id,
	name,
	to_tsvector(concat(sku,' ',name)) as search_vector
	from invoice_lines
)

select id, name from searchable
where search_vector @@ to_tsquery('Hanselman')
limit 5;

 id |                 name
----+--------------------------------------
  1 | The Art of Speaking: Scott Hanselman
 14 | The Art of Speaking: Scott Hanselman
 14 | The Art of Speaking: Scott Hanselman
 28 | The Art of Speaking: Scott Hanselman
 43 | The Art of Speaking: Scott Hanselman
(5 rows)
```

The first two CTEs are exactly the same - unwrapping the invoice items. The last one (called `searchable`) is concatenating the `sku` and `name` fields of the invoice item and then indexing it on the fly for full text searching using `to_tsvector`.

In the final query I just need to use that `ts_vector` and search for Hanselman's name.

As you can see, `jsonb` is a great data type to work with, but it takes some fiddling. Hopefully this post can get you off to a good start.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2015/03/pg_parkour.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2015/03/pg_parkour.jpg" />
  </entry>
  <entry>
    <title>Bringing The Power of Postgres to NodeJS</title>
    <link href="https://bigmachine.io/posts/bringing-the-power-of-postgres-to-nodejs" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.727Z</updated>
    <id>https://bigmachine.io/posts/bringing-the-power-of-postgres-to-nodejs</id>
    <summary type="text">I&apos;m building out an idea I have and, as you may have guessed from the last few blog posts I&apos;ve written - I&apos;m using Postgres to do it.

I like SQL a lot - but that doesn&apos;t mean I&apos;m going to drop SQL statements all over my application (despite what some of my commenters have sugges</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2015/03/massive-title.jpg" alt="Bringing The Power of Postgres to NodeJS" /></p>
I'm building out an idea I have and, as you may have guessed from the last few blog posts I've written - I'm using Postgres to do it.

[I like SQL a lot](http://bigmachine.io/2015/02/24/embracing-sql-in-postgres/) - but that doesn't mean I'm going to drop SQL statements all over my application (despite what some of my commenters have suggested). I _do_ like a little abstraction - but for the life of me I couldn't find a tool out there that **got the abstraction right** - in other words:

- Take care of the mundane `select * from` and `insert into` bits
- Let me use SQL where and how I wanted and (this is the most important)
- **help me use SQL where and how I wanted**

In addition there's nothing out there in the Node space that's working directly with [Postgres' radical `jsonb` document capabilities](http://rob.conery.io/2015/03/01/document-storage-gymnastics-in-postgres/). Not that I've seen anyway.

[So I built it](https://github.com/robconery/massive-js). You can install it today with `npm install massive`.

## MassiveJS 2.0: Rebuilt To Embrace Postgres

Over the last 3 weeks I've been working with [Jon Atten](http://twitter.com/xivsolutions) to build out something that I wish existed: **a dedicated Postgres data tool**. Something that didn't give a flying fuck about "database portability" and instead let you dive face-first into all that a database can do.

So, here you go. Let's take a look at some of the fun features I threw in there.

### SQL Files as Functions

This one is my favorite. You create a file with a query in it - just regular old SQL with a `.sql` extension - and on spin-up Massive will read that file and let you execute it.

By default massive looks in a `/db` directory, but you can override as you need. So let's say you have a query - `select * from users where id=$1` that you want to be able use with massive. You put that query into `/db/userById.sql` and then...

```js
var massive = require("massive");
massive.connect({ db: "my_db" }, function (err, db) {
  db.userById(1, function (err, res) {
    //you've got your data in res
  });
});
```

That's it. You can put whatever the hell you want in there - to play with the full power that Postgres offers, and we'll execute it for you cleanly.

### Full JSONB Support

Remembering the syntax for working with `jsonb` is tricky with Postgres. Not only do you need to remember `(thing) -> 'key'` syntax, you also need to remember the symbols to use (and when to use them). There are existence queries, contains, and then straight up matching that the engine allows you to do.

It's not hard, but it could do with some abstraction :). So I built it:

```js
var massive = require("massive");
massive.connect({ db: "my_db" }, function (err, db) {
  db.saveDoc("planets", { name: "Arrakis" }, function (err, planet) {
    //you've got your planet here
  });
});
```

This query did two very important things:

- created a table on the fly for you called "planets" with an `id` key, and a `body` field that's `jsonb`
- created a `GIN` index on that `jsonb` field so it's properly indexed

You can now query with it...

```js
db.planets.findDoc({ name: "Arrakis" }, function (err, planet) {
  //your planet sir
});
```

When massive is connected it scans the tables in your database as well as the SQL queries in your `/db` directory - and attaches them to the root namespace (in this case `db`. More on tables down below.

This query is smart enough to know that it's a straight up match, so it will use a `@>` matcher, flexing the index we created for you. You can do other queries, however...

```js
db.planets.searchDoc(
  {
    keys: "name",
    term: "Arr",
  },
  function (err, results) {
    //full text search on the fly
  }
);
```

This query builds a full-text index on the fly for you, over a JSON document. And it's really, really fast.

### Good Old Relational Support

Massive is at version 2.0 now, which means it came from a version 1.0. This tool was around for a while and I liked using it, but I didn't like how watered down I had to make it so that you could work with MySQL and Postgres. I wanted a tool that would rock Postgres - so I booted MySQL support (a horrible database in my mind).

You can query your tables directly:

```js
db.users.find(1, function (err, res) {
  //user with id 1
});

db.users.find({ "id >": 10 }, function (err, res) {
  //all users with id > 10
});

db.users.search(
  {
    columns: ["first", "last"],
    term: "rob",
  },
  function (err, users) {
    //full text on the fly
  }
);
```

The syntax is pretty rudimentary - you can see more [on the README up at github](http://github.com/robconery/massive-js).

### REPL

We're building out a REPL as well as some command-line fun - the idea being that Massive is all about helping you build on top of Postgres. Right now if you run the REPL:

```
node bin/massive -d my_db
```

You'll connect Massive directly to your DB and you can have a play (the `-d` flag tells Massive which local database to connect to).

Massive will load up and you can have some fun. Here I just want to have a look at the root namespace, `db`, so I enter "db":

```
db > db
{ scriptsDir: '/Users/rob/Projects/massive-js/db',
  connectionString: 'postgres://localhost/massive',
  query: [Function],
  executeSqlFile: [Function],
  tables:
   [ { albums: [Object], artists: [Object], docs: [Object] },
     { schema: 'public',
```

One of my tables is named `products`, so I can run a query to see what happens:

```js
db> db.products.find(1);
db > { id: 1,
  name: 'Product 1',
  price: '12.00',
  description: 'Product 1 description',
  in_stock: true,
  created_at: Fri Mar 13 2015 10:07:24 GMT+0100 (CET) }
```

Notice that I didn't enter a callback here? Massive does that for you (in any case) - if the callback is missing for a query, we'll add one that outputs the result to the console.

The REPL is still in very early stages, and we're tweaking some ideas with it - but it's fun enough (and useful enough for me, anyway) that I'm keeping it in there.

## Rob, Don't You Hate ORMs?

Yep, sure do. This isn't an ORM, not by any stretch. To me it's **Goldilocks Abstraction** - _just right_.

Hope you like it.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2015/03/massive-title.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2015/03/massive-title.jpg" />
  </entry>
  <entry>
    <title>Membership In a Box with PG-Auth</title>
    <link href="https://bigmachine.io/posts/membership-in-a-box-with-pg-auth" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.727Z</updated>
    <id>https://bigmachine.io/posts/membership-in-a-box-with-pg-auth</id>
    <summary type="text">I mentioned in a previous post that I threw together some ideas one weekend on how to do membership completely within Postgres (users, roles, logs etc).

It was fun and quite a few people asked if they could have a look at it - so I tidied it up and wrapped it with a Node project</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2015/03/pg_auth_cover2.jpg" alt="Membership In a Box with PG-Auth" /></p>
I [mentioned in a previous post](http://bigmachine.io/2015/02/21/its-time-to-get-over-that-stored-procedure-aversion-you-have/) that I threw together some ideas one weekend on how to do membership completely within Postgres (users, roles, logs etc).

It was fun and quite a few people asked if they could have a look at it - so I tidied it up and wrapped it with a Node project (for test/build) and [popped it up on Github](https://github.com/robconery/pg-auth).

You see, I don't have enough people calling me names. More than that - [since I've been traveling with my family](http://rob.conery.io/2014/09/18/being-a-nomad-for-a-year/) I've started to have some fun and feel good about my career and what I'm doing. So I experiment and come up with weird ideas **because, in short, I'm having too much fun**.

I'm leaving it to you all to rip me apart. Bathe yourself in [this logic-filled function](https://github.com/robconery/pg-auth/blob/master/build/src/functions/register.sql) that I planted **deep into the heart of my database**:

```sql
create or replace function register(
    new_email varchar(255),
    pass varchar(255),
    confirm varchar(255)
)

returns TABLE (
    new_id bigint,
    message varchar(255),
    email varchar(255),
    success BOOLEAN,
    status int,
    email_validation_token varchar(36))  
as
$$
DECLARE
    new_id bigint;
    message varchar(255);
    hashedpw varchar(255);
    success BOOLEAN;
    return_email varchar(255);
    return_status int;
    validation_token varchar(36);
    verify_email boolean;

BEGIN
    -- default this to 'Not Approved'
    select 30 into return_status;
    select false into success;

    select new_email into return_email;

    -- validate the passwords match
    if(pass &lt;> confirm) THEN
        select 'Password and confirm do not match' into message;

    -- make sure user doesn't exist
    elseif exists(select membership.members.email from membership.members where membership.members.email=return_email)  then
        select 0 into new_id;
        select 'Email exists' into message;
    ELSE
        -- we're good, do the needful
        select true into success;

        -- hash password with blowfish
        SELECT membership.crypt(pass, membership.gen_salt('bf', 10)) into hashedpw;

        -- create a random value for email validation
        select membership.random_value(36) into validation_token;

        -- add the new Member record
        insert into membership.members(email, created_at, membership_status_id,email_validation_token)
        VALUES(new_email, now(), return_status, validation_token) returning id into new_id;

        -- the return message to be passed back out
        select 'Successfully registered' into message;

        -- add login bits to member_logins
        insert into membership.logins(member_id, provider, provider_key, provider_token)
        values(new_id, 'local',return_email,hashedpw);

        -- add auth token
        insert into membership.logins(member_id, provider, provider_key, provider_token)
        values(new_id, 'token',null,validation_token);

        -- add them to the members role
        insert into membership.members_roles(member_id, role_id)
        VALUES(new_id, 99);

        -- add log entry
        insert into membership.logs(subject,entry,member_id, created_at)
        values('registration','Added to system, set role to User',new_id,now());

        -- if the settings say we don't need to verify them, then activate now
        select email_validation_required into verify_email from membership.settings limit 1;

        -- if the email doesn't need verification, set their status to active
        -- this is in the settings table
        if verify_email = false then
          perform membership.change_status(return_email,10,'Activated member during registration');
        end if;

    end if;

    -- all done here, pass back what happened
    return query
    select new_id, message, new_email, success, return_status, validation_token;
END;
$$ LANGUAGE PLPGSQL;
```

Mmmmmm LOGIC. [How will I ever test this](https://github.com/robconery/pg-auth/blob/master/test/registration_spec.js)?

## OK Seriously What Is This Why Is This WTF?

First: this is just a first stab at an idea. I have used [Devise for Rails](https://github.com/plataformatec/devise) for a really long time because it does its job well. The only bad part is that I don't really use Rails anymore and there's nothing like this for Node.

Or ASP.NET MVC or Django. And really - should Rails have all the fun?

The main thing I need is just the data interactions - registering a user, authenticating, role management etc. I don't care much for the way Devise shoves all of this into a gem and I get to configure it - I'd much rather have it somewhere where I can change the stuff.

There are no views or mailers here - just routines for you to use if you like **with whatever platform you want**. That, to me, is important.

## Over My Dead Body?

I know that people will have some pretty strong reactions to this - I say this only because my last few posts on Postgres and Stored Procedures made some people really mad at me on Twitter and other places.

If you're one of those people, consider this repo a mirror. *Why does it make you so upset?*. I'm happy to talk about all of the issues as long as you're willing to hear my answers. In fact I've [setup a Discourse board for my blog](http://discourse.conery.io) to see if will help improve the comment situation!

I really do want to hear from you; but only if you're willing to understand that I wasn't born yesterday. If it helps: **consider this project a 'Paleo Project'** - something that flies in the face of conventional wisdom and that may just help you lose 50 pounds.

## Work In Progress

I'm still working on some things, of course. If you want to have a play all you need do is:

 - Install Postgres (9.4 is best)
 - Create a `pg_auth` database
 - Clone the repo (https://github.com/robconery/pg_auth) and run the tests

I'm sure there's a ton I can improve - I'm not a great PLPGSQL programmer and I've only recently (in the last 4 years) gotten into Postgres. Would love to hear your thoughts if you have any!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2015/03/pg_auth_cover2.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2015/03/pg_auth_cover2.jpg" />
  </entry>
  <entry>
    <title>Designing a PostgreSQL Document API</title>
    <link href="https://bigmachine.io/posts/designing-a-postgresql-document-api" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.727Z</updated>
    <id>https://bigmachine.io/posts/designing-a-postgresql-document-api</id>
    <summary type="text">PostgreSQL as many know, supports JSON as a storage type and with the release of 9.4, Postgres now supports storing JSON as  - a binary format.

This is great news for people who want to move beyond simple &quot;store JSON as text&quot;.  supports indexing now using the GIN index, and also</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2015/08/pg_api_1.jpg" alt="Designing a PostgreSQL Document API" /></p>
PostgreSQL as many know, supports JSON as a storage type and with the release of 9.4, Postgres now supports storing JSON as `jsonb` - a binary format.

This is great news for people who want to move beyond simple "store JSON as text". `jsonb` supports indexing now using the GIN index, and also has a special query operator that takes advantage of the GIN index.

## Who Cares?
[It's been fun to explore `jsonb` in Postgres](http://bigmachine.io/2015/03/01/document-storage-gymnastics-in-postgres/) and to see what's possible. Which is kind of the problem: *it's only an exploration and some musing*, to get any work done leaves a little to be desired.

What I mean is that other systems (like [RethinkDB](http://rethinkdb.com)) have a ton of functionality already built in to help you save documents, query documents, and optimize things. Postgres has some interesting abilities this way - but out of the box querying is pretty ... lacking to be honest.

Consider this query:

```sql
select document_field -> 'my_key' from my_docs
where document_field @> '{"some_key" : "some_value"}';
```

It surfaces a bit of weirdness when it comes to JSON and Postgres: *it's all strings*. Obviously SQL has no understanding of JSON, so *you have to format it as a string*. Which means **working directly with JSON in SQL is a pain.** Of course [if you have a good query tool](https://github.com/robconery/massive) that problem is lessened to a degree... but it still exists.

In addition, the storage of a document is a little free-for-all. Do you have a single column that's `jsonb`? Or Multiple columns in a larger table structure? It's up to you - which is nice but too many choices can also be paralyzing.

So why worry about all of this? If you want to use a document database then *use a document database*. I agree with that... but there's one really compelling reason to use Postgres (for me at least)...

<a href="http://rob.conery.io/img/2015/08/iu.gif"><img src="http://rob.conery.io/img/2015/08/iu.gif" alt="iu" width="500" height="333" class="alignnone size-full wp-image-478" /></a>

Postgres is ACID-compliant. That means you can rely on it to write your data and, hopefully, [not lose it](http://hackingdistributed.com/2013/01/29/mongo-ft/).

Postgres is also *relational*, which means that if you want to graduate to a stricter schema as time goes on *you can*. There are a number of reasons you might want to choose Postgres - for now let's say you have made that choice and want to start working with Documents and `jsonb`.

## A Better API

Personally, I'd love to see more functions that support the notion of working with documents. Right now we have built-ins that support working with the JSON types - but nothing that supports a higher level of abstraction.

That doesn't mean we can't build such an API ourselves. Which I did :). Here goes...

## A Document Table

I want to store documents in a table that has some meta information as well as additional ways I can query the information, specifically: Full Text Search.

The structure of the table can be opinionated - why not we're building out this abstraction! Let's start with this:

```sql
create table my_docs(
  id serial primary key,
  body jsonb not null,
  search tsvector,
  created_at timestamptz not null default now(),
  updated_at timestamptz not null default now()
)
```

There will be some duplication here. The document itself will be stored in the `body` field, including the id, which is also stored as a primary key (we need this because this is still Postgres). I'm embracing duplication, however, because:

 - I'll own this API and I'll be able to make sure everything is synced up
 - That's just the way it is in document systems

## Saving a Document

What I'd like in a `save_document` function is the ability to...

 - Create a table on the fly
 - Create the necessary indexes
 - Create timestamps and a searchable field (for Full Text indexing)

I can do this by creating my own function `save_document` and, for fun I'll use PLV8 - *JavaScript in the database*. In fact I'll create two functions - one that specifically creates my table, and another that saves the document itself.

First, `create_document_table`:

```sql
create function create_document_table(name varchar, out boolean)
as $$
  var sql = "create table " + name + "(" +
    "id serial primary key," +
    "body jsonb not null," +
    "search tsvector," +
    "created_at timestamptz default now() not null," +
    "updated_at timestamptz default now() not null);";

  plv8.execute(sql);
  plv8.execute("create index idx_" + name + " on docs using GIN(body jsonb_path_ops)");
  plv8.execute("create index idx_" + name + "_search on docs using GIN(search)");
  return true;
$$ language plv8;
```

This function creates a table and appropriate indexes - one for the `jsonb` field in our document table, the other for the `tsvector` full text index. You'll notice that I'm building SQL strings on the fly and executing with `plv8` - that's the way you do it with JavaScript in Postgres.

Next, let's create our `save_document` function:

```sql
create function save_document(tbl varchar, doc_string jsonb)
returns jsonb
as $$
  var doc = JSON.parse(doc_string);
  var result = null;
  var id = doc.id;
  var exists = plv8.execute("select table_name from information_schema.tables where table_name = $1", tbl)[0];

  if(!exists){
    plv8.execute("select create_document_table('" + tbl + "');");
  }

  if(id){
    result = plv8.execute("update " + tbl + " set body=$1, updated_at = now() where id=$2 returning *;",doc_string,id);
  }else{
    result = plv8.execute("insert into " + tbl + "(body) values($1) returning *;", doc_string);
    id = result[0].id;
    doc.id = id;
    result = plv8.execute("update " + tbl + " set body=$1 where id=$2 returning *",JSON.stringify(doc),id);
  }

  return result[0] ? result[0].body : null;

$$ language plv8;
```

I'm sure this function looks a bit strange, but if you read through each line you should be able to figure out a few things. But why the `JSON.parse()` call?

This is because the Postgres `jsonb` type is not really JSON here - it's a string. Outside our PLV8 bits is still Postgres World and it works with JSON as a string (storing it in `jsonb` in a binary format). So, when our document is passed to our function it's as a string, which we need to parse if we want to work with it as a JSON object in JavaScript.

In the insert clause you'll notice that I have to synchronize the ID of the document with that of the primary key that was just created. A little cumbersome, but it works fine.

Finally - you'll notice that in the original insert call as well as the update, I'm just passing the `doc_string` argument right into the `plv8.execute` call as a parameter. That's because you need to treat JSON values as strings in Postgres.

This can be really confusing. If I try to send in `doc` (our JSON.parsed object) it will get turned into `[Object object]` by plv8. Which is weird.

Moreover if I try to return a JavaScript object from this function (say, our `doc` variable) - I'll get an error that it's an invalid format for the type JSON. Which is *ultra confusing*.

For our result I'm simply returning the body from our query result - and it's a string, believe it or not, and I can just pass it straight through as a result. I should note here as well that all results from `plv8.execute` return an Array of items that you can work with as JavaScript objects.

## The Result

It works really well! And it's fast. If you want to try it out you'll need to install the PLV8 extension and then write your query accordingly:

```sql
create extension plv8;
select * from save_document('test_run', '{"name" : "Test"}');
```

You should see a new table and a new record in that table:

<a href="http://rob.conery.io/img/2015/08/save_document_1.png"><img src="http://rob.conery.io/img/2015/08/save_document_1.png" alt="save_document_1" width="650" height="208" class="alignnone size-full wp-image-520" /></a>

## More To Do

In the next post I'll add some additional features, specifically:

 - Automatically updating the `search` field
 - Bulk document insert using arrays

This is a good start!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2015/08/pg_api_1.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2015/08/pg_api_1.jpg" />
  </entry>
  <entry>
    <title>Being a Nomad for a Year</title>
    <link href="https://bigmachine.io/posts/being-a-nomad-for-a-year" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.726Z</updated>
    <id>https://bigmachine.io/posts/being-a-nomad-for-a-year</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/amsterdam_canal.jpg" alt="Being a Nomad for a Year" /></p>
## In Summary Form...

No, I'm not independently wealthy and yes, we're homeschooling our kids (aged 12 and 9). Yes I'm still working as we travel and so far things are ... kind of working out for us (more on that later).

We decided to do this basically on the spur of the moment - or put another way we were _kind of bored_ and wanted to do something new. My wife's business was in transition as she is thinking about switching her career around and now that Tekpub is part of Pluralsight my daily load (support and site stuff, etc) is much, much lighter.

We were in our garage one weekend cleaning up a few things and I remember looking around at **all of the crap** I've accumulated over the years. I thought about our trip to Bali last summer and said to my wife:

 > Remember how we had our entire lives in our suitcases? How free it was? Remember when I said our house could vanish and I wouldn't miss a single thing in it? Ugh...

She looked at me with an expression that was all agreement and I simply said: **let's get out of here.**

We mused on it for a few hours - if we could do anything what would we do? Travel was the first thing we thought of and then I said "I want to go through Europe" almost at the same time she said it. And there it was.

![](https://bigmachine.io/img/stillness.jpg)

## Money

Family, friends, colleagues - each of them asks the same question when they find out we're traveling:

> Wow - sooooo how can you afford to do this? That Pluralsight deal must have been pretty sweet!

I do enjoy working with Pluralsight, yes, but no I'm not independently wealthy by any stretch - _I very much need to work_. And, it turns out, working remotely is what I do so... **why does it matter if I have a house?**.

And that's where you start.

## Choices

After that it's all about the decisions - "am I/are we willing to...":

 - sublet/sell the house to free up mortgage/rent?
 - homeschool our kids, taking them out of school
 - travel for up to a year to make it worthwhile?
 - suspend our social life and exit some commitments for an extended time?

We dealt with each of these questions one by one and they were very, very hard. We had just moved to a really nice place in Hanalei, quite close to the beach. Our very good friends then moved next door! Leaving this setup was not a simple decision.

We thought about these questions and I quickly realized that thinking about them for too long would stop us from making this decision. So I asked my wife point blank:

> Do we want to do this?

Her answer was an emphatic "YES". We asked the kids as well and they said the same: "YES YES YES!" They were excited but all of a sudden I found myself hesitant. I don't know why I did but I decided to take an extra day to make sure I wasn't being knee-jerky and destroying what I felt was a pretty good life.

![](https://blog.bigmachine.io/img/bergs_girls.jpg)

But I knew I wanted to go. I'd always wanted to do this!

And that was the key: **we decided we wanted to do it, then tackled each issue in turn**. Here's what we ended up doing:

 - a good friend needed to move and was quite flexible with the dates. We subletted our house to her and she's watching our cats for us, taking over all the utilities while we're gone.
 - we talked to the kids' school and told them what we were doing - they were very excited and told us this could be one of the best educational years they ever have! We then read up on what other families are doing and came up with a bit of a plan
 - we decided to travel for at least a year
 - we said goodbye for now to our friends who were also very excited for us.

And that was basically that. In the span of 2 months we were able to pick up and leave and **it all started by deciding to do it**.

## Work

As I mentioned - I need to work while I'm gone. In fact I'm sitting here right now, taking a break from editing up my next Pluralsight video. I work remotely and have done so for the last 14 years of my career.

I don't say that casually - it took a lot of hard work and convincing people that I didn't need to be in the office. I was able to do this for smaller clients as well as when I worked at Microsoft - and it wasn't because I was super special or amazing - I made my bosses/clients a deal:

> If I'm not able to deliver for you, don't pay me. If you want to ensure that I don't deliver for you, make me work in your office.

![](https://blog.bigmachine.io/img/soest.jpg)

For many potential clients this didn't work, but for some it did. And I built a rapport with these people (usually referrals from friends) and quickly my remoteness was not a problem.

These days working remotely is fairly normal. If you've never done it, it might seem like a privileged thing or something "only great devs can do". Not at all - it's more important that you can deliver and that you can communicate well.

But there's more than this if you want to work while you travel: **you have to decide to do it**. I swear that's the magic! Once you decide, you find opportunities popping up for work that you hadn't thought of.

For me, I was watching a travel show one night (Anthony Bourdain) and thought "hey wouldn't it be fun to do something like this, but for developers?" I knew I was going on the trip - so I pitched the idea to Pluralsight.

They thought it was a fine thing to try out - so I'm piloting my first video with [Frans Bouma](http://weblogs.asp.net/fbouma). I had always wanted to meet Frans in person and I love the Netherlands so... why not? I pitched the idea to him and he was quite excited... so last week I hung out in The Hague and have about 3 hours of great footage which I'm editing up right now :).

The point is: **pull yourself to the task**. If you want to travel, decide to. Opportunities will come to you - you just need to be unafraid and allow yourself to jump.

## Bloggin It

For those who are curious - I've [set up a travel blog](http://www.alloverthemap.io) at Squarespace where I'm writing about the things we're doing. If you're the kind of person who just wants techie stuff and "no more family pics" - this isn't the blog for you. I set it up mostly for friends and family - but you're welcome to drop by and see what we're up to.

So far we've spent 3 weeks in Iceland, 2 weeks in the Netherlands and we're now in Germany staying in a really cool little village in Bavaria for 3 more weeks so I can get some work done.

We're not planning more than 3 weeks out and are relying on Airbnb, Booking.com, and have decided to drive where we can as opposed to using trains. We like trains - but they're very expensive and we're trying to stay in smaller places instead of the big cities which can be very expensive.

## We're Two Months Into This... And...

I'm having a very, very good time. I'm learning just [how much I rely on my stupid phone](http://www.alloverthemap.io/blog/2014/9/17/having-no-plan) for **everything** and it drives me crazy. I'm relearning German, meeting some incredibly nice people, and have, for some reason, developed a fascination with all things World War II.

![The Atlantikwall](/img/wall_hague.jpg)

Our kids are having a great time too and, so far, we haven't had any "I wanna go home" meltdowns - from any of us! I was expecting something to hit in the first few months but... not yet. We're still motivated and quite happy.

Each place we've visited has been spectacular - nothing is sticking out as "well that was OK but... " and with each place comes something new. New food, new smells, new air to breath and sites that are completely different then what we're used to.

I've been able to step away from my computer - to _want_ to step away from it so I can be with my family, out in the world. Right now they're sitting in the yard waiting for me - it's my turn to do math with the girls today. We found this farmhouse here in Bavaria that's wayyyy out in the boonies - and it costs much much less than a hotel in Munich.

It's 700 years old and is right next to a nature preserve with trails to castles. Nope, not kidding... I need to wrap this post up so I can go!

## Other Traveling Families

I know others are traveling as well and I'm hoping to meet up with them. [Shawn Wildermuth](http://wildermuth.com/worldtour) is out and about with his lovely wife and currently we're separated on the globe by about 600km. Unfortunately we just haven't been able to make our paths cross but I'm sure we'll find a way!

[The Fassbender family ](http://www.takingontheworld.net/world-travel-blog/) is making their way across the Southern Hemisphere (currently in NZ with a HUGE motorhome) and we've exchanged a few emails about meeting up in Europe. They have 2 girls almost the same age as ours, and we're trying to figure out the math thing.

Finally, our good friends Kelly and Kiko Perozo whom we know from Kauai [are almost four months into their trip around the world](http://www.a-family-afar.com).

If you're finding yourself getting motivated to do something like this - [here's a podcast for you](http://www.familyadventurepodcast.com) which is all about families traveling the world. It's pretty well done and can get you fired up quite quickly.

Feel free to drop me a line or leave a comment if you have any questions!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/amsterdam_canal.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/amsterdam_canal.jpg" />
  </entry>
  <entry>
    <title>Pulling Documents From a Relational Query in Postgres</title>
    <link href="https://bigmachine.io/posts/pulling-documents-from-a-relational-query-in-postgres" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.726Z</updated>
    <id>https://bigmachine.io/posts/pulling-documents-from-a-relational-query-in-postgres</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/relational-documents.jpg" alt="Pulling Documents From a Relational Query in Postgres" /></p>
## JSON, JSONB and Postgres 9.4

Upfront: Postgres isn't the only database system in town to formally support JSON; SQL Server, MySQL, and Oracle do as well. Really - JSON is just text so there's no miracle when it comes to "supporting" it in a relational system.

The fun comes with *how* a system supports JSON. With Postgres you have had the ability to query and manipulate JSON data directly - along with a speedy parser and document validation built right in. Now, with JSONB, you have **binary support** for JSON. For NoSQL lovers out there, this is a godsend.

Full indexing and fast querying are about to become a reality within the Postgres *relational* engine. This is fun news, **but how can you use it, and why should you care?**. Let's take a look at some fun ways to leverage JSON and Postgres.

## Turning Relational Bits into Documents

I'm building out a full system using Postgres and Node right now and I'm doing the typical thing: *querying for a single user and I want all the information back*. With an ORM, this means relying on eager/lazy loading garbage and making sure you have the right relationships defined in your code.

Let's see how we can do this cleaner with Postgres. For this, I'll use a function.

I have a few constraints for this - here they are:

 - I don't want the full data to come back over the wire. This means no hashed password, no admin flags, no tokens that aren't needed, etc. I just want the user info directly.
 - I want role names
 - I want to see all admin notes and logs (which are each 1-many off the Member table)

Let's start by building the skeleton of the function:

```sql
create or replace function get_member(member_id bigint)
as $$

-- bits

$$ LANGUAGE PLPGSQL;
```

This syntax is a bit odd at first. The create/replace stuff is obvious, but the $$ stuff isn't. These are language flags to tell the PG parser "here comes some code" and at the very end we have which language was used. We could use SQL here, but I want the power of PLPGSQL.

Next, let's define the return type. I'll send back only what I need for now:

```sql
create or replace function get_member(member_id bigint)
returns table (
	id bigint,
	email varchar(255),
	first varchar(25),
	last varchar(25),
	last_signin_at  timestamptz
)
as $$

DECLARE

  found_user members;

BEGIN

  select * from members where members.id = member_id into found_user;

  return query
  select found_user.id, found_user.email, found_user.status, found_user.first, found_user.last, found_user.last_signin_at

END;

$$ LANGUAGE PLPGSQL;
```

Note the main additions to the function - the first is to set a return type - in this case it's an "anonymous" table that we declare on the fly - this goes *before* the "as" statement.

Next we set up variable declarations using `DECLARE`. Here we want to play with a member record, so we declare it. Finally we populate that variable and return it below. We're almost there - let's add some additional info!

## Adding JSON

So far the query we have could be written in plain old SQL without the function ceremony - now let's get down to business. I'll add 3 more fields here for roles, logs, and notes:

```sql
create or replace function get_member(member_id bigint)
returns table (
  id bigint,
  email varchar(255),
  first varchar(25),
  last varchar(25),
  last_signin_at  timestamptz,
  notes json,
  logs json,
  roles json
)
as $$

DECLARE

  found_user members;
  parsed_logs json;
  parsed_roles json;
  parsed_notes json;

BEGIN
  select * from members where members.id = member_id into found_user;

  select json_agg(x) into parsed_logs from
	(select * from logs where logs.member_id=found_user.id) x;

  select json_agg(y) into parsed_notes from
	(select * from notes where notes.member_id=found_user.id) y;

  select json_agg(z) into parsed_roles from
	(select * from roles
	inner join members_roles on roles.id = members_roles.role_id
	where members_roles.member_id=found_user.id) z;

  return query
  select found_user.id, found_user.email, found_user.first, found_user.last, found_user.last_signin_at,
  parsed_notes, parsed_logs, parsed_roles

END;

$$ LANGUAGE PLPGSQL;
```

OK we did a bit more work here. I added notes/logs/roles to my return table, then 3 variables to hold the values for me. In the body I used the built-in `json_agg` function to aggregate and parse the passed-in select statements. Note that I can select whatever I values I want returned in these subqueries - for convenience I'm just returning all the fields for each.

It's likely that I'll have many logs and many notes, etc - but the `json_agg` function will drop this into an array for me... which is quite nice.

Finally, down below I'm returning the JSON values back so we can now query it.

## Calling This Using Node

If you use the simple `node-pg` module for Node, you can call this code easily:

```javascript
pg.connect("postgres://rob@localhost/membership", function (err, db, done) {
  assert.ok(err === null, err);
  db.query("select * from get_member($1)", [MY-ID], function (err, result) {
    //release the connection
    done(db);
    //throw on err
    if(err) throw err;
    //return the result
    next(null, result.rows);
  });
});
```

Since we're returning a table from our function, you have to query it like one. We do that with the `select * from get_member($1)` call above. Note the `$1` is a flag for the parameter input.

What do these results look like? That's the fun of working with Node and Postgres. The PG driver will see the JSON return type and parse it for you, so we get back a lovely document:

```json
{ id: '843350353876354049',
  email: 'test@test.com',
  first: 'Joe',
  last: 'Blow',
  last_signin_at: Fri Oct 31 2014 12:33:38 GMT+0100 (CET),
  logs:
   [ { id: 1,
       subject: 'Registration',
       member_id: 843350353876354000,
       session_id: null,
       entry: 'Added to system, set role to User',
       ip_address: null,
       created_at: '2014-10-31 12:33:38.006534+01' },
     { id: 2,
       subject: 'Authentication',
       member_id: 843350353876354000,
       session_id: null,
       entry: 'Activated member during registration',
       ip_address: null,
       created_at: '2014-10-31 12:33:38.006534+01' } ],
  notes: null,
  roles:
   [ { id: 99,
       description: 'User',
       member_id: 843350353876354000,
       role_id: 99 } ] }
```

That there almost looks like a full-blown document doesn't it! And that's the point - we were able to store information in our DB with a nice, tight normalized scheme, and pull it out like a document.

There are a number of other things I can do with this function - setting flags like `is_admin` or `can_login` based on status.

In fact I like this document return style so much, I've taken to wrapping commands (like Register or Authenticate) into documents with fields like "success", "message", and "data" where "data" might be the new record created. This kind of thing lets you pump prepared information right from Postgres out to your API without having to write a ton of intermediate formatting code.

I like it.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/relational-documents.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/relational-documents.jpg" />
  </entry>
  <entry>
    <title>Using Custom Types in Postgres</title>
    <link href="https://bigmachine.io/posts/using-custom-types-with-postgres" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.726Z</updated>
    <id>https://bigmachine.io/posts/using-custom-types-with-postgres</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/youre-my-type.jpg" alt="Using Custom Types in Postgres" /></p>
## Using Types For Fun and Profit

In [my last post about pulling documents from queries](http://wekeroad.com/2014/10/31/pulling-documents-from-a-relational-query-in-postgres/) I showed an interesting way to return a result set using `row_to_json` to crunch down 1-many records into a JSON array. This works pretty well and is really fast - but it's not exactly pretty:

```sql
create or replace function get_member(member_id bigint)
returns table (
  id bigint,
  email varchar(255),
  first varchar(25),
  last varchar(25),
  last_signin_at  timestamptz,
  notes json,
  logs json,
  roles json
)
as $$

DECLARE

  found_user members;
  parsed_logs json;
  parsed_roles json;
  parsed_notes json;

BEGIN
  select * from members where members.id = member_id into found_user;

  select json_agg(x) into parsed_logs from
  (select * from logs where logs.member_id=found_user.id) x;

  select json_agg(y) into parsed_notes from
  (select * from notes where notes.member_id=found_user.id) y;

  select json_agg(z) into parsed_roles from
  (select * from roles
  inner join members_roles on roles.id = members_roles.role_id
  where members_roles.member_id=found_user.id) z;

  return query
  select found_user.id, found_user.email, found_user.first, found_user.last, found_user.last_signin_at,
  parsed_notes, parsed_logs, parsed_roles

END;

$$ LANGUAGE PLPGSQL;
```

Beauty is in the eye of the beholder I suppose - this looks nice to me, but one thing stands out: **I don't like the anonymous table return style**. I think I'll probably want to use that again somewhere so let's set that up.

The first thing to do is resolve it to a type:

```sql
create type member_summary as (
  id bigint,
  email varchar(255),
  first varchar(25),
  last varchar(25),
  last_signin_at  timestamptz,
  notes json,
  logs json,
  roles json
);
```

Lovely. This is a composite type in Postgres - you can define your own base types if you want - but that's a whole other story. This composite type will do nicely.

Now we can rewrite the function to be a bit more concise:

```sql
create or replace function get_member(member_id bigint)
returns setof member_type
as $$

DECLARE

  found_user members;
  parsed_logs json;
  parsed_roles json;
  parsed_notes json;

BEGIN
  select * from members where members.id = member_id into found_user;

  select json_agg(x) into parsed_logs from
  (select * from logs where logs.member_id=found_user.id) x;

  select json_agg(y) into parsed_notes from
  (select * from notes where notes.member_id=found_user.id) y;

  select json_agg(z) into parsed_roles from
  (select * from roles
  inner join members_roles on roles.id = members_roles.role_id
  where members_roles.member_id=found_user.id) z;

  return query
  select found_user.id, found_user.email, found_user.first, found_user.last, found_user.last_signin_at,
  parsed_notes, parsed_logs, parsed_roles

END;

$$ LANGUAGE PLPGSQL;
```

Much better. You'll notice that instead of saying `returns TABLE` I now need to say it's a `setof` a type. A "type" in Postgres can be a base type (like int, varchar, etc) or a table - which is a composite type by itself. `members` is a type. If you want to create your own for reusability - you sure can!

Now we can reuse this type if we like - say by finding a member by email:

```sql
create or replace function get_member_by_email(member_email varchar(255))
returns setof member_type
as $$

DECLARE
  found_id bigint;
BEGIN
  select id from members into found_id where members.email = member_email;
  return query
  select * from get_member(found_id);

END;

$$ LANGUAGE PLPGSQL;
```

## Enums

I also have a logging table that keeps track of things in the system. For that, I like to know what type of log is being stored. If I was being strict, I'd have two tables, like this:

```sql
create table log_types(
  id serial primary key not null,
  description varchar(25)
);
create table logs(
		id serial primary key not null,
		subject_id int not null references log_types(id),
		member_id bigint not null references members(id) on delete cascade,
		entry text not null,
		data json,
		created_at timestamptz default current_timestamp
);
```

This works fine and there's a nice Foreign Key constraint in there to be sure I have some type of description. However there's a simpler way that, to me, is a bit more descriptive:

```sql
create type log_type as ENUM(
  'registration', 'authentication', 'activity', 'system'
);

create table logs(
    id serial primary key not null,
    subject_id log_type not null,
    member_id bigint not null references members(id) on delete cascade,
    entry text not null,
    data json,
    created_at timestamptz default current_timestamp
);

insert into logs (subject, member_id, entry)
values ('registration',11111,'Member registered');
```

This works basically the same way, but instead of having a simple integer in my logs table, I have the description itself with a constraint on it that it must contain one of the specified values.


Lovely. There's a lot more we can do here on the write-side of working with data. I'll cover that in the next post.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/youre-my-type.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/youre-my-type.jpg" />
  </entry>
  <entry>
    <title>Staying Hungry, Staying Foolish</title>
    <link href="https://bigmachine.io/posts/staying-foolish" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.726Z</updated>
    <id>https://bigmachine.io/posts/staying-foolish</id>
    <summary type="text">Playing The Fool Again
I&apos;m building a new little venture right now and, for the first time in years, &lt;strong&gt;I&apos;m flipped out excited&lt;/strong&gt;. No no not because I think it will change the world and make me stupid rich - no &lt;em&gt;I&apos;m just excited to build it because it&apos;s fun and I l</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2015/01/IMG_0961.jpg" alt="Staying Hungry, Staying Foolish" /></p>
## Playing The Fool Again
I'm building a new little venture right now and, for the first time in years, <strong>I'm flipped out excited</strong>. No no not because I think it will change the world and make me stupid rich - no <em>I'm just excited to build it because it's fun and I like to build things</em>. And dammit I'm having a great time.

So much so that I'm blogging again, and given the burnout that I've been dealing with over the last few years.. well that is no mere trifle.

The deal is this - I think I've finally figured out what has been bogging me down, fucking my head up and sapping my energy for doing... well anything really. And it's this: <em>I think I know too much.</em> Or, put another way, <strong>I think too much</strong>.

![](https://bigmachine.io/img/2015/01/IMG_1787.jpg)

I've spent the last 7 years of my career studying and sharing what I know through blog posts and videos and you know what? <em>That was a good time.</em> The problem with that, however, is that <strong>you tend to retain some of the things you learn.</strong> And after a while... the noise... oh MY GOD THE NOISE...

So, while on this little trip of mine I decided to read a wonderful book called [Zen Mind, Beginner's Mind](http://www.goodreads.com/book/show/402843.Zen_Mind_Beginner_s_Mind) which, apparently, is one of the foundational Zen books for westerners. I heard about it while reading an article about Steve Jobs, and apparently this book and its author were a very big influence on him and what he did at Apple.

Within 5 minutes my head was spun sideways:

> In the beginner’s mind there are many possibilities, but in the expert’s there are few

I don't think I'd ever call myself an expert in anything, but I do know that I like to learn things. The point here is still quite valid: *the more you know, the less you believe.*

## Back Then, When I Was Dumb(er)
I thought about this the other day when reading Gary Bernhardt's Twitter stream:

<blockquote class="twitter-tweet" lang="en">I wish it were possible to communicate the sense of unrestricted possibility that was so prevalent in software as recently as the late 90s.

— Gary Bernhardt (@garybernhardt) <a href="https://twitter.com/garybernhardt/status/558771376736055300">January 23, 2015</a></blockquote>
<script src="//platform.twitter.com/widgets.js" async="" charset="utf-8"></script>

**I lived that.** I'm not proud of the software I wrote - most of it was <em>utter shit</em> by today's standards. But I remember how everything was changing, the world was opening up and I felt like if I just gave it some time, pried open a few books for code samples, that I could build *some seriously amazing stuff.*

In software terms I didn't, but to my clients - it was pure magic. And these weren't small clients either, they were quite large. I sat in meetings of 30 or more developers at Fortune 20 companies, flew to Chicago and New York from San Francisco regularly and was trusted to oversee multi-million dollar projects.

We built simulators using <strong>Visual Basic </strong>of all fucking things! With Lernout-Hauspie voice recognition and SOAP wire ups to an ASP Classic backend! It was XML madness with shouting at 2am to <em>get that fucking thing working an on the server!!!! </em>

We created the very first data-driven web app on the intranet of a major phone company (the largest baby bell in 1998) using Active Server Pages and SQL Server with some ActiveX love on IE4. Wow, just writing this right now is making me giggle - the code... was so very... wow... but man did it make some magic happen.

And that's the thing:  it <em>literally made people cry</em> who were used to working on main frame silliness and enterprise grade crapware. <strong>We had this new thing called the web and we were going to rock this shit. </strong>And rock it we did! I sat in a Usability Survey at this very big baby bell with cameras pointed at all parts of the test subject and watched them find the answer to "how do I install a phone line in my boat in the marina?".

I had pulled in over 10,000 text file dumps from their current main frame system and I used Windows NT4 with IndexServer to scan the files - which took about 10 minutes total. I then hooked up Active Server Pages to THAT and had a query that returned in 20 milliseconds. When I demoed this to my client he <em>literally punched me. </em> And we made the user cry with the speed and ease of use of our app.

![](https://blog.bigmachine.io/img/2015/01/350opindexlocation.gif)

That's right... breathe it in...

That's the best I've ever felt doing my job. <em>It was glorious.</em> And while the software I wrote was crap by today's standards I was on fire with the <strong>possibility that existed with my career.</strong>

<strong>And now here I am in 2015 writing blog posts on JavaScript</strong>. God damn that's really depressing.

![](https://blog.bigmachine.io/img/2015/01/iceice.jpg)

Yo. JavaScript?


## Seriously. WTF.

*What the hell has happened here?* Yeah yeah I know JavaScript is a fine language and yeah yeah yeah I know you can make it sing and dance if you focus on the right things etc. That's what I've been doing for the last few years, I get it. I guess I figured that, 16 or so years on we might have done something... you know... <strong>relevant</strong> with the web. Maybe you think we have, I don't.

Perhaps I expect too much, or maybe I haven't tried hard enough. It seems like all the great things have happened and all that we're doing these days is <strong>porting all that great shit backwards to JavaScript.</strong>

I know, hot wind and whining don't solve anything, and rather than change the world to suit me perhaps I should try, once again, to have some fun. Which brings me back to this quote:

>n the beginner’s mind there are many possibilities, but in the expert’s there are few

<strong>Possibilities</strong>. Back then there were so many - <em>you knew the technology was crap</em> but you also <em>believed it would get better as time went on</em>. In many ways it has, in many ways it hasn't (see 3 paragraphs above).

![](https://blog.bigmachine.io/img/2015/01/Star_Child___from___2001___by_Lukasx.jpg)

## Repaving My Brain

<strong>So I've decided to repave. </strong>This doesn't mean I'm rejecting the things I've learned and the things I've written about/made videos about - nope those things are all very relevant and real - things you should know.

I'm simply shoving those things out of the way because they are causing me to smother my inspiration, my belief in impossibility and my willingness to <em>do completely stupid things</em> because they are there. Note that I'm not saying the things I know are bad - <strong>it's my attention to that knowledge which is destroying my inspiration.</strong>

[I like how John Sonmez puts it](http://simpleprogrammer.com/2012/07/23/when-being-good-is-bad):

> The problem is that when we start out as software developers we don’t know the “right way” to do things so we are less constrained in what we do.  We just march forward and go do things.

As we start to learn the “right way” to do things, we are often stifled by that knowledge and the constraints it brings and that causes us to be less productive or to over engineer and design solutions to problems.</blockquote>
This very sentiment was echoed in [Steve Jobs amazing commencement address](http://news.stanford.edu/news/2005/june15/jobs-061505.html) to the graduating class of Stanford in 2005:

> When I was young, there was an amazing publication called The Whole Earth Catalog</em>, which was one of the bibles of my generation. It was created by a fellow named Stewart Brand not far from here in Menlo Park, and he brought it to life with his poetic touch. This was in the late 1960's, before personal computers and desktop publishing, so it was all made with typewriters, scissors, and polaroid cameras. It was sort of like Google in paperback form, 35 years before Google came along: it was idealistic, and overflowing with neat tools and great notions.

> Stewart and his team put out several issues of <em>The Whole Earth Catalog</em>, and then when it had run its course, they put out a final issue. It was the mid-1970s, and I was your age. On the back cover of their final issue was a photograph of an early morning country road, the kind you might find yourself hitchhiking on if you were so adventurous. Beneath it were the words: "Stay Hungry. Stay Foolish." It was their farewell message as they signed off. Stay Hungry. Stay Foolish. And I have always wished that for myself. And now, as you graduate to begin anew, I wish that for you.

>Stay Hungry. Stay Foolish.

**I'm going to be foolish for a bit**. In fact I've already started in both my personal and professional life. I'm building an idea that I think could be really fun - and we'll see. It might suck, it might not.

But I'm having fun and **that's what this career is all about.**]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2015/01/IMG_0961.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2015/01/IMG_0961.jpg" />
  </entry>
  <entry>
    <title>Using Entity Framework 6 with PostgreSQL</title>
    <link href="https://bigmachine.io/posts/using-entity-framework-6-with-postgresql" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.725Z</updated>
    <id>https://bigmachine.io/posts/using-entity-framework-6-with-postgresql</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/MadScientist.jpg" alt="Using Entity Framework 6 with PostgreSQL" /></p>
## You Can Do It, Yes You Can... But...

This post is about exploring things and maybe trying something new. This isn't a production-grade solution because, frankly, it's slow and SQL Server is built into the bones of Entity Framework. Swimming upstream is no fun.

BUT! **This is the essence of hacking** - trying to get something to work in a way that it wasn't designed to. So... get your goggles on and prepare the rubber room... here we go...

## Step 1: Install Postgres

Since we're working with EF I'm assuming you're on Windows - [so head on over to postgresql.org](http://www.postgresql.org/download/windows/) and download the latest version. 9.4 isn't due out until Q3 2014 so you'll probably be downloading 9.3.

Run the installer and reflect on [installing SQL Server](http://msdn.microsoft.com/en-us/library/ms143219.aspx). Even if you used Web PI to install express... it takes a while. Make sure you remember your passwords for the root user "postgres" - you'll need this later on.

## Step 2: Create a Database

You should have a GUI tool installed for administering Postgres called "pgAdmin III" - just hit the Windows key and type "pg" and you should see it. It leaves a bit to be desired - but we'll fix that in just a bit. Open it up and poke around - much of this will look foreign to you but as soon as you navigate around a bit, you'll know what's going on.

For now, just double click your server and right-click on "Login Roles" - set one for yourself, and then in the "Role Privileges" tab give yourself access to everything.

Next, close the Roles window and right-click on "Databases", select "New Database" and create one called "petstore", setting your new login as the owner. Yeah... petstore... why not.

![Postgres Database Window](/img/pg_databases.png)

Close it up - you're done.

## Tangent: Tooling

If you haven't used postgres I know you're underwhelmed right now... I would be too. I normally use the command line (psql) as I've learned the commands and find it very useful - but I know that can be a hard sell.

We're at the point where we need to create some tables, and **EF does not support migrations or code-first creation with Postgres**. Or, rather, it _does_ but you need to have a custom SQL generator to do it.

There are some out there - but I'd rather do something a bit different. In the Node/Rails/Python/Everywhere Other Than .NET worlds, Postgres is widely used. Many times you'll see multiple tools used in a single project - Rakefiles in Node projects or Gruntfiles with Rails.

That's what I'm going to do next - I like the simplicity and utility of Node and I think you will too. I hope.

## Step 3: Migrations

There are a number of migration projects out there for .NET. I've written some myself - but they're all a bit complicated. I love the simplicity of JavaScript for this so let's try something new.

[Install Node if you haven't](http://nodejs.org), and if you haven't had a chance to play with Node - today's the day!

Now, let's open Visual Studio and create a simple console application - call it what you will. Crack open NuGet and add in:

- EntityFramework
- Npgsql (the .NET driver for Postgres)
- Npgsql.EntityFramework (the EF provider)

Next, go to Tools/Extensions and Updates and find "dotConnect Express for PostgreSQL". This is very nice, free driver for Postgres that will allow us to hook up to Visual Studio - we'll do that in a minute.

![dotConnect for PG](/img/dotconnect.png)

Now let's open up the Package Manager Console in Visual Studio - it's time to work with Node. We want to install the tools we'll be using to migrate our database. The first is `db-migrate` - [a really useful utility](https://github.com/kunklejr/node-db-migrate) that does one thing well: migrate your DB.

```sh
npm install db-migrate -g
```

`db-migrate` is an executable so we need to be sure we can access it from our command line in the Package Manager Console - so we'll install it globally - thus the `-g` flag above.

Let's to the same thing with the Node postgres driver:

```sh
npm install pg -g
```

_If you get an error about "node-gyp" this is because it wants to use Python to compile some optimizations - you can ignore this._

Now we need to setup our database config - flip over to Visual Studio and create a file in the root of your project called "database.json". In this file we can add some connection info:

```json
{
  "dev": "postgres://rob:password@localhost/petstore",

  "test": {
    "driver": "sqlite3",
    "filename": ":memory:"
  },

  "prod": {
    "driver": "pg",
    "user": "joe",
    "password": "toottoot",
    "database": "petstore"
  }
}
```

Two nice things about this - you can separate connections based on environment, and you can be flexible in terms of connection string format. The one I'm using (`dev`) is a nice, concise format that's easy to remember - just replace "rob" and "password" with the login you created.

Now let's run it!

```sh
db-migrate create monkey
```

Now head over to Visual Studio and "Show All Files" - you should see a new directory call "migrations" with a file in there, like this:

![migrations](/img/db-migrations.png)

Now open it up and take a look at the familiar "up" and "down" functions. Let's fill those out with the examples from the [Github repo](https://github.com/kunklejr/node-db-migrate) with one minor change - I want my key to be of type "serial", which is how you setup auto-incrementing keys in Postgres:

```javascript
exports.up = function (db, callback) {
  db.createTable(
    "pets",
    {
      id: { type: "serial", primaryKey: true },
      name: "string",
    },
    callback
  );
};

exports.down = function (db, callback) {
  db.dropTable("pets", callback);
};
```

Save the file, and let's migrate!

```sh
db-migrate up
```

If you get an error here that says "Can't find module database.json" it's because you're a) not running in the same directory as your database.json file or b) you have a JSON error.

Wahoo! Migrated!

Now let's take a look at our handiwork in Visual Studio. Open up the Server Explorer and right click on "Data Connections". You'll see the familiar dialog for DB credentials - but here you want to change the driver from Microsoft SQL Server to "PostgreSQL". Click the "Change" button and select "PostgreSQL".

You can see this selection here because you installed the dotConnect Express tool above. If you don't see Postgres as an option - the install went wrong.

Set your connection information up, and test the connection. It should look like this:

![Connecting to Postgres](/img/dotconnect-connection.png)

You'll notice that you can query and browse data, but not much else. This is because the Express version is limited - if you upgrade you get all kinds of goodness... still cheaper then SQL Server. I don't use this tool... I just use Npgsql myself.

## Setting up EF

Believe it or not - this is the easy part! Flip back over to Visual Studio and confirm that you have EF installed from Nuget. Now, let's configure our App.config:

```xml
<?xml version="1.0" encoding="utf-8"?>
<configuration>
  <configSections>
    <!-- For more information on Entity Framework configuration, visit http://go.microsoft.com/fwlink/?LinkID=237468 -->
    <section name="entityFramework" type="System.Data.Entity.Internal.ConfigFile.EntityFrameworkSection, EntityFramework, Version=6.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089" requirePermission="false" />
  </configSections>
  <startup>
    <supportedRuntime version="v4.0" sku=".NETFramework,Version=v4.5" />
  </startup>
    <system.data>
        <DbProviderFactories>
            <add name="Npgsql Data Provider"
                  invariant="Npgsql"
                  description="Data Provider for PostgreSQL"
                  type="Npgsql.NpgsqlFactory, Npgsql" />
        </DbProviderFactories>
    </system.data>
    <connectionStrings>
        <add name ="MonkeyFist" connectionString="server=localhost;user id=rob;password=password;database=petstore" providerName="Npgsql"/>
    </connectionStrings>
  <entityFramework>
    <defaultConnectionFactory type="System.Data.Entity.Infrastructure.SqlConnectionFactory, EntityFramework" />
    <providers>
      <provider invariantName="Npgsql" type="Npgsql.NpgsqlServices, Npgsql.EntityFramework" />
    </providers>
  </entityFramework>
</configuration>
```

Two things to note here - the first is that we needed to add a Provider to the DbProviderFactories (Oh I just love saying that out loud). Next, we needed to tell Entity Framework to use the Npgsql provider that we installed. Not all that much wiring...

Finally I added a connection string pointing to my PG database (be sure to set your password as needed).

OK let's write some code already!

## Querying with EF

Let's drop in some code for accessing our data. We need the usual thing - a Context and a Class:

```csharp
[Table("pets",Schema="public")]
public class Pet{
  [Key]
  [Column("id")]
  public int ID { get; set; }
  [Column("name")]
  public string Name { get; set; }
}
public class DB : DbContext {
  public DB(): base(nameOrConnectionString: "MonkeyFist") {}
  public DbSet<Pet> Pets { get; set; }
}
```

I had to make a few concessions here because I'm working with Postgres. The first is that I needed to tell EF to use "public" instead of "dbo", which is ridiculous. Postgres is case-sensitive by default, so I also needed to tell EF column names and table names - as well as the primary key.

A bit of a bummer, but it's nice that I can do that.

The DbContext is nothing scary - I just pass in the connection string name here and we're set.

Now let's run it!

```csharp
  class Program {
    static void Main(string[] args) {

      var db = new DB();

      var pet = new Pet { ID = 1, Name = "Stevie" };
      db.Pets.Add(pet);
      db.SaveChanges();

      var pets = db.Pets;
      foreach (var p in pets) {
        Console.WriteLine(p.Name);
      }
      Console.Read();
    }
  }
```

And that should pop up a console for you:

![Stevie](/img/stevie.png)

## Final Thoughts

So, would I ever do this? **Nope.** Well, that's not exactly true - I love using Node to help out with projects, Grunt (or Gulp) in particular. I really like the way migrations work here as well - so yes I'd use that.

But EF is a monster. If you're not using SQL Server all kinds of fun little things pop up to trip you - such as the way I got to decorate my class above with all kinds of lovely attributes.

It's also a bit slow, for some reason. I know this isn't LINQ, and I know it's not the driver - for the last week I've been playing around with ... shall we say a fun little library that might see the light of day. It's super fast and uses LINQ against Postgres (with Unit of Work even!).

All the same, this is a fun exercise if only to investigate new things. Might not be ready for prime time, but if it helps you to try new things and gets your mind fired up, yay!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/MadScientist.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/MadScientist.jpg" />
  </entry>
  <entry>
    <title>Repositories On Top UnitOfWork Are Not a Good Idea</title>
    <link href="https://bigmachine.io/posts/repositories-and-unitofwork-are-not-a-good-idea" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.725Z</updated>
    <id>https://bigmachine.io/posts/repositories-and-unitofwork-are-not-a-good-idea</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/darth_swimming.jpg" alt="Repositories On Top UnitOfWork Are Not a Good Idea" /></p>
## The Rationale

It's generally believed that by using the Repository pattern, you can (in summary) "decouple" your data access from your domain and "expose the data in a consistent way". 

[If you look at any of the implementations of a Repository working with a UnitOfWork (EF)](http://www.asp.net/mvc/tutorials/getting-started-with-ef-5-using-mvc-4/implementing-the-repository-and-unit-of-work-patterns-in-an-asp-net-mvc-application) - you'll see there's not all that much "decoupling":

```csharp
using System;
using System.Collections.Generic;
using System.Linq;
using System.Data;
using ContosoUniversity.Models;

namespace ContosoUniversity.DAL
{
    public class StudentRepository : IStudentRepository, IDisposable
    {
        private SchoolContext context;

        public StudentRepository(SchoolContext context)
        {
            this.context = context;
        }

        public IEnumerable<Student> GetStudents()
        {
            return context.Students.ToList();
        }

        public Student GetStudentByID(int id)
        {
            return context.Students.Find(id);
        }

        //<snip>
        
        public void Save()
        {
            context.SaveChanges();
        }

    }
}

```

This class can't exist without the `SchoolContext` - so what exactly did we decouple here? **Nothing**.

In this code, from MSDN, what we have is **a reimplentation of LINQ**, with the classic problem of the "ever-spiraling Repository API". By "spiraling API" I mean fun things like "GetStudentByEmail, GetStudentByBirthday, GetStudentByOrderNumber" etc.

But that's not the primary problem here. The primary problem is the `Save()` routine. It saves a Student... I think. What else does it save? Can you tell? I sure can't... more on this below.

## UnitOfWork is Transactional

A Unit of Work, as it's name applies, is there to **do a thing**. That thing could be as simple as retrieving records to display, or as complex as processing a new Order. When you're using EntityFramework and you instantiate your DbContext - you're creating a new UnitOfWork.

With EntityFramework you can "flush and reset" the UnitofWork by using `SubmitChanges()` - this kicks off the change tracker comparison bits - adding new records, updating and deleting as you've specified. **Again, all in a transaction**.

## A Repository Is Not a Unit of Work

Each method in a Repository is supposed to be an atomic operation - once again either pulling stuff out, or putting it back in. You could have a SalesRepository that pulls catalog information, and that transacts an order. 

The downside to using a Repository is that it tends to spiral, and pretty soon you have one repository having to reference the other because you didn't think the SalesRepository needed to reference the ReportsRepository (or something like that).

This quickly can become a mess - **and it's why people starting using UnitOfWork**. UnitOfWork is an "atomic operation on the fly" so-to-speak. 

## The Only Thing You Could Do Worse: Repository < T >

This pattern is maddening. It's not a Repository - it's an abstraction of an abstraction. Here's one that's quite popular for some reason:

```csharp
public class CustomerRepository : Repository < Customer > {
  
  public CustomerRepository(DbContext context){
    //a property on the base class
    this.DB = context;
  }

  //base class has Add/Save/Remove/Get/Fetch

}
```
 On the face of it: _what's wrong with this?_ It's encapsulating things and the Repository base class can use the context so... what's the problem?

The problems are Legion. Let's take a look...

### Do You Know Where That DbContext Has Been?

No, you don't. It's getting injected and you have no idea which method opened it, nor for what reason. The idea behind Repository<T> is code "reuse" so you'll probably be calling it from a Registration routine, maybe a new order transaction, or from an API call - who knows? Certainly not your Repository - **and this is the main selling point of this pattern!**. 

The name says it all: **UnitOfWork**. When you inject it like this you don't know where it came from.

### "I Needed The New Customer ID"

Consider the code above in our `CustomerRepository` - it will add a customer to a the database. But what about the new CustomerID? You'll need that back for creating a log file and so you what do you do? Here's your choice:

 - Run `SubmitChanges()` right in your Controller so the changes get pushed and you can access the new CustomerID
 - Open up your CustomerRepository and override the base class `Add()` method - so it _runs `SubmitChanges()`_ before returning. This is the solution that the MSDN site came up with, and it'a bomb waiting to go off.
 - Decide that all Add/Remove/Save commands in your repo should `SubmitChanges()`

Do you see the problem here? The problem is in the implementation itself. Consider _why you need the new CustomerID_ - it's likely to do something else such as pop it onto a new Order object or a new ActivityLog.

What if we wanted to use the StudentRepository above to create a new student when they bought books from our book store. If you pass in your data context and save that new student... uh oh. You're entire transaction was just flushed.

Your choice now is to a) not use the StudentRepository (using OrderRepository or something else) or b) remove SubmitChanges() and have lots of fun bugs creep into your code.

If you decide to not use the StudentRepo - you now have duplicate code...

> But Rob! EF does this for you transactionally - you don't need to SubmitChanges just to return the new ID - EF does it in the scope of the transaction already!

**That. Is. Correct**. And it's also my point - which I'll come back to.

### Repositories Methods Are Supposed To Be Atomic

That's the theory anyway. What we have in Repository<T> is not a repository at all - it's a CRUD abstraction that doesn't do anything business-related. Repositories are supposed to be focused on specific operations - this one isn't.

If you're not using Repository<T> then you know it's almost impossible to avoid having "Repository Overlap Insanity" - losing all transactionality (and sanity) as your app grows.

## OK Smart Guy - What's the Answer?

There are two ways to stop this over-abstraction silliness. The first is Command/Query separation which at first might look a bit odd - but you don't need to go Full CQRS - just enjoy the simplicity of doing what's needed and no more...

### Command/Query Objects

Jimmy Bogard wrote a great post on this and I've tweaked his example a bit to use properties: but basically you can [**use a Query or Command object**](http://lostechies.com/jimmybogard/2012/10/08/favor-query-objects-over-repositories/):

```csharp
public class TransactOrderCommand {
  public Customer NewCustomer {get;set;}
  public Customer ExistingCustomer {get;set;}
  public List<Product> Cart {get;set;}
  //all the parameters we need, as properties...
  //...

  //our UnitOfWork
  StoreContext _context;
  public TransactOrderCommand(StoreContext context){
    //allow it to be injected - though that's only for testing
    _context = context;
  }

  public Order Execute(){
    //allow for mocking and passing in... otherwise new it up
    _context = _context ?? new StoreContext();

    //add products to a new order, assign the customer, etc
    //then...
    _context.SubmitChanges();

    return newOrder;
  }
}
```

You can do the same thing with a QueryObject - read Jimmy's post for more on this but the idea is that a query as well as a command has a specific reason for existence - you can change as needed and mock as needed.

### Embrace Your DataContext

This is an idea that [Ayende came up with] and I absolutely love it: wrap what you need in a filter or, use a Base Controller (assuming you're using a web app):

```csharp
using System;
using System.Web.Mvc;
 
namespace Web.Controllers
{
  public class DataController : Controller
  {
    protected StoreContext _context;
 
    protected override void OnActionExecuting(ActionExecutingContext filterContext)
    {
      //make sure your DB context is globally accessible
      MyApp.StoreDB = new StoreDB();
    }
 
    protected override void OnActionExecuted(ActionExecutedContext filterContext)
    {
      MyApp.StoreDB.SubmitChanges();
    }
  }
}

```

This will allow you to work with the same DataContext in the scope of a single request - you just need to be sure to inherit from DataController. This means that each request to your app is considered a UnitOfWork... which is quite appropriate really. In some cases it may not be - but you can fix that with QueryObjects above.

## Neat Ideas - But I Don't See What We've Gained

We've gained a number of things:

 - **Explicit Transactions**. We know exactly where our DbContext has come from, and what Unit of Work we're executing in each case. This is helpful both now and into the future.

 - **Less Abstraction == Clarity**. We've lost our Repositories which didn't have a reason to exist other than to abstract an existing abstraction. Our Command/QueryObject approach is cleaner and the intent of each one is clearer.

 - **Less Chance of Bugs**. The Repository overlap (and worse yet: Repository<T>) increases the chance that we could have partially-executed transactions and screwed up data.

So there it is. Repositories and UnitOfWork don't mix and hopefully you've found this helpful!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/darth_swimming.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/darth_swimming.jpg" />
  </entry>
  <entry>
    <title>A Simple Approach to BDD</title>
    <link href="https://bigmachine.io/posts/pragmatic-bdd" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.725Z</updated>
    <id>https://bigmachine.io/posts/pragmatic-bdd</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/pragmatic_bdd.png" alt="A Simple Approach to BDD" /></p>
## So. Many. Opinions.

I want to start here as it's some of the first ... "feedback" I received when I told my friends what I was up to:

> Oh no. Not that again.

BDD causes arguments and people become rather obsessed with what it is, the syntax you use, the tools you use, and how you part your hair when doing it. Some of these things are indeed important - but I think it's fair to say that a bit of a Cargo Cult has emerged because of it.

[I wrote a fairly long post about this](http://bigmachine.io/2013/08/28/how-behavioral-is-your-bdd/) a few months back as I was putting this production together, and now I'm happy to report that [the screencast is now live at Pluralsight](http://pluralsight.com/training/Courses/TableOfContents/pragmatic-bdd-dotnet).

My goal with this screencast was to strip all the noise away, focus on [Dan North's Big Aha Moment](http://dannorth.net/introducing-bdd/), and see what we could do with XUnit, Visual Studio and EntityFramework. I also used [NCrunch](http://www.ncrunch.net/) to shorten the "feedback loop". I have to say, **this was the most fun testing I've had, ever**.

## A Focus On Simple

I think BDD has become a cargo-cult in the .NET space, with people focusing on tools and syntax - losing focus entirely on the core idea behind BDD (testing the behavior, not the mechanics, of your application).

![BDD Jargon](/img/bdd_jargon.png)

I like many of the tools out there, but more and more I hear people ask me if I used "BDD framework X" or "BDD tool Y" for this production. My answer:

> No, I used Visual Studio, XUnit, and EntityFramework.

![BDD with VS](/img/membership_specs.png)

If you like SpecFlow (it's a great project) then _rock you some gherkin good dev_. I like the clarity and simplicity of the tests I wrote using XUnit and nothing else:

```csharp
[Trait("Authentication", "Password doesn't match")]
public class PasswordDontMatch : TestBase {

  AuthenticationResult _result;
  public PasswordDontMatch() {
    var app = new Application("rob@tekpub.com", "password", "password");
    new Registrator().ApplyForMembership(app);

    _result = new Authenticator().AuthenticateUser(new Credentials { Email = "rob@tekpub.com", Password = "fixlesl" });

  }
  [Fact(DisplayName = "Not authenticated")]
  public void NotAuthenticated() {
    Assert.False(_result.Authenticated);

  }
  [Fact(DisplayName = "Message provided")]
  public void MessageReturned() {
    Assert.Contains("Invalid email", _result.Message);
  }
}
```

## Over The Shoulder

I was talking to [Mr. Hanselman](http://hanselman.com) the other day and I asked him if he likes fancy fonts for his slides and where he gets his color palettes from. His reply got me thinking:

> I don't do slides anymore, son.

And it's true - he doesn't (unless he has to - which is weird some conferences insist you use at least one slide). I've always appreciated his ability to pick excellent demos and talk his way through the code - showing **real results** that underscore what he's saying rather then silly pictures and diagrams.

So that's what I did for this (although there are some slides in there for conceptual things... just a few. Sorry Scott).

I do a live-coding thing and I wanted you to feel like you are sitting next to me. I clipped out all the umms/ahhs/oh-damn's and kicked up the pace so it's watchable, but I really wanted you to get that "pair-coding" feel. I'm hoping to see more of this at Pluralsight as well.

Anyway - [it's up and live](http://pluralsight.com/training/Courses/TableOfContents/pragmatic-bdd-dotnet) so go watch it. If you're not a Pluralsight member [got get a free 30 days](http://pluralsight.com/training/Products/Individual) and "try my product" - they have a huge library with [all of our stuff in it](http://pluralsight.com/training/Courses/Find?highlight=true&searchTerm=tekpub) (the migration is complete) so **get you some**!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/pragmatic_bdd.png" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/pragmatic_bdd.png" />
  </entry>
  <entry>
    <title>Writing a Better Abstract</title>
    <link href="https://bigmachine.io/posts/writing-a-better-abstract" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.725Z</updated>
    <id>https://bigmachine.io/posts/writing-a-better-abstract</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/write-better-abstract.jpg" alt="Writing a Better Abstract" /></p>
## Channeling Dexter

The first thing that any abstract needs to do is connect with the potential audience. This is hard, especially when you're dealing with some ultra-technical stuff.

Let's start with a success. My friend [Rob Sullivan](http://datachomp.com) wanted to speak at NDC Oslo last year (my absolute favorite conference on the planet) and sent me this note:

> I'm having a little trouble getting my thoughts in order for 1 of my NDC abstracts-

> **How to Murder Your SQL Server and Get Away With It:**

> _Is .NET without SQL Server possible? Not only is it possible, it is cheaper, faster and more liberating to do so. Using Redis and postgres we will be murdering our SQL Server in the most loving way possible. Who knows, by the end we may even find ourselves not even needing Windows._

From this I could tell that Rob wanted to be edgy - really edgy. Rob is/was a SQL Server DBA who just fell in love with Postgres and wanted to share this with his audience by, essentially, trolling them. Rob's sense of humor is pretty... dry and definitely a bit "in your face" - but that doesn't come through at all in the abstract above.

His complaint was spot-on: his thoughts were a bit jumbled and his humor wasn't coming through. So we exchanged a few emails and he said something that completely put a picture in my head:

> The talk itself is sort of Dexter like with references to the Dark Passenger and needing to find justice for the app...

Wow. What an image. Next thing I knew, I was writing - taking [the monologue from Dexter's opening](https://www.youtube.com/watch?v=HDVZuqRuAYs) and bending it around a bit:

> **How to Murder Your SQL Server and Get Away With It:**

> It's come to this: tonight's the night. All the waiting, the deadlocks, corruption of tempdb and devastating abuse to innocent SQL Statements… tonight's the night I let my Dark Passenger roam free and solve… finally solve this problem that plagues developers the world over: **SQL Server**.

> There it sits, pretending… just like me. Pretending to love my data and be an upstanding citizen of our development group. All the while… in the dark, fetid reaches of it's kernel it's silently plotting. Soaking up RAM and carefully laying licensing traps that will suck the blood from our company… But not this time.

> This time I won't let it - and I have a plan. I won't do it alone - I'll bring in my trusty friend Postgres to confront SQL Server and force it to see the evil thing it's become for developers. And when the time is right, and SQL Server is strapped to the table begging to be set free, I'll pull out my favorite weapon of choice… shiny, simple, sharp and brilliant: Redis.

> And I'll send it straight into SQL Server's heart once and for all.

This is SO OVER THE TOP but I can totally hear Rob's voice in it - **and it gives me a sense of what the talk will be about**: Alternatives to SQL Server. And I'll probably be entertained by some shock value.

Rob's talk was accepted with some very slight tweaks to this abstract, and the room was packed. One of the best talks I went to - Rob is a pro.

## Solve A Problem, Answer a Question. Above All: Entertain

You don't need to sing and dance to entertain. In a Good Talk, someone will come away having learned something - which is always a fun thing to do. In a Great Talk, they'll come away smiling, happier then when they went in - remembering some of your quotes or slides, telling their friends how good the talk was.

Always remember: **people want you to be great**. This is mostly for selfish reasons because everyone wants to have been in "that talk" where the presenter just nailed it.

And it all starts with the title and abstract of your talk. Walking the conference halls or sitting in the hotel in the morning - people will have their schedules open on their phones (or in their hands) looking for a talk to go to. They're looking for a name and failing that, a title/subject that sounds interesting.

Read Rob's title and abstract above. Would you choose that over Yet Another Talk About JavaScript? I sure would.

## Stay Focused, Tell Me What You're Going To Say

I got an email today from my friend [Anders Ljusberg](http://twitter.com/codinginsomnia) asking for a bit of help with a failed submission. He felt his abstract might have been a bit boring, or maybe didn't convey everything in his talk correctly. I asked if I could help by blogging about it (as I've been meaning to blog about this) and he said "YES!".

Anders then gave me this background:

> The talk I submitted to Øredev is about CQRS and what I've learned so far by working with it for the past few years. I suppose one problem may be that it's not as "hip" of a subject as it was three years ago but I'm quite sure there are plenty of attendees out there who'd be interested in the topic if I could sell it to them.. :)

Yep. I'd be one of them. Here's his title/abstract:

> **Commands, Events, Views and everything in-between**

> CQRS and Event Sourcing. When you see it in theory it looks quite easy, doesn't it? You just need some Command Handlers, an Aggregate Root, a few View Builders and then you're done!

> Well, obviously there's the Event Store too. Probably a Service Bus. And let's not forget the NoSQL database. Oh did I mention that many businesses prefer it if there are redundant servers that handle the load? And that a View Builder generally doesn't like it when you update the same model simultaneously from two different instances? But at least you're certain that the messages going through your Service Bus are consumed in the order they were published, right..?

> In this session I will take you through some of the gotchas I've run into when implementing CQRS based systems and show you how you can handle them. Expect some diagrams, expect code, and definitely expect a demo with plenty of moving parts.

From this I can say that **I have no idea what this talk is actually about**. Here's why

- The title says CQRS, but the abstract tosses a lot of jargon around
- There are leading questions in there without any resolution, assuming that I might somehow understand what the point is.
- The last sentence makes the talk sound like a bit of a variety show: some dancing, some singing, and a juggling cat

The abstract also suffers a bit from Anders sense of humor not really coming through: "_You know about this thing, right? And this other thing, right? Well, obviously..._" This can come off as pedantic and condescending unless you know the speaker - and I have to be honest - reading this abstract in the hotel room in the morning... I'd be hard-pressed to go (but I know Anders so I probably would).

How can we fix this up?

Anders point is that there's a lot to know about CQRS and that initially it can be a simple thing to think about. But, like most of programming, there are problems to deal with under certain circumstances (scaling, client demands, etc).

Let's focus this talk by telling the audience what it's about, and what problem we're going to solve. I always like to use a bit of humor if the talk will be humorous - if you're not a funny person then make sure your abstract is straightforward. Anders is a funny person so let's have the title mirror his personality. We'll start with 3 takes and see what happens:

- **Everyone's Got a Plan Until They Get Hit: My Adventures Working With CQRS Everyday Over the Last Four Years**. A famous line from Mike Tyson - so true - and it echoes the idea of theory vs. reality and "this is what I've learned".
- **A Thousand Cuts of CQRS**. Echoes the idea of "experience through small, tiny bits of pain".
- **Taking the Service Bus To the Event Store, and Other Bad CQRS Puns**. Some people like puns - a bit of a weaker title but if your goal is to talk about jargon and you like puns... there's a lot you could do here with your slides.

OK, we have some working titles. If you have spoken before and you think your talk will be accepted - maybe go a bit more risky. If you've never been to this conference, or you're a bit newer to speaking, maybe go a bit less risky.

Anders says this of his experience:

> I'm fairly new to speaking. Have done a couple of local conferences in Sweden and some user groups. Got accepted at NDC Oslo last year which is the biggest one I've done so far. I tend to get reviews that range from OK to good, but not glowing.

Given this, I'd say to choose the first title. It's clearer and has a nice hook. Now we need to do the hard thing - write the abstract that will set the hook from the Tyson quote. Expect to write this 3-5 times and **show it to friends before you send it in**.

Some simple rules:

- Never assume people know what you're talking about (ie: don't use jargon)
- Tell people the problems you'll solve and/or the questions you'll answer
- Tell people who you are and why you're there

Let's keep this abstract straightforward. CQRS is a tough subject, but we're narrowing it down to the idea of Adventures and What Can Go Wrong - always a fun talk to go see:

> Command/Query Responsibility Separation (CQRS) is an interesting way to architect larger software systems. It's also a great tool for shrinking your ego as it comes with some unique pitfalls that aren't apparent from the start. I've been working with CQRS continually for the last four years building a system that handles millions of transactions per hour and CQRS has helped tremendously - but it's also brought me to tears. In this talk I'll show you how I failed - and then solved - some very unique problems that come from scaling a large system using CQRS.

Reading this over - I think it's a pretty good start... at least I can say I'm much more interested in this talk then I was before. I don't know Anders' particulars but I think he can take an outline like this and buff it out a bit - plugging in some details (like system size... people love that stuff) and maybe where he worked (is it a bank? Or maybe a large retail chain?)

Notice the arc of the paragraph as well: _Here's the subject that you may have heard about - it's not perfect. I've been using it for a long time and I like it, but it's also been difficult at times. I'll tell you why - and what I've learned_.

This is a classic storyline of the hero that travels to far-off lands, confronts dragons, and emerges victorious. Really - its that simple. Always remember you're the hero of your talk - maybe you're saving the empire, or you're meeting strangers in exotic lands.

The human brain has been conditioned over the last million or so years to learn at story time by the fire - so [get to know some basic story structures](http://en.wikipedia.org/wiki/Narrative_structure) and keep to that - in both your abstract **and** your talk. This will help people to follow you and your thoughts a little easier.

Good luck Anders!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/write-better-abstract.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/write-better-abstract.jpg" />
  </entry>
  <entry>
    <title>Your Own Private Heroku with DigitalOcean and Dokku</title>
    <link href="https://bigmachine.io/posts/your-own-private-heroku-with-digitalocean-and-dokku" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.725Z</updated>
    <id>https://bigmachine.io/posts/your-own-private-heroku-with-digitalocean-and-dokku</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/badass_unicorn.jpg" alt="Your Own Private Heroku with DigitalOcean and Dokku" /></p>
## Dokku, Like Heroku But Not

Dokku is one of those projects that I've been hearing about, but haven't had the time to look into. I did that today, finally, and I have to say it's... pretty impressive.

It's 100 lines of shell scripts that manage Docker for you - creating a Heroku-like experience where you can deploy your application using Git. There's a lot to it - but [have a read on Jeff Lindsay's blog](http://progrium.com/blog/2013/06/19/dokku-the-smallest-paas-implementation-youve-ever-seen/) (he's the creator of Dokku) - he has an interesting screencast that will give you more of an idea.

In summary form: you push your git repo to Dokku and it figures out what to do with it. Let's say it's a Node app - Dokku will create a Docker container for your app (Docker is sort of like a VM, but it uses your processor and memory) and install everything needed for it to run.

For Node, it does this by analyzing your package.json. For Ruby/Rails, your Gemfile. You can also use with Java if you like - but I have no idea how that works.

The whole process takes about 10 seconds, and when you're done... you have a working deployment.

## The Walkthrough Is Drop-dead Simple

In my last post about Gitlab I basically summarized DigitalOcean's online walkthrough. I did that because I wanted to underscore just **how bleedingly simple** this stuff has become.

For setting up Dokku - [just have a read of this post and follow what it says](https://www.digitalocean.com/community/articles/how-to-use-the-digitalocean-dokku-application).

I used a quick Node app and it _almost worked perfectly_ the first time, but I had to jigger a few things:

- Make sure you have a Procfile in the root of your app that tells Docker how to start your app. This can be as simple as `web: npm start` or a longer Rails incantation. Don't forget the `web:` key in there.

- Make sure you specify which node and npm version to use in your package.json file. These go in the "engines" setting.

Here's the package.json I used for a successful deployment:

```javascript
{
  "name": "application-name",
  "version": "0.0.1",
  "private": true,
  "scripts": {
    "start": "node ./bin/www"
  },
  "engines": {
    "node": "0.10.26",
    "npm": ">=1.3"
  },
  "dependencies": {
    "express": "~4.0.0",
    "static-favicon": "~1.0.0",
    "morgan": "~1.0.0",
    "cookie-parser": "~1.0.1",
    "body-parser": "~1.0.0",
    "debug": "~0.7.4",
    "jade": "~1.3.0"
  }
}
```

## Do You Need To Do This?

Yes, absolutely. If you're working with Node/Ruby/Python/Whatever Heroku Supports - you'll likely have client demos or just public goof-off code you'll want to share. As a web developer these days, it's really important to have a "playground" where you can publicly play with ideas too.

Heroku is amazing, but it's also pretty expensive when you ramp things up. Deployment can take forever as well! I'm a big fan of owning as much of your content as you can - your blog included (DigitalOcean does Wordpress too).

At the very least - see what Dokku can do, and be amazed at what's possible these days.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/badass_unicorn.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/badass_unicorn.jpg" />
  </entry>
  <entry>
    <title>A Better ID Generator For PostgreSQL</title>
    <link href="https://bigmachine.io/posts/a-better-id-generator-for-postgresql" rel="alternate" type="text/html"/>
    <updated>2025-05-19T22:09:32.725Z</updated>
    <id>https://bigmachine.io/posts/a-better-id-generator-for-postgresql</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/bullet-proof-glass..jpg" alt="A Better ID Generator For PostgreSQL" /></p>
## The GUID Problem

When developers think about a globally-unique identifier, [they usually think of UUIDs (or GUIDs)](http://blog.codinghorror.com/primary-keys-ids-versus-guids/) and will then create a table with a GUID as a primary key. This is problematic if your system grows or your writes/second increase.

The first reason is that GUIDs are large string blobs and take up more space than a typical integer (althoug both can be up to 16 bytes). The larger problem, however, is the default behavior of most databases is to set the primary key to _also be the clustering key_ - in other words the default key upon which the table is sorted.

The Primary Key and Clustering Key are two very different things. Primary Keys normalize your data and help you uniquely identify a row - a Clustering Key is the mechanism by which your server organizes and accesses data on disk.

Hopefully you can see the issue - sorting GUIDs (arbitrary strings) [can lead to poor data organization under the hood in terms of page and index fragmentation ](http://www.sqlskills.com/blogs/kimberly/guids-as-primary-keys-andor-the-clustering-key/) - precisely because the GUID data is so random.

SQL Server has some fixes for this with the `uniqueidentifier` data type and the `newsequentialid()` default value (which creates sortable GUIDs) - and that helps, but it still requires a lot more work then using a simple integer-based, auto-incrementing key. Which is precisely why so many developers like to use them.

## Enter Twitter Snowflake

Twitter [started out with MySQL as their storage medium and then moved to Cassandra](https://blog.twitter.com/2010/announcing-snowflake) to deal with the insane scaling issues they were facing. Cassandra doesn't do auto-incrementing keys and doesn't do UUIDs either - so Twitter was left to create its own system:

> Unlike MySQL, Cassandra has no built-in way of generating unique ids – nor should it, since at the scale where Cassandra becomes interesting, it would be difficult to provide a one-size-fits-all solution for ids. Same goes for sharded MySQL.

> Our requirements for this system were pretty simple, yet demanding:

> We needed something that could generate tens of thousands of ids per second in a highly available manner. This naturally led us to choose an uncoordinated approach.

> These ids need to be roughly sortable, meaning that if tweets A and B are posted around the same time, they should have ids in close proximity to one another since this is how we and most Twitter clients sort tweets.

> Additionally, these numbers have to fit into 64 bits. We’ve been through the painful process of growing the number of bits used to store tweet ids before. It’s unsurprisingly hard to do when you have over 100,000 different codebases involved.

Twitter's solution became [Twitter Snowflake](https://github.com/twitter/snowflake) a "network service for generating unique ID numbers at high scale with some simple guarantees". Its worked very well for them and similar solutions. In fact Eric Lindvall of Papertrail said [exactly this in PeepCode's great "Scaling Up" video](http://pluralsight.com/training/courses/TableOfContents/scaling-up-lindvall) - wherein he talks about simple ways to avoid database problems when scaling:

> Move ID generation out of the database to an ID generation service outside of the database... As soon as a piece of work enters their system, an ID gets assigned to it... and that ID generated in a way that is known to be globally unique within their system... and they can then take that message and [drop it in a queue]

This is the first database issue that Eric discusses - it's one of the primary scaling concerns! **Creating a sortable, globally-unique ID for all bits of data in your system** which allows you to shard/cluster your database without worrying about colliding IDs.

This is an understandable hurdle for a key/value system like Cassandra which can't generate it's own unique keys - but can't we do this with MySQL or Postgres?

## A Functional Snowflake Equivalent for PostgreSQL

There are Snowflake-style systems out there for generating unique ids, but the problem is that these systems become a bottleneck! They better be fast - and if they go down your entire system grinds to a halt.

[This was Instagram's concern](http://instagram-engineering.tumblr.com/post/10853187575/sharding-ids-at-instagram):

> With more than 25 photos & 90 likes every second, we store a lot of data here at Instagram. To make sure all of our important data fits into memory and is available quickly for our users, we’ve begun to shard our data—in other words, place the data in many smaller buckets, each holding a part of the data.

> Our application servers run Django with PostgreSQL as our back-end database. Our first question after deciding to shard out our data was whether PostgreSQL should remain our primary data-store, or whether we should switch to something else. We evaluated a few different NoSQL solutions, but ultimately decided that the solution that best suited our needs would be to shard our data across a set of PostgreSQL servers.

> Before writing data into this set of servers, however, we had to solve the issue of how to assign unique identifiers to each piece of data in the database (for example, each photo posted in our system). The typical solution that works for a single database—just using a database’s natural auto-incrementing primary key feature—no longer works when data is being inserted into many databases at the same time. The rest of this blog post addresses how we tackled this issue.

The author, Mark Krieger goes on to discuss these options: using UUIDs/GUIDs, a Snowflake-style service, or writing a routine specifically for Postgres.

Instagram ultimately decides that they don't want to rely on app code to create the id, nor do they want to introduce complexity with a Snowflake-style system. Instead, they cracked open Postgres and created their own Function:

```sql
CREATE OR REPLACE FUNCTION insta5.next_id(OUT result bigint) AS $$
DECLARE
    our_epoch bigint := 1314220021721;
    seq_id bigint;
    now_millis bigint;
    shard_id int := 5;
BEGIN
    -- there is a typo here in the online example, which is corrected here
    SELECT nextval('insta5.table_id_seq') % 1024 INTO seq_id;

    SELECT FLOOR(EXTRACT(EPOCH FROM clock_timestamp()) * 1000) INTO now_millis;
    result := (now_millis - our_epoch) << 23;
    result := result | (shard_id << 10);
    result := result | (seq_id);
END;
$$ LANGUAGE PLPGSQL;
```

A really neat idea! Sharding Postgres logically using schemas is a very interesting way to speed up reads and writes - but it obviously messes up id generation. This solution, however, seems pretty elegant!

I gave this function a spin and slightly tweaked it for a project I'm working on - here's a full script you can run right now:

```sql
create schema shard_1;
create sequence shard_1.global_id_sequence;

CREATE OR REPLACE FUNCTION shard_1.id_generator(OUT result bigint) AS $$
DECLARE
    our_epoch bigint := 1314220021721;
    seq_id bigint;
    now_millis bigint;
    -- the id of this DB shard, must be set for each
    -- schema shard you have - you could pass this as a parameter too
    shard_id int := 1;
BEGIN
    SELECT nextval('shard_1.global_id_sequence') % 1024 INTO seq_id;

    SELECT FLOOR(EXTRACT(EPOCH FROM clock_timestamp()) * 1000) INTO now_millis;
    result := (now_millis - our_epoch) << 23;
    result := result | (shard_id << 10);
    result := result | (seq_id);
END;
$$ LANGUAGE PLPGSQL;

select shard_1.id_generator();
```

Running that you should see a nice, clean `bigint` that you can use for a key with any table. Speaking of - here's how you can declare a Users table to use this function to automatically generate your key:

```sql
create table shard_1.users(
  id bigint not null default id_generator(),
  email varchar(255) not null unique,
  first varchar(50),
  last varchar(50)
)
```

## When Do You Face This Problem?

That's something that's up to you and your company. Over-engineering from the get-go is a problem in our industry, but at the same time you can at least plan for a year out. With a system like Twitter, a year's growth could easily cause write problems for a MySQL database - same with a logging system like Papertrail.

If you run ExpiredFoods.com, however, you might never need to deal with a scaling issue like this.

Either way, it's nice to know the options are out there.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/bullet-proof-glass..jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/bullet-proof-glass..jpg" />
  </entry>
  <entry>
    <title>The Subtle Arts of Logging and Testing</title>
    <link href="https://bigmachine.io/posts/the-subtle-art-of-logging-testing" rel="alternate" type="text/html"/>
    <updated>2023-12-30T03:05:25.000Z</updated>
    <id>https://bigmachine.io/posts/the-subtle-art-of-logging-testing</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2023/12/screenshot_411.png" alt="The Subtle Arts of Logging and Testing" /></p>
I'm writing the Testing Strategies chapter of **a book I've been working** on for the last 2 years: _The Imposter's Playbook_, and I dove into **a bug I had in production** that plagued me for longer than it should have. 

I'll describe the bug in a second, but let me say that it took me almost 2 hours to figure it out, which is horrible. The main reason it took so long is that I kept writing more tests to isolate the issue, thinking it was a logic one (it wasn't). I'm curious if I suck at testing or this is just "one of those things".

## Take My Damned Money

I found out about this problem in the worst way possible: **a customer told me**. They sent me an email that said:

> I'm trying to buy your book right now but your site keeps crashing.

At the time, about 8 years ago, my site was a simple Node/Express application using PostgreSQL. A simple commerce setup that I had used before and I had a load of tests for it too... especially when it came to the "commerce flow" - find a product, add it to the cart, checkout.

I had a look at the logs and instantly felt a chill run down my spine (summarizing here):

```json
{"message": "Item added to cart", "sku":"imposter", "level": "info"},
{"message": "Item added to cart", "sku":"imposter", "level": "info"},
{"message": "Item added to cart", "sku":"imposter", "level": "info"},
{"message": "Product doesn't exist!", "sku":"imposter", "level": "error"},
```

There was obviously a lot more in the logs (requests, etc), but this is the important stuff. My logs were telling me that a product that used to exist didn't exist any more.

**Oh crap**.

## Dropping Prod?

My first thought was that I dropped the production tables. I had been working on the app all week, tweaking things locally without deploying. I don't mind hitting the database during testing because... well I have my reasons... and I make sure to drop/rebuild the test database with every test run. It takes milliseconds to do so, and my "fixtures", if you will, are all in a single SQL statement that is run using Make.

It's how I work, OK? It's fast enough for me and I like the speed of using `psql` before a test run.

Normally I hard code the connection string for testing to be ultra super whammadyne double-secret-probation sure that the only database that gets the drop/reload treatment is my test one. But if, somehow, through some combination of dumb-assery the production connection string got substituted... _oh no_.

It took 10 minutes to verify, three times over, that this was not the case. My production system was fine, and when I peeped at the production database, there was my `products` table with my little book, _The Imposter's Handbook._

So it existed. But my app thinks it doesn't? WTF?

## Thanks, Logs

**I was sloppy with my logging**. Over the years I've created my own personal strategy which feels verbose, at first, but has helped many times:

* Log any application error (in addition to runtime), and keep them focused.
* Log any state change to any model (as `info`).
* Log any `status` change, if a model has a `status` field, as a `warning`.
* Don't `try/catch` at the controller level unless no choice, only in service classes.
* Know what you're `catch`ing!

Logging errors is kind of an art form, and it can be difficult to interrogate every possible error thrown, but your future self will thank you if you can be as complete as possible. When I worked at Microsoft back in 2008 I was given a pretty hard time because I kept catching `InvalidOperationException`s everywhere, and my boss at the time said something snarky:

> So your code only throws InvalidOperationExceptions?

So I went back and, in my service classes, tested every assumption I made. My approach to this is to let models throw their own validation exceptions - that's it. Let a model be a model, if you will.

Service classes handle "business stuff" and is typically where my `try/catch` stuff goes. The problem is: _where do you put these blocks?_

My answer to that question is to challenge every assumption I make in these classes. Like "if the result of my query is null, a product doesn't exist". If I would have challenged that assumption I _might_ have caught this bug before it bit me.

Oh well.

I learned the `status` trick from an old lead I had long ago. The idea is that when something's `status` has changed, it's quite possible that it will have a ripple effect on your application... thus the warning. For instance: a user's status going from "subscribed" to "unsubscribed" changes the application 

This is particularly true with my problem!

So, no, I didn't use my typical logging plan for my own application because I was being lazy and didn't have the threat of a client/boss getting cranky with me. I would have found out the problem much quicker.

Do you have a personal logging strategy? If so, share!

## The Problem

I pulled as much information as I could from my logs, which wasn't much, to help me try and debug this situation. I wrote as many additional edge-case tests that I could think of, but it still didn't help. _I couldn't find this damned bug_.

I finally grabbed a dump from my production system and tested against _that_ (without dropping things, of course), and found the problem quickly. This is embarrassing.

**I created my database to handle physical products** as well as digital. I had it in my head that I might also sell physical copies of my books, and that they would be in my garage in boxes that I could send to people.

That meant I needed to handle inventory, which means inventory logic, which went like this:

* If inventory > 0, add to cart is OK, otherwise show error.
* If inventory >= 0, set `status` to "in-stock".
* If inventory <= 0, set `status` to "backorder".
* If product `status` is "backorder", don't show it.

To be honest, I didn't spend too much time thinking this through because **all my products, up to that point, were digital**. The books-in-garage thing was kind of a Big Idea that may or may not happen, so I kind of ignored the tests I needed for this process because, you know, YAGNI.

The exact problem came when the reporting customer had added the book to their cart, went to checkout, and then got distracted for a few hours. When they went to checkout, the inventory had gone to 0 because someone else checked out (debiting the inventory) and because I didn't test this situation, my product query returned `null` (because of my assumption above) when loading the checkout page and threw an error.

Laziness. Gets you every time! 

## A Question: Would TDD Have Caught This?

I debated this a few years ago with a friend. We were at NDC London discussing TDD and how it was actually fun if you did it in pairs, and I told them the particulars of this problem.

His answer was "you have to be disciplined and find the edge cases", to which I responded "this case shouldn't have existed... I don't think."

TDD is only as good as your ability to break things, and that last statement I made there blinded me to something that is all too real a possibility: admins (me) might screw things up.

The _real_ problem here is that **I mistakenly set _The Imposter's Handbook_, which is digital, to be a physical product with inventory**. And because I wasn't paying enough attention, I had a default value of 100 set for `stock_level`. This should never have happened... _but it did_.

I suppose TDD _should_ have caught me on this but, in my mind, that functionality wasn't ready to go anyway so why bother testing it? 🤦🏼‍♂️.

## Would Love Your Feedback

The book I'm writing_, The Imposter's Playbook,_ is a variation on one of my most favorite books, _Coder to Developer_ by Mike Gunderloy. That book was basically "here are the skills you need to cultivate if you want to become a pro", and I loved it no end.

I wanted to do a modern version of that, but taking it up a notch and framing it for self-taught people wanting to move into a senior position. It covers things like:

* Principles of interface design
* Commonly used project management things (Agile, Scrum, Lean, etc.)
* Using GitHub like a Pro (GitFlow, Trunk-based, etc.)
* Configuring GitHub for a team
* Intro to Docker and Docker Compose
* Basic "DevOps"
* Kubernetes
* Common Architectural Patterns (Monolith, MonoRepo, Microservices, Evented, SOA, etc.)
* Testing Strategies
* The Art of Debugging
* Logging and Monitoring
* Disaster Planning
* Benchmarking and Scaling
* Making sure you're recognized

That's the working TOC, not a complete list, and I'm about 66% through it, though there is a lot of work needed - and there's also asking y'all if you have any thoughts!

If you'd like to see a topic covered, hit reply and let me know. I'm having a blast getting into the details on each of these topics - especially Kubernetes. I've wanted to learn that to a deeper level and this last holiday I did!

You can also let me know if TDD would have prevented the problem above, which resulted in a customer not being able to buy something.

## Just Added an Old, New Short Video

When writing this chapter, I figured it would be a good idea to see what my friend [Brad Wilson](https://mastodon.social/@bradwilson?ref=bigmachine.io) might have done. I recorded an hour-long TDD video with him, where I challenged him to implement a subscription billing system, keeping things as real as he possibly could.

I also threw him a few curveballs. You can watch it here:


Thanks for reading!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2023/12/screenshot_411.png" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2023/12/screenshot_411.png" />
  </entry>
  <entry>
    <title>Test-driven Development In Action</title>
    <link href="https://bigmachine.io/posts/test-driven-development-in-action" rel="alternate" type="text/html"/>
    <updated>2023-12-29T10:32:44.000Z</updated>
    <id>https://bigmachine.io/posts/test-driven-development-in-action</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2023/12/screenshot_408.jpg" alt="Test-driven Development In Action" /></p>
A few years back I recorded a coding session with [Brad Wilson](https://mastodon.social/@bradwilson) where I asked him to do TDD "for real", as if he was at work and not doing a demo... even though he was.

The result is the video below, which was pretty popular on one of my other video sites. It is a bit dated but I don't think the methods changed at all! Hope you enjoy.

<div style="padding:56.25% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/898418061?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" style="position:absolute;top:0;left:0;width:100%;height:100%;" title="TDD with Brad Wilson"></iframe></div><script src="https://player.vimeo.com/api/player.js"></script>]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2023/12/screenshot_408.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2023/12/screenshot_408.jpg" />
  </entry>
  <entry>
    <title>Meet Playwright</title>
    <link href="https://bigmachine.io/posts/meet-playwright" rel="alternate" type="text/html"/>
    <updated>2023-12-15T01:59:02.000Z</updated>
    <id>https://bigmachine.io/posts/meet-playwright</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2023/12/screenshot_128.jpg" alt="Meet Playwright" /></p>
A few months ago I released [_The Imposter's Frontend Accelerator_](https://sales.bigmachine.io/accelerator) with the goal of helping people feel what it was like to **build a real world application** using **Vue 3 and Nuxt 3**. That's hard to do, especially when I have to make a few choices along the way that you might not make.

To that end, I created two "case studies", one of which was "what if we did this with Firebase" and the other is this one, which is "what if we would have written tests with Playwright".

I like Playwright a lot, but it's kind of confusing. The DOM selectors have a weird syntax (that turns out to be very important) and the runners are pretty complex. If you spend an hour with it, however, you'll fall in love.

At least I did. Here's the video, and here's a [link to the code](//course/accelerator/#/playwright) and the rest of the course.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2023/12/screenshot_128.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2023/12/screenshot_128.jpg" />
  </entry>
  <entry>
    <title>\U0001F916 Does Functional Programming Matter To You?</title>
    <link href="https://bigmachine.io/posts/does-functional-programming-still-matter" rel="alternate" type="text/html"/>
    <updated>2023-12-06T04:47:59.000Z</updated>
    <id>https://bigmachine.io/posts/does-functional-programming-still-matter</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2023/12/screenshot_251.png" alt="\U0001F916 Does Functional Programming Matter To You?" /></p>
It seemed like functional programming got a boost back in the mid to late 2010s when [Elixir](https://elixir-lang.org) started gaining in popularity. I, for one, had my entire professional outlook turned inside out by getting to know this language and the underlying BEAM runtime and OTP framework.

I couldn't understand why we hadn't always worked this way. I didn't understand why OTP and frameworks like it weren't the norm! I began to understand, however, why functional programming people tend to be ... passionate functional programming people.

Now you might be wondering if the title is clickbait and I don't think it is because I am genuinely curious about your answer. If you're receiving this via email, I would love a reply! I found functional concepts to be life-changing, literally, changing the way I think about code, tests, and putting applications together.

What do I mean? Here are a few things...

## Purity

You might know this already but "pure code" is completely self-contained and doesn't rely on anything outside of its scope. A simple example would be a math function (using JavaScript here):

```js
const squareIt = function(num){
  return num * num;
}
```

I know there are more elegant ways to do this and guards to put on here but you get the idea: this is a _pure_ function.

Let's change the above function to be _impure_:

```js
const RobsConstant = 2.58473;

const squareItRobsWay = function(num){
  return num * num * RobsConstant;
}
```

My function will now _behave differently_ if the value of `RobsConstant` changes, which it shouldn't because it's a constant and all, but it's possible that I could redefine this value and pull it from a database, who knows! My function sure doesn't, and it's possible that we could introduce an error at some point (turning `RobsConstant` into a string, for instance) which is really, really annoying.

If we were being good functional people, we would use two functions and shove them together:

```js
const RobsConstant = function(){
  return 2.58473;  
};

const squareIt = function(num){
  return num * num;
}
const squareItRobsWay = RobsConstant() * squareIt(4); 
```

This seemingly small change is profound! We can test both functions to make sure they do what they're supposed to, which means we can have full confidence that our `squareItRobsWay` value should _always_ return what we expect (again: assuming we have tests in place).

## Currying. Crazy Talk.

You may have heard this term when talking to a functional person and thought it sounded a bit _mathy_. I know I did. Currying is splitting a function with multiple arguments into a chain of smaller functions with only a single argument. 

Dig this:

```js
const buildSelect = function(table, criteria, order,limit){
  let whereClause="", params=[];
  if(criteria){
    whereClause = "where 1=$1" //pseudo code, obvs
    params=[1] //placeholder for this example
  }
  const orderClause = order ? `order by ${order}` : ""
  const limitClause = limit ? `limit ${limit}` : ""

  const sql = `select * from ${table} ${whereClause} ${orderClause} ${limitClause}`;
  return {sql: sql, params: params};
}
```

I'm punting on writing out the `where` stuff because it's not important. What _is_ important is the idea that we have code here that we can use elsewhere. If we put our functional hats on, focus on _purity_, we can actually _curry_ this into a set of smaller functions that only do one thing:

```js
const where = function(item){
  //build a where clause by inspecting the item
  return item ? `where 1=$1` : "";
}
const params = function(item){
  //create parameters from the criteria item
  return item ? [1] : "";
}
const orderBy = function(clause){
  return clause? `order by ${clause}` : ""
}
const limitTo = function(clause){
  return clause ? `limit ${clause}` : "";
}

const selectQuery = table => criteria => order => limit => {
  //create a where statement if we have criteria
  const sql = `select * from ${table} ${where(criteria)} ${orderBy(order)} ${limitTo(limit)}`;
  return {sql: sql, params: params(criteria)};
};
```

Believe it or not, this works! We can invoke it like this:

```js
const sql = selectQuery("products")({sku: "one"})("cost desc")(100);
console.log(sql);
```

```sh
❯ node query.js
{
  sql: 'select * from products where 1=$1 order by cost desc limit 100',
  params: [ 1 ]
}
```

In functional languages you typically chain methods together, passing the result of one function right into another. In Elixir, we could build this exact function set and start with the table name, passing along what we need until we have the select statement we want:

```elixir
"products"
  |> where({sku: "one"})
  |> orderBy("cost desc")
  |> limitTo(100)
  |> select
```

This, right here, is a _functional transformation._ You have a bunch of small functions that you pass a bit of data through, transforming it as you need.

## Partial Application

The first draft of this post went out as an email so if you're here from that email you didn't see this section! Sorry - it happens.

You might be looking at that invocation wanting to barf, but that's not how you would use this code. Typically, you would build up a _partial_ use of the functions, like this:

```js
//assuming there's a sales_count in there
const topProducts = selectQuery("products")()("sales_count desc");
```

We're _partially applying_ our functions to create a new one that shows us the top _n_ products, which we can specify in this way:

```js
const top10Products = topProducts(10);
console.log(top10Products);
```

This is where things get really, really interesting. Functional composition is at the heart of functional programming, and damned fun to use, too! Here's what our code will generate for us:

```sh
{
  sql: 'select * from products  order by sales_count desc limit 10',
  params: ''
}
```

Small, simple functions, easily testable, composable, and the clarity is wonderful.

So: what do you think? Is this style of programming interesting, simpler, elegant or horrid? Or is everything just React these days :D.

There's more, of course, and I made a fun video many years ago for [_The Imposter's Handbook_](https://sales.bigmachine.io/imposter-second)which you can watch right here, if you like!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2023/12/screenshot_251.png" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2023/12/screenshot_251.png" />
  </entry>
  <entry>
    <title>\U0001F47B Hacking Ghost for Fun and Profit</title>
    <link href="https://bigmachine.io/posts/hacking-ghost" rel="alternate" type="text/html"/>
    <updated>2023-11-14T10:13:36.000Z</updated>
    <id>https://bigmachine.io/posts/hacking-ghost</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https:/images.unsplash.com/photo-1633826523932-fb137c3353b5" alt="\U0001F47B Hacking Ghost for Fun and Profit" /></p>
Back in 2014 I created a course for Pluralsight called [_Hacking Ghost_](https://www.pluralsight.com/courses/hacking-ghost), which is the CMS platform you're reading this post on (or maybe got an email from). I had a good time putting that course together, but **Ghost has grown up a lot** since then.

I've been using it on my personal site (this one) for the last year or so and I really, really like it. Recently, however, I decided to see just how far I could push this platform to do a few things it was never meant to do.

Allow me to share, because some of y'all might benefit!

## Hosting Video Courses

One of the things I have wanted for a long time is a site that's all about content production while at the same time capable of **hosting video courses**. You can do it in WordPress, of course, and there are a few services out that there come close... but nothing like Ghost.

It's wasn't all that hard to do and, moreover, it was kind of fun. One thing that's extremely simple to do is to [create your own theme](https://ghost.org/docs/themes/). Ghost uses Handlebars under the hood (a Node/JS templating engine) and exposes a ton of helpers to you that allow you to create what you need. They also give you access to routing using a simple YAML file.

I have quite a few templates laying around, so I took one of them and made a blog theme that I like based on Bootstrap 5\. I also did something that I've wanted to do for years: **I wedged in a Vue application so people can watch the courses I've made**.

Here's one that I just launched: [_The Imposter's Frontend Accelerator_](https://sales.bigmachine.io/accelerator/). I think it works pretty well so far, though there might be a few bugs here and there.

Point is: Ghost is flexible enough that I could write up a Vue app, drop it into my theme, and show some courses! But there's a little more here too...

## Hooking Up Supabase

Ghost doesn't give you access to its database, unlike WordPress (thank god). That means that if you need to access some data, like whether someone has bought one of your courses, you need to do something different.

For this, I used [Supabase](https://supabase.com/). It's basically a "backend in a box" that runs on Postgres and for me, _say no more,_ I'm all over it. All of my business data is in there going back years and if you bought something from me, you're in there!

![](https://bigmachine.io/img/2023/11/screenshot_205.jpg)

One service that Supabase offers is user authentication. They do this using magic links, email/password, and social. This presents a problem with Ghost because Ghost provides authentication too - so how do you synchronize the two?

_By the way: Supabase has a very generous free tier but I pay them money anyway because I love the service. **I get no consideration for this post**. Same with Ghost._

Here's the fun part - and I think it works pretty well. Every user of this Ghost site has an account with a GUID as an ID. When you're logged in, Ghost gives me access to your information from my theme:


```javascript
{% raw %}
const email="{{@member.email}}";
const password="{{@member.uuid}}";
{% endraw %}
```

Now this might make you want to puke, especially seeing the `password` reference there - but just think of it as an access token. I have a routine that fires when you visit the site that tries to log you in to Supabase (if you're logged in to Ghost). If that fails, I send your credentials to an edge function (another Supabase service) which adds you on the fly.

This is some simple JavaScript I have in my theme. It's a simple wrapper for Supabase that does the "heavy" lifting:

```js
class DB {
  constructor(){
    const {createClient} = supabase;
    this.client = createClient("[credentials]", {
      persistSession: true,
      autoRefreshToken: true,
    });  
  }
  async login(email, uid){
    return this.client.auth.signInWithPassword({
      email: email,
      password: uid
    });
  }
  async getUser(){
    return this.client.auth.getUser()
  }
  async ensureLogin(email, uid){
    //if they're logged in, return
    const exists = await this.getUser();
    if(exists.data.user) return true;

    //if not, let's try and log them in
    console.log("Logging in...");
    const {data, error} = await this.login(email, uid);

    if(error) {
      //if we're here then the user is logged in and we need to sync things
      console.log("Syncing...");
      const res = await fetch("[supabase function url]", {
        method: "post",
        body: JSON.stringify({email: email, uid: uid})
      });
      await this.login(email, uid);
      return true;
    }
  }
}
```

In my Ghost theme I check to see if a `member` is logged in right in my layout at the top of the page. If they are logged in, I sync things up with Supabase (yes that's jQuery don't judge me):

```hbs
{% raw %}
{{#if @member}}
<script>
	$(async () => {
		const db = new DB();
		await db.ensureLogin("{{@member.email}}", "{{@member.uuid}}");
	});
</script>
{{else}}
<script>
	localStorage.removeItem("[token key]");
</script>
{{/if}}
{% endraw %}
```

The result of `db.ensureLogin` will be a JWT that's kept in `localStorage` for the Supabase SDK to use when I make API calls.

The Supabase function that creates an auth record if a user isn't there is a Deno endpoint that has one job only: _adding a user to the authentication backend:_

```js
const client = createClient(
  Deno.env.get('SUPABASE_URL') ?? '',
  Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') ?? '',
)
const {email, uid} = await req.json();

const { data, error } = await client.auth.admin.createUser({
  email: email,
  password: uid, 
  email_confirm: true
});
```

I've thought a lot about this and also asked a few friends if this seemed "secure". Sure it's possible to log yourself in if you know one of my user's email address and `uid` (which is a GUID). Doing that will allow you to watch some videos, if they bought any - but that's it.

To me, it's akin to calling an API endpoint with a user's unique id and asking for some data. Maybe I'm wrong on that, but there's no sensitive user information that you would have access to. Supabase's client access is locked down, so authentication here simply means you can see videos. That's it.

**Do let me know if there's something I'm missing**. These credentials aren't stored on the client, by the way, that's all done with JWTs coming from Supabase. All of the data in the Supabase database is protected by PostgreSQL's row-level security, which is based on your JWT, so I think we're good here but then again... I'm not a security expert.

Is it a hack? Sure! Does it work? Yes!👨🏻‍🎤

## Building Your Own Theme

Ghost has extensive documentation on building a custom theme, which you can [read here](https://ghost.org/docs/themes/). In summary: **a theme is a bunch of Handlebars pages with data available to them**. Things like `post`, `page`, `member` and so on.

For my site, I decided to buy a theme from [Bootstrap](https://themes.getbootstrap.com/) called [Eduport](https://themes.getbootstrap.com/product/eduport-lms-education-and-course-theme/). It has every single page you could need, and splitting it out into a Ghost theme took me about 3 days over a long weekend.

One really nice thing about Ghost themes is that membership popup screens are part of Ghost itself - you don't need to style that stuff. You can, if you want to, but logging in, subscribing, and profile pages are already there.

That's the easy part - the video app is a whole different deal!

## The Vue App

This is where things got tricky. I needed a literal single-page app with routing and data access to Supabase. I tried to just drop Vue into a template page, but the routing and other things quickly made a mess.

![](https://blog.bigmachine.io/img/2023/11/screenshot_203.jpg)

To get around this, I created a standalone Vue app with the CSS for the template as part of it. I kept the directory for the Vue app _outside_ of the theme directory - in fact I put it in the root of my local Ghost instance.

For convenience, I reset the build output to be my theme's assets directory. This is my Vue app's `vite.config.js`:

```js
export default defineConfig({
  build: {
    outDir: '../content/themes/bootstrap/assets/app',
    rollupOptions: {
      output: {
        entryFileNames: `assets/[name].js`,
        chunkFileNames: `assets/[name].js`,
        assetFileNames: `assets/[name].[ext]`
      }
    }
  },
  //...
```

Also notice that I renamed the output files. Normally these have "cache-busting" hashes appended to them, but that became a pain in the butt so I'm going with the same name for each asset, every time which seems to work pretty well.

### Routing

I decided to go with hash-based routing because I didn't want permalinks to freak out Ghost, which would try and serve any request coming in. That's a simple thing to do with Vue's router:

```js
const router = createRouter({
  history: createWebHashHistory(),
  routes: [
    {
      path: '/',
      name: 'home',
      component: HomeView
    },
    {
      path: '/:slug',
      name: 'lesson',
      component: LessonView
    }
  ]
});
```

The app itself is pretty simple. There's a main course page that shows summary information and a list of lessons, and then there's actual lesson page with the videos on it. 

### The Build

Admittedly, there are a lot of moving parts with this and I really don't like that. I know that in a year's time I'll forget everything I've done and I'm pretty good at leaving myself notes and comments in the code, but I also know myself really well... this is going to make me cranky.

Case in point: my local build process. As I mention, I have my CSS, images and Bootstrap JS stuff in my Vue project because I need to see how things look separate from my Vue theme. These are all stored in the Vue app's `public` directory as I don't want them built with the Vue app because it would double up the CSS and JS files I need.

I also need to be sure that I replace all of the built files in my theme with the new ones coming in. Here, let me just show you the code from my `package.json`:

```js
  "scripts": {
    "dev": "vite",
    "build": "rm -R ../content/themes/bootstrap/assets/app && vite build && rm -R ../content/themes/bootstrap/assets/app/assets/images",
    "preview": "vite preview"
  },
//...
```

The `build` task does the work here. It's removing my themes `assets/app` directory, which is where my app lives, building the local project which pushes the code to the theme, and then it's deleting the images that get pulled over.

It feels a bit janky, but it's working and it's just a few bash commands so... I guess it's OK.

## Publishing Your Theme

It's pretty simple to upload your finished theme, including your Vue app, using the Admin UI. It works, but it involves a lot of clicks and I like scripts so I went hunting for one a few years ago.

It turns out you can post a zip file to the Ghost API and it will do the needful, popping the theme files you need in place. I can't remember where I found this script - I think it might have been on the Ghost forums. Normally I pay close attention to crediting people as there no way I could have figured this out myself!

Anyway, [here's a script](https://gist.github.com/robconery/78ed337a4a049057aafa560de7b0af1c) that will push your theme when run:

You just need to add your admin key, theme location and blog URL. This script has saved me so, so much time!

Well that's it! If you use Ghost, I do hope you've picked up some helpful tips here. If you have a comment or question, they're open so ask away!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https:/images.unsplash.com/photo-1633826523932-fb137c3353b5" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https:/images.unsplash.com/photo-1633826523932-fb137c3353b5" />
  </entry>
  <entry>
    <title>Surviving the Structured Interview</title>
    <link href="https://bigmachine.io/posts/surviving-the-structured-interview" rel="alternate" type="text/html"/>
    <updated>2023-10-24T02:43:17.000Z</updated>
    <id>https://bigmachine.io/posts/surviving-the-structured-interview</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https:/images.unsplash.com/photo-1626447857058-2ba6a8868cb5" alt="Surviving the Structured Interview" /></p>
I went to a meetup a few nights ago and met a young woman whom I'll refer to as Sara. She had just applied to Microsoft, AWS and a few other companies and wanted to know if I had any advice.

I took a deep breath and she thought I was frustrated.

> I'm so sorry I don't mean to be all network-y! This is a social event ... I just really want to transition my career into tech...

I explained I was just trying to summarize **the last few _decades_ of interviewing** at these big companies - I could talk for hours about it!

"OK", she said, "maybe just tell me the biggest thing I need to know?"

## Get To Know the Structured Interview 

"The what?" she asked.

If you don't know, and it seems that many people don't, _structured interviews_ are becoming more and more popular. 

Here is how Indeed.com defines them (emphasis mine):

> A structured interview is a conversation in which an interviewer asks an interviewee **set questions in a standardized order**. The interviewer collects the responses of the candidate and **grades them against a scoring system**. Asking the same questions in the same order helps interviewers collect similar types of information delivered in a uniform context from interviewees.

There are many benefits to interviewing in this way, but the main ones (to me) are:

* A more **consistent** evaluation process.
* Removal of a good amount of **bias** that always creeps in.
* Interviewers don't get themselves into **trouble** asking questionable questions.

Some examples of a structured interview question from the tech industry might be something like:

> Tell me about a time when you overcame a rather difficult challenge pertaining to a past project

This question is straight from the AWS behavioral interview, and everyone applying there will get at least one of these.

**_Note_ _: I work at Microsoft and it might seem weird that I'm showing you an Amazon interview question, but they're very public with their interviewing process. Also, these questions, as you can see, are pretty generic._**

The idea is that **you tell a story**, but in a very structured way, touching on one or more of their [Leadership Principles](https://www.amazon.jobs/content/en/our-workplace/leadership-principles). These aren't gimmicky corp-speak things, either, _they're very serious about these principles!_ Friends that work at Amazon tell me that they hear this principles invoked on almost a daily basis.

The interview is _insanely_ formulaic, as are most structured interviews, but the neat thing is that it doesn't matter! **What matters is your story and how you tell it**.

That's Amazon, but as I mention, you're likely going to get at least one of these types of questions, even if you're going for an engineering position. They're used with Culture Fit questions mostly, but can also be used for a purely technical session too. 

## Structuring Your Response

Amazon has a thing they call the "STAR" method, which is:

* **Situation**. Briefly discuss the context of your story.
* **Task**. What did you have to do?
* **Action**. What did you actually do?
* **Result**. What happened when you did it?

You **shouldn't spend more than 5 minutes** going into your story and if your interviewer wants more detail, they'll ask you. The important thing, however, is that **your answer should be framed properly**, so the company you're interviewing with scores you the highest.

## A Better, More Generic Structure

Amazon's STAR method is fine, but I think you can add better detail in there that fits just about any company you're applying for. I don't have a catchy acronym though:

* **Situation**. Briefly discuss the context of your story.
* **Problem**. What, exactly, was the problem?
* **Solution**. How did you solve the problem?
* **Impact**. How did your solution impact the project/company?
* **Lesson**. What did you learn?

It's hard to tell a good story, especially when you're in an interview. Thankfully these things are online these days, which means you can...

## Write These Stories Out In Advance

Might sound like cheating, but most companies will tell you exactly what they're going to be asking you about if you're facing a structured interview (and you likely will).

Either way, it's a **really good idea to get your stories straight** before you start the process. Start with the question above (the one about facing a challenge), as it's so, so common.

Here are a few more to consider:

* Tell me about a time that you **challenged your team** and management, putting the customer first.
* Tell me about a time that you **took it upon yourself to create a win** for your project.
* Tell me about the **last product you shipped**.
* Tell me about a time when you were **completely stuck** with a technical challenge.
* Tell me about a time where you **blew your boss away**.
* Tell me about something you **innovated**.
* Tell me about a time you were **wrong**.
* Tell me about a time when you **rescued** your work through negotiation.

The more you research these questions, the more you'll find variations on a theme from company to company. How you handle risk (and take chances), winning, losing, etc.

Write them out and then outline them - but be careful! **If you pin them to your wall your interviewer will know** when you're reading and, trust me, I've heard some hysterical stories about this.

## This Week's Video: The Job Hunt!

Sara inspired this week's video: _10 Tips and Tricks For Tech Interviews_. If you've been interviewing for a while, you'll likely know a lot of these ... but maybe not!

I go into the structured interview, culture-fit questions, mindset, pair coding, take home questions, and more.

**Enjoy, and good luck** with your next interview.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https:/images.unsplash.com/photo-1626447857058-2ba6a8868cb5" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https:/images.unsplash.com/photo-1626447857058-2ba6a8868cb5" />
  </entry>
  <entry>
    <title>Explain It Like I&apos;m 5 - Why Are Hashes Irreversible?</title>
    <link href="https://bigmachine.io/posts/why-are-hashes-irreversible" rel="alternate" type="text/html"/>
    <updated>2023-10-16T02:39:57.000Z</updated>
    <id>https://bigmachine.io/posts/why-are-hashes-irreversible</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2023/10/hash-five.jpg" alt="Explain It Like I&apos;m 5 - Why Are Hashes Irreversible?" /></p>
I was reading Twitter/X the other day when I came across a compelling question from [Kevin Naughton](https://www.youtube.com/watch?v=9gIi6UK6w4Q):

> someone pls explain how hashing algorithms like SHA-256 are irreversible like i'm 5 years old
> 
> — Kevin Naughton Jr. (@KevinNaughtonJr) [September 24, 2023](https://twitter.com/KevinNaughtonJr/status/1706005238899945814?ref%5Fsrc=twsrc%5Etfw)

This is a wonderful question and, to be honest, something I didn't understand until a few years ago when I wrote part 2 of [_The Imposter's Handbook_](https://sales.bigmachine.io/imposter-second). 

As you can see, Kevin got a lot of replies and, if I'm honest, there seems to be a lot of confusion. Old Rob might take up the challenge here, striking up some exciting conversation on Twitter about the nature of one-way functions... but let's be positive and dig in to some details.

## The Enemy Knows the System

There are a lot of very wrong replies to Keven's tweet and I don't want to call anyone out, but I will say that the theme of wrongness goes something like this:

> I have a bunch of things and if I scramble those things up and give them to you, you'll have no way to unscramble them.

There were examples of candy, cake, a deck of cards, and so on and many of them made sense in a human way. I wouldn't want to unshuffle a deck of cards or unbake a cake! 

But see here's the thing: if I know the result (a lovely cake) and your _exact_ process, which I will because these are algorithms after all that _must_ produce the exact same result - then it's possible for me to figure out the initial ingredients - easily I might add. It might take a while and some guessing, but in short order I _will_ produce the exact same cake.

**When you bake a cake or shuffle cards, you're doing _encryption_**: turning one value into another following a process. 

Hashing, on the other hand, is completely different. **Hashing turns some value into an unrelated number using functions that you can't reverse** \- this last bit is the thing I think most commenters were missing. 

## What's a One-way Function?

Let's take our cake's individual ingredients and weigh them on a scale that only counts up to 11 grams and then starts again at 0:

![](https://bigmachine.io/img/2023/10/screenshot_172.jpg)

Some ingredients will weigh more than 11 grams, of course, but you would still record the number you see on the dial. For instance: 35o grams of flour would spin this dial around quite a few times before ultimately landing on the number 9.

This is a _modular_ operation; `350 mod 11` to be specific:

![](https://blog.bigmachine.io/img/2023/10/screenshot_173.jpg)

350 mod 11

Now, imagine that I do the same for every ingredient and record what the dial says using my `mod 11` scale, and then write that number down. It might end up being something like:

```
Sugar: 4
Flour: 9
Eggs: 3
Oil: 3
Vanilla: 10
Milk: 3
Salt: 0
Butter: 8
Baking Powder: 4
```

Now we have a number we can play with, or _compress_, in our hashing algorithm: `4933103084`. How did we get this number? Well you just saw me do it, but there's no way you could reliably figure out my ingredients list from here!

## Hey, It's a Video!

Want to see more? I made a video about all of this and I do hope you enjoy...

<iframe  src="https://www.youtube.com/embed/9gIi6UK6w4Q?si=mTp8CTcVcvZ7dYmb" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2023/10/hash-five.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2023/10/hash-five.jpg" />
  </entry>
  <entry>
    <title>\U0001F916 A Real World Approach to Playwright</title>
    <link href="https://bigmachine.io/posts/a-real-world-approach-to-playwright" rel="alternate" type="text/html"/>
    <updated>2023-08-23T21:03:03.000Z</updated>
    <id>https://bigmachine.io/posts/a-real-world-approach-to-playwright</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https:/images.unsplash.com/photo-1617575521317-d2974f3b56d2" alt="\U0001F916 A Real World Approach to Playwright" /></p>
I started using Playwright a few years ago but didn't dig into it until late last year when I upgraded my site to Nuxt 3\. I was paranoid that I was going to screw something up so I decided to **use an end-to-end testing approach** so I could sleep a bit better at night.

Just recently, **I released the final bits for my latest course, [_The Frontend Accelerator_](//frontend-accelerator/)** and I devoted an entire hour-long video to Playwright which is yours to watch free, if you want (link below - there's some context you should read first). 

If you don't know: **Playwright is a fascinating testing tool** for frontend applications. It uses various rules to snoop your DOM and can interact with it just like any user will.

I won't say that using Playwright is easy, but I find that I can use my favorite style of testing (scripted stories) without much trouble. Oh, and before I forget: you can use Playwright to do all kinds of testing, including unit and behavioral - I'm going to focus on e2e.

## Character and Plot

When you do end-to-end testing it's a good idea to understand who your viewers are and what they want from you. I suppose that's obvious, but it takes a degree of creativity and skill to do it well. 

This will always be a challenge for me, but I'll share with you something that I learned over the years: _create some characters with motivations and let your site provide the plot_.

I've never met you (probably), but I'm going to guess that you might be one of the following:

* A seasoned frontend dev that wants to do more frontend testing. You've heard of Playwright, but haven't used it much so are curious.
* A Playwright veteran who is curious about what I'm going to say and whether I'm full of crap.
* Mildly curious about frontend stuff, skimming the article.

There are probably a few more "buckets" but let's start here.

## Build a Story

Using these motivations, I can now create a script about your visit to my blog. Let's give a name to the seasoned frontend dev who's heard of Playwright... we'll call her _Anya_:

```js
import { test, expect } from '@playwright/test';

test.describe("Anya visits the Playwright post", () => {
  test.beforeEach(async ({ page }) => {
    await page.goto('https://robconery.com/playwright');
  });
  test("... and sees an engaging title", async ({page}) => {
    await expect(page).toHaveTitle("YOU WON'T BELIEVE WHAT PLAYWRIGHT DOES TO YOUR CODE")
  });
  test("... and watches a free video", async ({page}) => {
    const player = await page.getByLabel("video-player");
    await expect(player).toBeVisible();
  });
});

```

The idea is to script her interaction, understanding her motive for dropping by. Our application's answer to that is to grab her attention using a click-bait title and then show a free video as a reward for reading.

This is where the creative part comes in, at least for me. I need to put myself in Anya's position and do my best to figure out what will solve her problems. I'm going to guess she'll want to know more about the above code.

## The Power of Locators

One thing I absolutely _love_ about Playwright is how it pushes you to think about accessibility and assistive technology. We put `alt` tags on our photos, `title` on our links, I'm sure, but do you know how to use the ARIA tags?

If you didn't know (like me), ARIA stands for "Accessibility Rich Internet Application" and if you tag your application using `aria` attributes, it can be really helpful.

Most browsers have some form of Accessibility developer tooling. Chrome gives you Lighthouse, which examines your page and offers suggestions. Firefox has an actual Accessibility tab:

![](https://bigmachine.io/img/2023/08/screenshot_121.jpg)

This is how assistive technology sees my blog, and also how Playwright (using certain locators) will find DOM elements on my page.

### Choosing Your Locators Wisely

In my test example above, I'm using the `getByLabel` locator:

```js
const player = await page.getByLabel("video-player");

```

This locator is looking for the `aria-label='video-player'` attribute, which describes an interactive element (buttons, links, form elements, etc.). I could have also used in `id` or something more specific, such as `data-testid`, which is Playwright's way of saying "just give me this damned DOM element".

You can also `getByRole`, which allows you to find a link, for instance, with the text "Click Me". This is how folks with assistive technology see your app, and it's great that Playwright pushes you to think along those lines.

Personally, I find myself using only four of the locators:

* `getByTestId` which uses the `data-testid` attribute tag to find your element.
* `getByRole` as discussed above.
* `page.locator("#id")` which will find a DOM element by ID
* `getByLabel`, also discussed above.

The trick is to know which one to use, when. If something will help with ARIA concerns, that's what you should use. If something isn't interactive or something a viewer will care about, then `id` or `data-testid` works great.

## Wanna Watch a Video?

[This is the case study video](https://app.bigmachine.io/courses/accelerator/playwright) I put together for the Frontend Accelerator course. It's just over an hour long and uses the application we build during the walkthrough as a test subject.

It's not exhaustive, but I think it covers a major chunk of what you'll need to know:

* Configuring tests in a sane way
* Setting up authenticated users
* How to deal with some common problems
* Using the CLI, VS Code and the UI app that comes with Playwright

I had a ton of fun putting this course together and I'll be releasing it final in the coming week, at which point the discount will end so get it while you can!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https:/images.unsplash.com/photo-1617575521317-d2974f3b56d2" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https:/images.unsplash.com/photo-1617575521317-d2974f3b56d2" />
  </entry>
  <entry>
    <title>What Is Your Yeet Threshold?</title>
    <link href="https://bigmachine.io/posts/when-is-enough-actually-enough" rel="alternate" type="text/html"/>
    <updated>2023-07-10T02:24:42.000Z</updated>
    <id>https://bigmachine.io/posts/when-is-enough-actually-enough</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https:/images.unsplash.com/photo-1614638485257-7efdbb2f9495" alt="What Is Your Yeet Threshold?" /></p>
I'm wrapping up a **2.5 day, 9 hour thrashfest** where trying to fix this bug, right here:

![](https://bigmachine.io/img/2023/07/screenshot_17.jpg)

I'm working with Nuxt 3, Vue 3 and Vuetify for the [Accelerator Walkthrough](%5F%5FGHOST%5FURL%5F%5F/frontend-accelerator/) production I'm putting together and hit this little snag on Friday. It's now Sunday, right before noon PDT, and **I finally fixed it.**

I'm frustrated, of course. I did learn a few things, which is obviously groovy (and I'll share those)... but overall I think the thing that I exercised the most was my patience.

**I tend to pivot, quickly, when shit hits the fan**. Especially when dealing with three things as complex as Vue, Nuxt, and Vuetify.

I think [this quote from Adam Wathan](https://adamwathan.me/renderless-components-in-vuejs/), creator of Tailwind CSS, nails it:

![](https://blog.bigmachine.io/img/2023/07/screenshot_18.jpg)

I have an extremely low tolerance for nonsense and I faced a non-stop parade of nonsense this weekend that forced me to go outside and breath fresh air _far more_ than I wanted to.

Here's what happened...

## A Simple Tweak Explodes

I'm wrapping up the section of the walkthrough where we plug in authorization using an API call. Everything had started to click and I was breezing through the backend code stuff, and the final bit was to change the icon in the navigation list when the user logs in and verified as an owner of the given video course:

![](https://blog.bigmachine.io/img/2023/07/screenshot_19.jpg)

As you can see, the icons are laying out just fine and everything should work, right? This is when things fell apart and this fun little error showed up, destroying everything:

![](https://blog.bigmachine.io/img/2023/07/screenshot_17-1.jpg)

Everything stopped rendering properly when error triggered. The videos wouldn't load, the logout button stopped working, text didn't show up... painful.

Clicking through for more info led nowhere. No stack trace, nothing. Eventually **I tried a different browser (Brave) and saw something weird**. Instead of a "TypeError: child is null" error, I saw this:

![](https://blog.bigmachine.io/img/2023/07/screenshot_20.jpg)

Clicking through there I was able to set a few breakpoints and see what Vue was up to under the hood:

![](https://blog.bigmachine.io/img/2023/07/screenshot_21.jpg)

Vue is trying to remove an element from the DOM that it thinks shouldn't be there. But which one? Reading the `child` value didn't help at all as I couldn't tell what was being called to begin with.

Eventually I started removing everything, bit by bit, to see if the error would go away.

## Cut, Cut, Cut, Cut... Where Is This Coming From!?!

It took me 90 minutes to slowly and methodically slice out every single component in my application, and eventually I got to this:

![](https://blog.bigmachine.io/img/2023/07/screenshot_22.jpg)

This is the login modal that I've been using to log people in, which looks like this:

![](https://blog.bigmachine.io/img/2023/07/screenshot_23.jpg)

It pops up and, like so many sites out there, you get a code in your email and pop it in place. It looks great, sliding back and forth... but **using it in a modal window is, apparently, problematic**.

The solution? _I have no idea_. There is something in the way these windows are loaded into the DOM and removed from the DOM that is blowing up Vue. I didn't write any tricky code here - [I used exactly what was on the documentation page](https://vuetifyjs.com/en/components/windows/). In fact, if you scroll down that page you'll see the exact form I used for this login dialog.

_My_ solution turned out to be creating my own login page and ditching the modal. I could reuse most of the code and didn't need the sliding windows so ... what the hell. Turns out that it worked.

## Oh But It Wasn't Done With Me Yet

Creating my own page led to the next error, which was a new one, that happens when you click "Next" after entering your email:

![](https://blog.bigmachine.io/img/2023/07/screenshot_24.jpg)

Same deal - click through, no help. Try Brave, see a whole different error report:

![](https://blog.bigmachine.io/img/2023/07/screenshot_25.jpg)

This one, however, came with a crucial detail that I missed with Firefox:

![](https://blog.bigmachine.io/img/2023/07/screenshot_28.jpg)

Vue doesn't like a type I'm using with... an avatar? On my login page? Oh... right...

![](https://blog.bigmachine.io/img/2023/07/screenshot_26.jpg)

This is the little step number indicator in the top left. It doesn't like that I'm sending a number into `v-text`. So it blew up the application.

Wait a minute... where did this code come from? Oh, right, the Vuetify documention which is quite extensive and apparently full of weirdness:

![](https://blog.bigmachine.io/img/2023/07/screenshot_29-1.jpg)

I know they're doing their best and, if I'm honest, Vuetify has saved me mountains of time over the years. Except when I hit walls like this and end up giving that time back.

I was able to fix this by moving the `loginForm.step` to the slot, which is stupid, but it fixed the problem.

## Serious Q: How Long Do I Keep Beating On This?

I'm so, so close to being done with the UX "heavy lifting" but it's errors like this that stop me for _days_ that kill my motivation. I**'m trying to approach this whole walkthrough effort as real as I can make it** \- which means that I can't be afraid to stop and rebuild if I get cornered.

How long do _you_ wait? There are some great frameworks out there with ready-made templates that I could plug in and move on. I do know that they, just like any framework, come with their own quirks and ramp-up time too. I can't expect to just change and have it all work, but I _can_ expect troubleshooting to be easier the closer I get to pure CSS and HTML.

I really would love your feedback on this. Reply (if you get this via email) or pop a comment - I'd love to hear from you!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https:/images.unsplash.com/photo-1614638485257-7efdbb2f9495" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https:/images.unsplash.com/photo-1614638485257-7efdbb2f9495" />
  </entry>
  <entry>
    <title>What Should Be a Plugin vs a Composable vs a Store in Vue?</title>
    <link href="https://bigmachine.io/posts/what-should-be-a-plugin-vs-a-composable-vs-a-store-in-nuxt" rel="alternate" type="text/html"/>
    <updated>2023-06-16T01:31:03.000Z</updated>
    <id>https://bigmachine.io/posts/what-should-be-a-plugin-vs-a-composable-vs-a-store-in-nuxt</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https:/images.unsplash.com/photo-1512209840695-c9b154d2a2aa" alt="What Should Be a Plugin vs a Composable vs a Store in Vue?" /></p>
One of the things that has confused the hell out of me as [I work with Vue 3 (and Nuxt 3)](//frontend/trying-something-different-a-real-world-tutorial-for-frontend-programming/) is when I should be using a Composable, Plugin, or Pinia Store for centralized "stuff".

The reasonable thing to do, when confused like this, is to RTFM, which I have. Here's how the docs explain what these things are.

We'll start with [plugins](https://vuejs.org/guide/reusability/plugins.html):

> Plugins are self-contained code that usually add app-level functionality to Vue.

Seems pretty obvious, I suppose. I've also read that plugins "extend" Vue (and Nuxt), which I guess is another way of saying "app-level functionality".

Next up, [composables](https://vuejs.org/guide/reusability/composables.html):

> In the context of Vue applications, a "composable" is a function that leverages Vue's Composition API to encapsulate and reuse **stateful logic**.

I think the keyword here is "stateful" but I'll admit that I have no idea what "stateful logic" means. The documentation explains it this way:

> When building frontend applications, we often need to reuse logic for common tasks. For example, we may need to format dates in many places, so we extract a reusable function for that. This formatter function encapsulates **stateless logic**: it takes some input and immediately returns expected output. There are many libraries out there for reusing stateless logic - for example [lodash](https://lodash.com/) and [date-fns](https://date-fns.org/), which you may have heard of.

> By contrast, stateful logic involves managing state that changes over time. A simple example would be tracking the current position of the mouse on a page. In real-world scenarios, it could also be more complex logic such as touch gestures or connection status to a database.

I still don't see how there's a qualitative change in the idea of _logic_ here. Dates change in the same way a mouse location changes or, for that matter, the state of a database connection (open vs. closed). Moreover: I don't see how changing data should dictate whether I use a composable.

I've reread this page many times and have never come away feeling that I understood what it is the Vue team wants me to understand. I know they have a plan, it's just not getting through my thick head.

Finally, let's [dig in to Pinia](https://pinia.vuejs.org/introduction.html):

> Pinia is a store library for Vue, it allows you to share a state across components/pages.

Nice and concise! The only place where this gets weird is `actions`, which are described thus:

> Actions are the equivalent of [methods](https://v3.vuejs.org/guide/data-methods.html#methods) in components. They can be defined with the `actions` property in `defineStore()` and **they are perfect to define business logic**

Logic and state... would that be _stateful logic?_ I am genuinely confused on all of this and I'll cut to it: **over the last few years working with Vue 3, I've never understood (clearly) what goes where**.

So I came up with what I think make sense.

## A Case Study: Using Firebase

My confusion really flared when I needed to integrate Firebase into a Nuxt app I was creating for my main site, [bigmachine.io](https://bigmachine.io). I used to use a package for this but I had enough custom needs that I decided to just pop it in myself.

But where? Would this be a plugin, extending Vue and my application? Or is it a simple set of composable functions? Firebase _is_ a database, you know, and it also handles authentication so you _could_ argue that it's a state store. I'll take that last bit further: _Firebase **is**_ _your state store when working with Vue_. In my experience, that's the best way of working with it.

Confused? Me too. But it gets worse.

When using Firebase with Nuxt you have to consider what Vite (the server powering Nuxt and building Vue) is going to do. Unless you're running a static application, you'll have server-side "stuff" going on behind the scenes. _Firebase is a not a server-side thing_. If you try to use the client SDK on the server, you'll get an error as it looks for `window`.

I could descend into the details but I won't. Here are the basic constraints we need to live with:

* We need to initialize the client SDK on the client _only_
* Anything `auth` related needs to wait for initialization to happen so we can know the state of our `user`

Here's how I solved this problem.

### Plugins Are Out

Plugins are initialized on the server when using Nuxt (which I was using) which means our SDK will bonk on start. Yes, there are ways to mark plugins as "client only" which will work, but, to me, that's a code smell telling you "this isn't the right place, mate".

### Composables Could Be Made To Work, I Guess

If we thought of Firebase as a completely separate service and something our application talks to as needed, then yes a composable might work OK. I tried this as I like simpler approaches to things but I quickly found that trying to work with events (such as when the `user` is recognized and authenticated) was causing me to write a bunch of workarounds.

Listener functions that planted stuff in a Pinia store, for instance, that would then change the `currentUser` which would then ripple out throughout the application... it felt wrong and I hated it.

### Firebase is a Store, Treat It That Way

This is what I ultimately came to. I hated the idea of having two separate state stores running - one in the cloud and one in my app - that I had to synchronize. When I tossed out my `authStore` (with a few others) and just went with a general `firebaseStore` everything seemed to click.

I'm not entirely certain this is the way to do things, but it worked for me. I have an `init` method that I call `onmounted` in the `app.vue` component and everything works from there.

There are too many details to go into here, but I will be making a video case study on this as part of the [Frontend Accelerator production](%5F%5FGHOST%5FURL%5F%5F/frontend-accelerator/) so if you're curious - keep an eye out for that.

## In Summary: Rob's Way

I won't say that this is _The Way_, but it's helped me when trying to figure out what goes where, so here goes:

* **Plugins are generic, reusable bits of logic** you can use from project to project or open source. They extend Vue (or Nuxt) and make it easy to "drop in" something you need.
* **Composables are like Helpers** in Rails: reusable functionality that does a thing within the scope of a function. I have one I really like called `useSeo` which will build the header in Vue to have an image, twitter info, open graph and more. Simple, straightforward and reusable.
* **Stores deal with data across the app** but, to me, in the cloud as well. This could be from Firebase or wrapping an API.

I think my take on composables could use some detail, so here's the `useSeo` one I was mentioning and that you['ll also see in action in the Accelerator production](%5F%5FGHOST%5FURL%5F%5F/frontend-accelerator/):

```js
export default function({title, description, image}){
  const config = useAppConfig();
  const route = useRoute();
  const meta = [
    {hid: "title", name: "title", content: title},
    {hid: "description", name: "description", content: description},
    {hid: "og:title", name: "og:title", content: title},
    {hid: "og:description", name: "og:description", content: description},
    {hid: "og:image", name: "og:image", content:  `${config.siteRoot}/images/${image}`},
    {hid: "og:url", name: "og:url", content: `${config.siteRoot}${route.path}`},
    {hid: "twitter:title", name: "twitter:title", content: title},
    {hid: "twitter:description", name: "twitter:description", content: description},
    {hid: "twitter:image", name: "twitter:image", content: `${config.siteRoot}/images/${image}`},
    {hid: "twitter:creator", name: "twitter:creator", content: config.twitterHandle},
    {hid: "twitter:site", name: "twitter:site", content: config.twitterHandle},
    {hid: "twitter:card", name: "twitter:card", content: "summary_large_image"}
  ];
  useHead({
    title: `${title} | ${config.title}`,
    description: description,
    meta
  })
}
```

Two things to know if you want to use this:

* `useAppConfig` is a Nuxt thing which pulls in global data. If you're using straight Vue you could have a `siteStore` which has things your site title, email, twitter handle, etc.
* `useHead` is from the [unjs crew](https://github.com/unjs/unhead) and you'll need it installed in order to inject the `head` with this stuff

Hopefully you can see how this is a helper function as opposed to a store or plugin? It seems that way to me so... I'm going with it.

Have some counter thoughts or different ideas? Leave a comment! I'm not sold on any of this entirely - it's just what has made sense to me over the years and I'd love to hear from you.

Hope this helps!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https:/images.unsplash.com/photo-1512209840695-c9b154d2a2aa" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https:/images.unsplash.com/photo-1512209840695-c9b154d2a2aa" />
  </entry>
  <entry>
    <title>Trying Something Different: A Real World Tutorial for Frontend Programming</title>
    <link href="https://bigmachine.io/posts/trying-something-different-a-real-world-tutorial-for-frontend-programming" rel="alternate" type="text/html"/>
    <updated>2023-06-10T00:24:55.000Z</updated>
    <id>https://bigmachine.io/posts/trying-something-different-a-real-world-tutorial-for-frontend-programming</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https:/images.unsplash.com/photo-1496355723323-30286a0b340d" alt="Trying Something Different: A Real World Tutorial for Frontend Programming" /></p>
**TL;DR** _I'm making a course on Vue 3 and Nuxt 3 which is about 60% complete. There's a book (90% done), a 30-ish video walkthrough of building an actual application (I'm 75% done with that), and 3-5 case studies. It's available for presale now and you can [read more here](//frontend-accelerator/) or just [go and buy it here at 50% off](https://sales.bigmachine.io/accelerator/) for the presale. You can read the book today and watch 18 of the planned videos as well. You'll own everything forever._

---

Sharing something you've learned is such a rush, especially if it's changed the way you work, think, or generally see the world. To me, this is a key part of [getting over Imposter Syndrome](%5F%5FGHOST%5FURL%5F%5F/career/seven-simple-steps-to-overcoming-imposter-syndrome/): _share what you've learned before you forget what not knowing it was like._

But how do you share something in a way that people can understand and, hopefully, also gain the same value? This question has plagued me for years.

## The Contrived Demo Trap

Let's say you've finally learned Vue 3 after years of wanting to but never getting around to it. It was an exciting journey but not without **numerous failures and obstacles** you had to overcome - almost like playing a video game! You think to yourself: _I can probably save people some time and effort if I wrote a post and let them know about some of the obstacles I faced..._

This is very generous of you! You sit down and start to write your post and then **realize that explaining the entire back story of a given obstacle would take up most of the post**, so you distill the idea down into a `foo` and `bar` demo because, hey, the details don't matter, do they?

Maybe you use a Todo list, a pet tracker, or some other nonsensical app that no one makes. You're trying to convey concepts, after all, so hopefully people can fill in the gaps.

The trap has sprung. **Showing someone the end result is like telling the punch line of a joke first**, then filling in the details. It _can_ be funny, but it's usually boring.

## It's About the Journey, Not the Result

Your decisions leading up to the obstacles are a critical part of understanding the _why_ you `foo`'d and `bar`'d your solution. When Vue's router wouldn't resolve the component properly - sure - you were able to fix it but how did you get to that point in the first place?

I do realize that writing 10,000 words just to show your solution to a weird router bug is out of the question. That would be _truly_ boring, unless you're an amazing writer which maybe you are!

I don't have an exact answer for this, but I will say that I would _much_ rather read someone's solutio to their own problem, rather than a made up Todo list. Maybe try writing about it in summary form: "so there I was, my app in pieces on the ground as I tried to get this component to render properly and `VueRouter` was laughing at me. Just prior to this, I had..."

A journey in summary form is a wonderful thing. People love journeys and especially love understanding the monsters you faced along the way.

## It Reminds Me of Elden Ring

Elden Ring is my favorite game of all time, right next to Diablo 2 and 3\. It took me forever to get into it, however, as **Elden Ring is hard as hell**.

Until I [found this](https://www.youtube.com/watch?v=PN7YFKHOR9Y&list=PL7RtZMiaOk8gdRf130w4gFYyhstL-5VRh):

![](https://bigmachine.io/img/2023/06/bip_1880.jpg)

The 82-episode Elden Ring Walkthrough from FightinCowboy

This guy, "FightinCowboy" (I don't know his real name) **created an absolute _masterpiece_** with his 82-video walkthrough of Elden Ring. Each video is 30-ish minutes, keeps a brisk pace, and his narration is pretty damned solid.

I watched the whole thing - which is a major feat because finishing this game requires 100+ hours of gameplay. _It's massive_. Cowboy's videos became something more than a walkthrough to me, however. It was like playing the game with a friend. 

**When I reached the end of the walkthrough I was actually sad,** as were many others. Reading through the comments made it pretty clear: _this guy touched people's lives:_

![](https://blog.bigmachine.io/img/2023/06/image.png)

I know, I know! It's a video game - how can people be so _maudlin_ about running around dungeons and trekking through castles? You might think it's all a bunch of weird gamers, too, but nope:

![](https://blog.bigmachine.io/img/2023/06/image-1.png)

**As a content creator, this is the ultimate high: touching someone's life, making their world just a bit brighter.** That's why I do what I do, at least.

Hey wait a minute... what if I could do something like this, but for building an application?

## 🤔 I Wonder if I Could Do The Same for Coding Videos?

Building and shipping applications, to me, has the same set of ups and downs as playing video games. It's all about decisions and experience... at least that's my opinion on the matter.

**The development journey... that's the fun part**. I want to capture that fun - and that's exactly what I've started doing with my own walkthrough using Vue 3 and Nuxt 3:

![](https://blog.bigmachine.io/img/2023/06/bip_2019.jpg)

The app, so far

As of today, **I am 22 episodes in** to (what I think will be) 30-ish total. It's a full walkthrough of how I have built and shipped 10+ sites using:

* Vue, both 2 and 3
* Nuxt, both 2 and 3
* Pinia (state store for Vue)
* Nuxt Content (markdown CMS for Nuxt)
* Vuetify (material design kit for Vue)

Many of the sites I made are for my business, but a few were for clients and one was internal to Microsoft. The point is: _I've built a lot of these things and learned some tough lessons_. I'd like to share all of that with you.

## Come Code With Me

The goal, here, is to **make this seem like a pair-coding experience** where we build a complex application together. That application will ultimately get shipped and will be the exact application that people will view the walkthrough on.

So far, **I've hit the wall numerous times and I've recorded all of it**. Yes, I've shipped a bunch of applications but I still run into problems - the same ones you'll run into when you go to build something with these tools.

I do edit these videos so you won't be watching me thrash. I'm trying to keep them "episodic", which means you can watch them in the same way you might watch Mandalorian - one or two during your lunch break or at night. Or just binge it... why not?

Hopefully, by watching this walkthrough, **you'll remember that you faced this problem before** and you'll know how to solve it.

### There's a Book, Too

I've also written a 300-page book on Vue 3 and Nuxt 3 with a focus on self-taught people like me. The book is more conceptual and, therefore, uses a demo that's a bit more lightweight but is also something you can take and use on your own.

I would say the book is 90% complete with grammar, formatting and final tech checks on the horizon.

### Case Studies as Well

You can't cover everything in these walkthroughs, so I'm also going to add some targeted case studies, such as:

* How to integrate Firebase (auth, storage, data, etc)
* Migrating from Vue/Nuxt 2.x to 3.x
* Using Playwright to test your app

This is a tentative list and will likely change based on feedback during this presale.

Oh yeah - I forgot to mention...

## This Is Available Now for Presale at 50% Off

I wrote _The Imposter's Handbook_ completely in the open right after I hit the 60% complete point. It was a gamble, but it paid off as so, so many people jumped in with help and suggestions. The book would look nothing like it does today without that process.

So I'm doing the same with this effort. I'm trying to pack a TON of information into a complete offer so you can shortcut the pain of getting up to speed with these frontend application frameworks. They're super powerful, but they take a lot of time to master.

Anyway: if you're interested, **the presale is open** and I invite you to have a look:

[Todo list tutorials just don’t cut itJavaScript client frameworks are powerful and help you create an amazing experience for your end user. Unfortunately, learning how to use them sucks.![](https://blog.bigmachine.io/img/size/w256h256/2022/08/suns-1.png)Rob ConeryRob Conery![](https://blog.bigmachine.io/img/2023/09/860997441.jpg)](%5F%5FGHOST%5FURL%5F%5F/frontend-accelerator/)

I would absolutely LOVE to have your help, if you're willing, and yes you'll have access to everything forever.

If you want to sign up right now, you certainly can:

[Get Access Now at 50% Off](https://sales.bigmachine.io/accelerator/)

I have [a project page](https://github.com/users/robconery/projects/2) up and running so you can track my progress. I turned on [Discussions](https://github.com/robconery/nuxt-walkthrough/discussions) as well, so you can ask questions, make suggestions and more. And of course I have [issues all set up](https://github.com/robconery/nuxt-walkthrough/issues) right next to the code.

I really like this idea and while I know there have been other attempts by people to "build something with me", I don't think anyone has done this with a focus on a "playthrough", if you will - edited for pace and assembled to tell a story.

If you have any questions please leave a comment below or fire an email reply (for those on my list).

## The Roadmap

Ive been at this since April but was interrupted briefly to prepare for a conference and also to setup the site and blah blah blah. My goal is to be finished by August 15th as that's when I leave to go travel Europe for a while. It's not a hard date, but it sure would be nice to be finished!

I have the recording cadence down and things have become pretty seamless. The case studies might take a bit longer, but the writing (book) and video outlines (walkthrough) are all done.

This, of course, depends on what kind of feedback I get from people signed up for the presale. If you have some rad ideas, I might just tweak the schedule so I can accomodate!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https:/images.unsplash.com/photo-1496355723323-30286a0b340d" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https:/images.unsplash.com/photo-1496355723323-30286a0b340d" />
  </entry>
  <entry>
    <title>Incoming: Nuxt Walkthrough Preview</title>
    <link href="https://bigmachine.io/posts/incoming-nuxt-walkthrough-preview" rel="alternate" type="text/html"/>
    <updated>2023-05-08T01:44:16.000Z</updated>
    <id>https://bigmachine.io/posts/incoming-nuxt-walkthrough-preview</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https:/images.unsplash.com/flagged/photo-1571771710019-ca58cf80f225" alt="Incoming: Nuxt Walkthrough Preview" /></p>
Hey there subscriber friends, this is a note just for you. First: as always **thanks so much** for being a premium subscriber. Your support means a ton to me and honestly keeps me going!

**I'm about to drop a preview of my next Big Effor**t and I wanted to make sure to address any questions you might have about it, so here goes.

## What Is It?

Long story short: **I've been building Vue/Nuxt applications non-stop for the last 6 years** and even shipped one for Microsoft, which was wild! Recently I shipped one for [one of my video apps](https://vue.bigmachine.io,), which was a fascinating study in patience.

I learned a lot **and decided to share**, but I wanted to do it as "real" as I possibly could, so I'm making an "episodic walkthrough", which will be **25-30 videos roughly 30 minutes in length**, lightly edited for pace. I'll build out a video application and treat you like you're pair coding with me.

In addition, I'll be **adding "Case Study" videos**, devoted to a single topic. Things like testing with Playwright, using Firebase, "just show me what I need to know about JavaScript and Typescript" - things like that. Those will be around 1-2 hours apiece, tightly edited and focused.

There are other things to go along with that - but I'll mention that in another post which will be public, and I'll also include a video about what's coming.

In short: _it's a lot_. **A massive brain dump** of everything I've learned about Vue, Nuxt, Vuetify, Firebase and more.

## What You Need to Know

I'm pricing this around $199/$299 **but you, as my subs, won't pay a thing**. I'll send you out a coupon code you can use, if you like, to access the goods. I'll also post a few of the videos here for your access only but the walkthrough will be hosted on another site.

**If you have any questions please don't hesitate to reply.** I'll have another post coming in a few days which will cover things in more detail, so stay tuned.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https:/images.unsplash.com/flagged/photo-1571771710019-ca58cf80f225" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https:/images.unsplash.com/flagged/photo-1571771710019-ca58cf80f225" />
  </entry>
  <entry>
    <title>Everyone Has a Plan, Until They Get Hit in the Face</title>
    <link href="https://bigmachine.io/posts/my-day-completely-thrashing" rel="alternate" type="text/html"/>
    <updated>2023-04-21T02:45:16.000Z</updated>
    <id>https://bigmachine.io/posts/my-day-completely-thrashing</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2023/04/bip_1822.jpg" alt="Everyone Has a Plan, Until They Get Hit in the Face" /></p>
That's a quote from Mike Tyson, who was paraphrasing another quote from a German military strategist:

> No battle plan survives contact with the enemy

Great quotes, which fit my experiences over the last week. You see, **I'm experimenting with a new format for online videos**, which I think will be interesting... but it requires a ton of patience. And a steel jaw.

In short: I've deployed the stack I'm working with (Vue, Pinia/Vuex, Nuxt, Nuxt Content (more recently), Vuetify and Nitro (also more recently)) 8 total times over the last 3 or so years. I've done this for myself (for online course stuff) and for Microsoft (internal projects). I enjoy it, I really do, but there are enough moving parts that **it's inevitable you're going to walk face-first into the right hook** of framework chaos.

If you don't know: Vue is a popular JavaScript frontend framework and Nuxt is the opinionated app framework built on top of it. Vuetify (the subject of this post) is a UX design framework (a bunch of components) that is tremendously useful for buidling complex UIs. I love these tools. I use them often. I dig pain.

That's what happened last weekend.

## My Idea: The Soap Opera Walkthrough

The video format I'm going for can best be described as "episodic walkthrough", or more directly: "coder soap opera". I'm _doing it live_, building a complete application from the ground up. **Less editing, more looking over my shoulder** as I build out something I know very well. You gain real-world experience and **see exactly how someone with their livelihood on the line actually builds things.**

That last bit is very important. This _is not a to-do list, DVD rental app or a blog_. This is an [actual site that I have deployed](https://vue.bigmachine.io) to make customers happy. It's a complicated thing, as anyone knows who has tried to watch a tutorial video and translate it into the Real World: to-do lists can only take you so far, which isn't that far at all.

I've made a lot of these things. **I know the mistakes, the wrong assumptions, the shortcuts and the tricks to use to actually get working software out the door.** I think that's a lot more valuable than rehashing concepts you can read from the documentation.

If all goes well, you'll end up with something that looks like this, which is a real, working site that I deployed a few months ago:

![](https://bigmachine.io/img/2023/04/bip_1798.jpg)

That's the plan, anyway. But then again...

> Everyone has a plan until they get hit in the face.

**It took 9 episodes for things to go completely pear-shaped**, which they did, in spectacular fashion. This wasn't a simple bug that I had to hunt down (and there were many of those) - this was a full-stop, critical bug that I had no idea how to fix.

## WTAF?

As I mention I'm 9 episodes in - around the 3.5 hour mark - and Vuetify landed a beauty right on my chin:

![](https://blog.bigmachine.io/img/2023/04/bip_1813-1.jpg)

What you're seeing here is the menu bar for a video course viewer (the app I'm making), with the last two icons (discussions and GitHub link) being repeated. More than that - the first four icons are shoved to the left about 180 pixels... _for no apparent reason_.

The console had some generic Vue warnings regarding a "hydration mismatch" which are common and happen if you have invalid HTML, among other things. This is what it looks like:

![](https://blog.bigmachine.io/img/2023/04/bip_1762.jpg)

A most annoying Vue error

But I don't have invalid HTML. Not that I could tell, anyway. I'm using bare-bones Vuetify, which is designed to "just work" and, moreover, I don't have any tricky `if` statements or loops that would cause things to duplicate like they are.

## Yes, I Found the Answer (I think)

I tried recording a debug session but 45 minutes in I was swearing far more than I wanted to and generating what I consider to be pointless content: me, trying anything and everything to make something happen. In other words: _completely thrashing_. You don't want to see that.

During one of the thrashing moments I decided to change the way I was outputting the links

![](https://blog.bigmachine.io/img/2023/04/image.png)

Vuetify in action

All of that `v-` stuff is Vuetify, specifically buttons and icons that have prebuilt styling. It looks great, when it works! Anyway - when I removed the `v-button` everything lined up, like magic:

![](https://blog.bigmachine.io/img/2023/04/image-1.png)

This is what it should look like.

Which told me there was something weird going on with regards to the DOM and my buttons. But there's more! As you can see from the problem screenshot, where things are repeated (scroll up a few screenshots) - _only two buttons were repeated_, not the whole lot!

This was a clue!

![](https://blog.bigmachine.io/img/2023/04/image-2.png)

**The repeated bits have dynamic links** which come from Pinia, the state store. The non-repeated bits have hard-coded values from the `href` tags, which you can tell because of the `:href` notation using the prepended colon.

If you write larger applications using a frontend framework it's generally a good idea to keep your reactive data (stuff that changes or is needed in more than one place) in a centralized store. That's what Pinia is for Vue.

There are three parts to a Pinia store: the _state_ (aka reactive data), the _actions_ (methods that mutate state) and _getters_ (computed, read-only functions). Here's what my state looks like:

![](https://blog.bigmachine.io/img/2023/04/image-3.png)

The variable we care about here is `course`, which as you can see is defaulted to an empty object. I load up the course using an action called `setCourse`:

![](https://blog.bigmachine.io/img/2023/04/image-4.png)

I'm at the wireframing stage of development, which is why you see `stub` here - it's just fake data to get us off the ground. But the big realization came with the way I was setting `course`, which is outlined in red.

## Respect the Reactive

Pinia will automatically wrap any variable declared in the `state` block as `reactive`, which is a special Vue function that turns ordinary JavaScript objects into little evented bits capable of notifying the application if they change (aka "reactive"). 

The problem here was simple, and also annoying as I had run into this before and didn't remember as I was recording! **If you replace the state variable entirely, you blow up reactivity**. Here, I'm reassigning `this.course`, which you can do with Pinia and things will still remain reactive. Vuetify, however, doesn't see it this way. The first `course` value does not equal the second `course` value so **it helpfully adds a second element to the DOM for you.**

Makes sense, I suppose. Would be nice if it popped a warning about that but it would also be nice if I could play better and remember things.

The fix becomes straightforward at this point:

![](https://blog.bigmachine.io/img/2023/04/image-5.png)

The fix

**If you're going to wholesale replace a bit of state, make sure you use** `Object.assign`, which will graft one object's values onto another.

Once I did this, everything just worked!

## Is This Valuable?

I'm going to assume that it is because **if watching this saved you the 3 hours I lost (not to mention the frustration and very real possibility of giving up on Vuetify altogether) then yes, I think there's value here**.

Vuetify is easy to swear at but if you know it's quirks, it's incredibly powerful. You can prop up an amazing site in a weekend - but yeah you need to know the particulars.

That's why I'm making these videos, which are supposed to go along with the book I just finished the first draft of. **The conceptual stuff goes in the book, the Real World stuff goes into the soap opera videos**.

Specifically: I want to walk through building an application with the above components, but I also want to show how I would integrate Stripe (payment processor) as well as a full API using a tool like Sequelize. I want to show how I test these things (using Playwright) and also do simple authentication.

That's a tall order, to be sure, but it becomes more doable if I follow the soap opera idea: light editing to keep a good pace, play-by-play, etc. Probably the longest video I'll ever make - I'm up to 3.5 hours already and I'm barely 1/3 of the way through!

I would love to know if this is interesting for you, or if you have any suggestions. You can leave a comment if you like (you have to be logged in) - would love to hear from you!

Hope this post saved someone a few hours...]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2023/04/bip_1822.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2023/04/bip_1822.jpg" />
  </entry>
  <entry>
    <title>How Elixir&apos;s Concurrency Changed Me as a Programmer</title>
    <link href="https://bigmachine.io/posts/how-elixirs-concurrency-changed-me-as-a-programmer" rel="alternate" type="text/html"/>
    <updated>2023-03-21T02:18:19.000Z</updated>
    <id>https://bigmachine.io/posts/how-elixirs-concurrency-changed-me-as-a-programmer</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2023/03/bip_1707.jpg" alt="How Elixir&apos;s Concurrency Changed Me as a Programmer" /></p>
Back in 2016 I learned the Elixir Programming language and had a blast. I hadn't used any functional languages up until that point so I was as green as green gets. I suppose that was intentional.

## Functional Programmers Can Be Annoying

There's a certain smugness that functional programmers get when you discuss _anything_ language-related. A practiced condescension they wear with great pride, like a yellow hat that only people who wear yellow hats could appreciate.

I don't like the color yellow very much (though it's in the following video quite a lot) and I don't wear hats. I don't quite have the right head for one - but I do like Elixir and despite it's functional roots, I found the community to be full of non-yellow-hat wearers.

You don't need to know what a functor is to use Elixir. Nor do you need to understand monads or mutexes. Eventually it's a good idea to get to know these things, but you can happily jump right in and splash around and no one will come at you with a jargon gun. At least that was my experience.

## Elegant, Fast, Amazing

Elixir flexes the friendly, approachable, bubble-gumminess of Ruby to get around the general faffery of Erlang - all to great effect. You have the entire, mountainous gravity of the BEAM, Erlang's VM, running under the most elegant language I've ever used.

I know, I know. People waxxing on about their new language discovery can be annoying. But this, for me, was one of those "**oh holy shit this is why people pay this much money for these shoes**" moments. My brain just slipped right into this language and wiggled it's neurons happily... and yes, there was a learning curve, but my god... my god...

To top it off, it's one of the most scalable, powerful platforms you can use. What's not to like?

## A Great Language Needs a Shite Demo

I was at NDC London in 2016 and decided to answer a question that I received oftent: 

> I hear you're really into Elixir these days. What's so great about it?

There are so many things... the video below is a 20 minute excerpt (edited down for time) from the talk I gave. The entire talk was about an hour but, to me, this clip captures the essence of why I like this language so much.

Hope you enjoy it!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2023/03/bip_1707.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2023/03/bip_1707.jpg" />
  </entry>
  <entry>
    <title>Seven Simple Steps to Overcoming Imposter Syndrome</title>
    <link href="https://bigmachine.io/posts/seven-simple-steps-to-overcoming-imposter-syndrome" rel="alternate" type="text/html"/>
    <updated>2023-02-06T02:59:10.000Z</updated>
    <id>https://bigmachine.io/posts/seven-simple-steps-to-overcoming-imposter-syndrome</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https:/images.unsplash.com/photo-1524117074681-31bd4de22ad3" alt="Seven Simple Steps to Overcoming Imposter Syndrome" /></p>
I'm committing the cardinal sin of online digital marketing wonks: **I'm still editing, refining and tweaking a book I released _almost 7 years ago!_** You would think that I would move on by now... but... nope.

Here's my problem: I never addressed imposter syndrome directly in _[The Imposter's Handbook](https://bigmachine.io/products/the-imposters-handbook)_, which has the word _imposter_ right in the damned title! This is a problem we've all felt, and I'm happy to say I've moved beyond it - but I never addressed just how I did that, until last night, when I added an entire section devoted to that problem.

So, here you go: **Rob's seven step guide to getting past imposter syndrome** and breaking out of a shitty cycle of self-loathing and defeat. Oh - and if you want to get the updated version you can [download it right here](https://app.bigmachine.io/downloads). Just pop your email in and, if you've bought it already, I'll send you a link. **If you haven't bought it yet, [what the hell are you waiting for!](https://bigmachine.io/products/the-imposters-handbook)**

## Let's Talk About Imposter Syndrome

You might feel perfectly at home in your job as a programmer, or you may feel that you’re fooling everyone and if you don’t act quickly you’ll be found out, canceled and your entire world blown to hell.

I used to feel this way. I _hated_ it. Sometimes I find myself falling into the trap of “what the hell do you know, anyway? Who are you to be telling people about compilers and RSA encryption?”

Both of those are good questions that I think deserve and answer, but it all depends on how my inner voice decides to ask those questions.

![](https://bigmachine.io/img/2023/02/bip_1573.jpg)

My inner voice can be **challenging**.

With apologies to my Aussie friends out there - I really don’t think y’all are mean or anything. There’s just something… so… _non-bullshitty_ about the Aussie twang. Like you know it’s telling you the brutal truth with a smile.

Anyway, I’m not a psychologist, believe it or not, but I have read a _ton_ about emotional health and well-being over the years and I put a lot of what I read to work, and I would like to share that with you now.

## Overcoming Imposter Syndrome

I took some time last year to lay out the steps I took to move beyond my imposter syndrome. At first I thought that it just naturally ebbed as I learned new things but nope, that’s not at all what happened.

What I found when I looked over my journal and notes from putting this book together (videos too) is a _self-reinforcing cycle_. I guess you could also call it a _resonant cycle_ or just an _upward spiral_ \- but the idea is generally this: **one good thing led to another**, which pulled me out of the negative storm of imposter syndrome.

Sounds easy, right? “_Just get over it and think positive_!” I really hate advice like that but, at its core, this is basically what happened to me although it happened in much smaller steps, which I’ll share with you now.

I’ll start with a little splash of cold water that’s probably going to sting, but stay with me, it’s important.

## Imposter Syndrome is Toxic

This is the stark truth. If you’re convinced that you don’t belong somewhere because you’re fooling someone, **_you_ are causing problems for others.** It’s counterintuitive - how could you be causing problems if you see _yourself_ as a problem?

The simple answer is this: **people are generally nice and care about you**. When you’re upset, they want to help. If you keep asking for help, they’ll keep helping and this will lead to a lopsided social dynamic that lopsidedly revolves around you. This will eventually cause problems.

Here’s a not-so-fun conversation along these lines:

**_Me_**_: Wow, I am in so far over my head… I don’t know how I got hired here in the first place. I feel like studying for my interviews as hard as I did gave the entirely wrong impression of my skills._

**_Friend_**_: they do this kind of thing every day Rob, I think they know when someone has talent and when they don’t. You’re fine, just focus on getting the work done._

**_Me_**_: I’m hardly fine - they expect me to X and Y and somehow they think I know what I’m talking about. I really don’t!_

**_Friend_**_: didn’t you ship 2 applications last year with X and submit to PRs to the project, which were accepted?_

**_Me_**_: yeah well we all get lucky sometimes._

**_Friend_**_: you sound like a broken record, get over it. You’ll do fine._

**_Me_**_. But I really think that –_

**_Friend_**_: ROB. Seriously, you’re fine. Shut up._

![](https://blog.bigmachine.io/img/2023/02/bip_1596.jpg)

One of the worst things in life is to have someone pile on the pain when you’re feeling exceptionally crappy. But when it comes to self-obsession (which imposter syndrome is), the only way out is through.

Thankfully, _through,_ for us, simply means realizing what you’re doing and seeing it for what it is: **a social problem**.

## Using Gratitude to Reverse The Suck

I’m sure you’ve heard this before: _be grateful! Write down 5 things you’re grateful for and you’ll feel so much better!_ This never made sense to me. Give me a pint and I’ll feel better!

The truth is: _it actually works_. But you have to mean it and feel it, and really dig into the deep well of happy feelings that are already sitting inside of you. Believe me: I hate sounding like a guru-in-training but damn if it doesn't really work! I used to want to punch people in every one of their glowing chakras who would tell me to "think positive thoughts"... but when you _really_ dig in and think about the things in life you love and are grateful for, well there's true power there.

I decided to take “grateful breaks” whenever I felt crappy about work, life, or my future. I learned to recognize these foul moods and give myself 7 minutes (don’t ask, it’s the way my brain works) to ponder the **simple** things that I am truly grateful for:

* Something comfortable I’m wearing, like clean socks or new shoes.
* My kid’s smile when she tells a funny joke.
* The fact that I get to write books like this which help people.
* The smell of freshly baked cookies.
* Chicken pot pies.

Yes, pretty mundane things to be grateful for, but that’s actually the point, believe it or not. We don’t spend enough time recognizing how fortunate we are, which is a powerful exercise.

The entire purpose here is to splash our insides with the golden glow of happy thoughts. _Your_ goal should be to cause a _true smile_ and if that smile comes from something work-related, even better.

**You know how to code**! That’s a pretty amazing skill, I think. Even better, **you know how to _type_**, which a lot of people can’t do! That means **you also know how to _read_**, which much of the world can’t. You were also able to buy this book, which makes you pretty well off.

![](https://blog.bigmachine.io/img/2023/02/bip_1598.jpg)

Photo by [Linus Nylund](https://unsplash.com/@dreamsoftheoceans?utm%5Fsource=ghost&utm%5Fmedium=referral&utm%5Fcampaign=api-credit) / [Unsplash](https://unsplash.com/?utm%5Fsource=ghost&utm%5Fmedium=referral&utm%5Fcampaign=api-credit) with text added by me

You get the idea. It’s so easy to forget the advantages we have, especially in our industry! The jobs, the creativity, the people we meet and the innovation!

Ride that wave, friendo! It’s a wonderful feeling and you can call it up any time you want, which is kind of a super power!

## Unleashing Your Natural Curiosity

When your gratitude flows it causes positive feeling and thinking, which can unblock so many things locked up in your brain. That little voice that tells you you’re stupid or don’t belong tends to grow quieter as you give yourself permission to _seek answers_.

This can cause confusion and disorientation, which will cause our happy wave to collapse, but that’s OK! Being curious is so, so powerful and if you let it flow it will take you somewhere fun.

The trick is to _let it be there_ and to give yourself the space to ask questions. I like to keep a list of things I’m curious about, along the lines of Richard Feynman’s [12 Favorite Problems](https://juliasaxena.com/12-favorite-problems-pop-writing-how-to-ask-better-questions/)_:_

> You have to keep a dozen of your favorite problems constantly present in your mind, although by and large they will lay in a dormant state. Every time you hear a new trick or a new result, test it against each of your twelve problems to see whether it helps. Every once in a while, there will be a hit, and people will say, ‘How did he do it? He must be a genius!

![](https://blog.bigmachine.io/img/2023/02/image.png)

Richard Feynman, from [The Pleasure of Finding Things Out](https://www.amazon.com/exec/obidos/ASIN/0465023959/)

I’ll talk more about writing things down in just a minute, as it’s a critical part of this process. The main thing I want to get across to you is that your curiosity is critical to propelling you forward from this point.

In fact, I hope you’re seeing that each step in this process builds on the previous! Stopping the negative spiral by seeing your toxicity is the start, and then reversing the process with gratitude is the second stage.

Using your natural curiosity as a fuel is the third (and very important) next step. Without this, it all falls apart.

## Removing Obstacles with Courage

I’m sure you have a friend who’s told you to “just go out there and…” or “you just need to tell them how you feel” or maybe “you’ll be fine, you just need to learn to stand up for yourself.”

**Feh**.

This is easy advice and also easily useless. How do you just “become courageous”? After all, that’s what our friend is telling us when they say “just stand up for yourself”.

The answer, at least to me, is that _you just need a good reason_. We’ve all had those moments where we say or do something and surprise ourselves (for better or worse) so we just need to find one of those (a good reason).

But where?

I’ll give you one: **_you don’t want to lose this happy momentum_**. If you actually do the process I’m suggesting (toxic realization, gratitude, letting your curiosity out) you’ll start to feel a surge of happy creativity. If you stop now, it means descending back into the Pit of Sucky Self-obsession that no one wants to be around.

But here’s the thing: _it’s doesn’t require a ton of courage to begin with, and it’s mostly dealing with just yourself_. Your only task is to remove the stupid obstacles in your own mind that are stopping you from _finding out_.

![](https://blog.bigmachine.io/img/2023/02/bip_1600.jpg)

My kid's active imagination

Your natural curiosity wants to know stuff! LET IT. You can trust yourself by courageously removing the blockers in your own head. I told myself that I wanted to learn all the things in the MIT CompSci curriculum and my inner voice instantly thought that was nuts! I had more important things to do, like finish my next video for Pluralsight (whom I was working for at the time).

My inner voice sounded something like this:

> More videos, more money mate. Simple equation - now get back to work you daft twat!

Maybe this is why my inner voice sounds Australian to me (though this last was a bit more Birmingham) - I find it easier to laugh at and ignore.

Your instincts are there to guide and help you. You would be surprised at how often they can help you find something wonderful as opposed to fear, which almost always kick-starts the downward spiral into self-obsession and self-loathing.

The only way to know if your instincts are serving you is to actually let them do it. Give yourself the space to _try something new_ or to go completely off the rails for a little bit, following your muse.

Fight the tendency to see this as permission to screw off all day - it’s not. **You have a powerful pattern recognition machine behind your intelligent, reasoning mind** that is exponentially more powerful. When you don’t give it exercise, it causes you problems.

Dr. Maxwell Maltz devoted [an entire book to the idea](https://www.amazon.com/Psycho-Cybernetics-Updated-Expanded-Maxwell-Maltz/dp/0399176136) that's one of my favorites. It has a goofy title, but a wonderful premise:

> A human being always acts and feels and performs in accordance with what he imagines to be true about himself and his environment...For imagination sets the goal ‘picture’ which our automatic mechanism works on. We act, or fail to act, not because of ‘will,’ as is so commonly believed, but because of imagination.

You inner giant will go where it will, but you can gently guide it by feeding it with the good thoughts you're cultivating.

## Journaling and Taking Notes

This process becomes real when you write it down. Having dreams and ideas is wonderful, but when you put it in ink and describe your goal, it becomes a commitment.

If you’ve given yourself permission to go where your curiosity takes you, **document the journey**. I cannot stress this enough! I use both a journal and a note-taking app and they have been absolutely critical to this process.

In fact, when I’m feeling doubtful or my inner voice is getting the better of me, I’ll write myself little bits of encouragement. The best are my stick figure doodle friends that tell me I’m OK and doing good stuff.

If you haven’t journaled before, I would encourage you to have a [look on YouTube](https://www.youtube.com/results?search%5Fquery=bullet+journal) and watch a few videos on [Bullet Journals](https://bulletjournal.com/). You’ll be awash in people showing off how beautiful their illustrations are, but that’s kind of the point. Creating a beautiful space for your mind to wander and feel settled. I can’t draw worth a damn but I keep trying! There’s no judgement in there and I like it when something actually _does_ come together and look nice.

![](https://blog.bigmachine.io/img/2023/02/image-1.png)

For notes, I use [Obsidian](https://obsidian.md/) and I love it. There are loads of apps and also loads of processes for using these apps and I’ll leave that to you to figure out what works best.

I filter the notes I take, writing down only the things that “stick” or resonate in my head. I go back and look through them from time to time, and when they relate it gets magical!

![](https://blog.bigmachine.io/img/2023/02/bip_1592.jpg)

My notes for this post

Anyway: **get the stuff out of your head and on paper** somehow so you can see your progress and _feel_ the reality of it. It really does fill your soul!

## Sharing With Others

If you’ve made it this far in your journey, hopefully you’re finding each step getting easier. The momentum you build in your upward journey reinforces itself, in the same way imposter syndrome reinforces itself through _negative_resonance.

Good news friendo: _it only gets better from here_. If you’ve unleashed your curiosity, removed obstacles courageously AND have documented your journey, **you’re ready to share**!

This might be hard to read if you’re still in the grips of imposter syndrome, but believe me, it’s so, so wonderful!

_Sharing what you’ve learned is a wonderful gift_.

You might think: “who, me? What do I know? I’m no expert on X, why in the world would I tell anyone else about it!” I think these concerns are valid, especially in the hyper-stupid world of stupid stupid social stupidity where everyone shares everything and it can be extraordinarily … stupid.

That’s not what I’m talking about. Consider this quote from my friend [Derek Sivers](https://sive.rs/):

> You should share something you’ve learned right when you learn it so you remember what not knowing it was like.

I so, so love that. Look: **I’m not a psychologist or spiritual guru**. I’m not a consciousness expert or therapist of any kind, yet here I am, sharing a profoundly personal experience with you that changed my life. **I don’t need to be an expert** to share this!

I don't have a degree in Computer Science and I've failed more job interviews than I can count (and succeeded too!) - but that doesn't mean I can't share what I've learned with you. In fact, **I might say it puts me in a unique position because** I've learned what failure looks like - success too.

[![](https://blog.bigmachine.io/img/2023/02/bip_1588.jpg)](https://app.bigmachine.io/courses)

Video courses I've made over the years

That’s the key: **you’ve taken the time to discover and learn something**, fueled by curiosity and courage. You’ve written down what you’ve learned and considered it at length. _You’ve learned something_.

Boom. Share it with others and change someone’s life. How fun is that! The trick, here, is not to tell someone what to do but, instead, share with them how you see the problem, the solutions you considered, the ways in which you failed, and what ultimately worked for you. Invite feedback and discussion - and everyone wins.

Now it goes without saying that **there are ego traps here**. It’s easy to figure something out and have someone refer to you as an expert, which is never fun. Someone once called me an expert on imposter syndrome and I think I screamed something horrible.

People that _do_ fall into this ego trap become _[Expert Beginners](https://daedtech.com/how-developers-stop-learning-rise-of-the-expert-beginner/)_, the poster children of [Dunning-Kruger](https://thedecisionlab.com/biases/dunning-kruger-effect) syndrome. It’s human, I suppose, but do be aware of these traps. Becoming an expert at anything takes a lot of experience, which means a lot of failures, and that doesn’t happen quickly.

Take your time getting to the top of the mountain but, remember, the true joy is in the climb.

## The Final Step: Recognition

So here we are. You’ve reversed that horrible, toxic imposter nonsense and you’ve pulled yourself up, learning amazing things and setting up plans for a brighter career!

That’s the goal, anyway. It does take time, which is where this book comes in.

My goal is to wedge this book in right above the second stage _gratitude_. I can’t stop you from being toxic, you need to do that yourself. I also can’t help you with becoming curious, that’s something only you can do.

**I _can_ fire that curiosity, however, and that’s my plan.**

I want to help push you along the wonderful course your career is about to take. You just need to supply the courage, notes and, hopefully, the sharing. If you do these things, I just have one request: _stop and notice_.

At some point good things will start happening to you professionally, It’s a good practice to recognize these things by writing them down or celebrating with friends and family. It could be as simple as getting a promotion or as dramatic as having the courage to leave that dead-end job.

Maybe you finally understand _P vs. NP_ (that took me a while!) The point is: **recognize and be grateful for how far you’ve come**.

Yes, there’s that word again: _grateful_. You’ll find it’s akin to pure magic when it comes to transforming yourself and your work. What’s even better is that the people around you will feel it too, and everyone benefits.

Right then! Hopefully you feel mentally prepared because WOW do we have some fun things to learn. Get yourself to stage two, friendo, and let the curiosity flow because we’re going to hit the ground running.

Here we go…

![](https://blog.bigmachine.io/img/2023/02/bip_1599.jpg)

Photo by [Gian Luca Pilia](https://unsplash.com/@gnlc?utm%5Fsource=ghost&utm%5Fmedium=referral&utm%5Fcampaign=api-credit) / [Unsplash](https://unsplash.com/?utm%5Fsource=ghost&utm%5Fmedium=referral&utm%5Fcampaign=api-credit)]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https:/images.unsplash.com/photo-1524117074681-31bd4de22ad3" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https:/images.unsplash.com/photo-1524117074681-31bd4de22ad3" />
  </entry>
  <entry>
    <title>Using Constraints to Protect Calendar Data in PostgreSQL</title>
    <link href="https://bigmachine.io/posts/schedules-and-timespans-in-postgresql" rel="alternate" type="text/html"/>
    <updated>2023-01-23T12:48:43.000Z</updated>
    <id>https://bigmachine.io/posts/schedules-and-timespans-in-postgresql</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2023/01/bip_1545.jpg" alt="Using Constraints to Protect Calendar Data in PostgreSQL" /></p>
It's a lovely sunny day here in Northern CA (where I am currently in the world) and I was doodling in my journal about today's events when memories of a fun project came flooding in. 

I (currently) work at Microsoft as a developer advocate and part of our job is to help out where help is needed. I have a lot of experience doing front end "stuff", specifically with Vue and Nuxt (I've shipped quite a few apps), so I was asked to chip in and **lead the engineering effort behind LearnTV**, a video streaming service that Microsoft wanted to use for streaming events, news, and videos from the Channel 9 archives:

![](https://bigmachine.io/img/2023/01/bip_1547.jpg)

I had a ton of fun with this project - mostly because I got to work with PostgreSQL and do some very interesting scheduling problems with it!

## The Scheduler Problem

We wanted to present our users with a "guide" like you see on any TV: a grid of programs divided out by the time of day. These programs included conference talks, live streams and recorded video presentations. The conference videos were almost _always_ 30 to 60 minutes apiece, with some stretching to 120 minutes. The archived video stuff (old Channel 9 videos) were completely random in terms of duration and the live streams would start on time but could end whenever they chose. Putting these things together into a guide was a challenge. 

This is what the old "guide" looked like:

![](https://blog.bigmachine.io/img/2023/01/bip_1546-1.jpg)

Hopefully you see the problem: weird start and end times that weren't very TV-like. To solve this problem, we created programming `slots` and filled them as best we could - sort of like TV guide Tetris. 

A `slot` would be a block of time 30, 60, 90 or 120 minutes in length and in an ideal world you could fit a live event (like a conference talk) in there perfectly. But that was rarely the case so we had plenty of small videos (between 1 and 15 minutes in length) that we could pack in to fill the extra time so we didn't have dead air. We wanted those small videos to be relevant to the talk (as much as possible) so we ended up creating a fun bin-packing problem for ourselves.

Thankfully we had PMs 😹 so we made them deal with the bin-packing thing - but _I_ needed to be sure the data that got entered was as correct as possible. I don't know if you've ever worked with PMs under stress but let's just say that they have this weird ability to circumvent rules and pump crappy data into any system, which usually involves Excel somehow.

Anyway, I couldn't have that, so I decided to flex Postgres, implementing the following rules and _building it right into the table schema:_

* Start and end times had to happen at **the top or the bottom of the hour**, just like TV.
* The **duration of the program needed to be 30, 60, 90 or 120 minutes**
* **No overlaps**! We only had one broadcast so having overlapping times would flip out our streamer (OBS) and the system would crash.

Sure, I could have used code to do this, but that would have left my scheduling table unprotected! No - I needed to be _sure._ Besides: the code for this stuff would have been lengthy and messy, abusing ORMs by using callbacks, voodoo and bourbon.

**Yuck**. I'd rather use Postgres.

For instance: I can ensure that a program starts at the top or bottom of the hour using a very simple `check`:

```sql
create table programs(
  id serial primary key,
  name text not null,
  start_at timestamptz not null 
  	check(date_part('minute', start_at) in (00,30)),
  ends_at timestamptz not null 
  	check(date_part('minute', ends_at) in (00,30))
);
```

Oh but this is only the start - I should be using a range for the time data (a `tstzrange` to be specific) and I should also have `duration` field in there, which also has a `check` on it to be sure it's 30, 60, 90 or 120 minutes. Oh yeah - no overlapping slots! 

This is _not_ a simple problem! Unless we're...

## Using Constraints Like a Boss

**I can do all of these things with a kickass set of constraints**, guaranteeing my data is correct, letting me sleep at night.

That's what we're going to do in this video: freak out with Postgres, flexing built-in data types and some valuable date functions.

Enjoy!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2023/01/bip_1545.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2023/01/bip_1545.jpg" />
  </entry>
  <entry>
    <title>Weird Brazil Date Bug with Jon Skeet</title>
    <link href="https://bigmachine.io/posts/bang-in-brazil-with-jon-skeet" rel="alternate" type="text/html"/>
    <updated>2022-12-31T08:57:53.000Z</updated>
    <id>https://bigmachine.io/posts/bang-in-brazil-with-jon-skeet</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https:/images.unsplash.com/photo-1608370617993-a5c9ee904646" alt="Weird Brazil Date Bug with Jon Skeet" /></p>
Back in 2011 or so I recorded a series of videos with Jon Skeet, dissecting answers to his StackOverflow questions that he found interesting.

Jon starts things off by sleuthing out a rather hairy Date/Timezone bug which, evidently, only happens in Brazil. To make matters even more interesting, the code is written in Objective-C on iOS6!

Jon is _not_ an iOS developer yet he still managed to get credit for this answer! I love how he digs into a passion of his: _dates_.

Here's the original question:

[Why does NSDateFormatter return nil date for these 4 time zones?Try running this in iOS6 (haven’t tested pre iOS6): NSDateFormatter \*julianDayDateFormatter = nil;julianDayDateFormatter = \[\[NSDateFormatter alloc\] init\];\[julianDayDateFormatter setDateFormat:@”...![](https://cdn.sstatic.net/Sites/stackoverflow/Img/apple-touch-icon.png?v=c78bd457575a)Stack Overflowlpa![](https://cdn.sstatic.net/Sites/stackoverflow/Img/apple-touch-icon@2.png?v=73d79a89bded)](https://stackoverflow.com/questions/12922645/why-does-nsdateformatter-return-nil-date-for-these-4-time-zones)

### Other Links

* [Wikipedia Entry on Julian Dates](http://en.wikipedia.org/wiki/Julian%5Fday)
* [Documentation on NSDateFormatter](http://developer.apple.com/library/mac/#documentation/Cocoa/Reference/Foundation/Classes/NSDateFormatter%5FClass/Reference/Reference.html)]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https:/images.unsplash.com/photo-1608370617993-a5c9ee904646" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https:/images.unsplash.com/photo-1608370617993-a5c9ee904646" />
  </entry>
  <entry>
    <title>The First Draft of Anything Is Shit</title>
    <link href="https://bigmachine.io/posts/when-done-isnt-actually-done" rel="alternate" type="text/html"/>
    <updated>2022-12-25T02:54:40.000Z</updated>
    <id>https://bigmachine.io/posts/when-done-isnt-actually-done</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https:/images.unsplash.com/photo-1517917822086-6988b4ca9b31" alt="The First Draft of Anything Is Shit" /></p>
The title of this post is from Ernest Hemingway and it **describes the last 24 hours of my creative life**. I really enjoy writing and making videos, and I also hate it. The process is fulfilling and torturous at the same time - a battle between my desire to share fun things and **my inner perfectionist calling me a hack**.

One of my goals for my holiday break was to wrap up and ship out my next book/video production. I really liked this one! **It's all about Vue, Nuxt (the opinionated Vue framework) and Firebase** and is based entirely on my experience building my publishing site from scratch:

[![](https://bigmachine.io/img/2022/12/bip_1316.jpg)](https://bigmachine.io)

I built this site during my vacation in August, _over a 3 week period_. I don't want to mislead you: **I know Nuxt very, very well** as I've built and shipped about 15 sites with it to date. Some for me, some for others - either way putting my site together was straightforward.

But that site was built using Nuxt 2.x. **Nuxt 3.0 was released a few weeks ago** so naturally that's what I was going to spend this holiday vacation doing: _rolling my site over to Nuxt 3._

This time, however, I was going to document the journey.

## A Simple, One Hour Video, Right?

That's where I started: _how I rolled my Nuxt 2 site to Nuxt 3_. I decided I was going to do everything in a little book and then do a video based on that. I figured it would be 150 or so pages and I could probably knock it out in a week. I'm a very fast writer.

As I started writing, however, I realized that there was a bit more of a story to tell...

![](/2022/12/bip_1317.jpg)

That's my manuscript, which I created with Ulysses. Each chapter is about 1200 words on average and what you see here is roughly 2/3 of the total size of the manuscript. _For reference: 100 pages is about 25,000 to 30,000 words, depending on layout and font size, etc_.

The point is: I kind of got carried away.

## Telling a Real Story That's Not Boring

This is the worst part about writing: where's the line between Hemingway and Neal Stephenson? Hemingway is famous for his brevity:

> To be successful in writing, use short sentences.

Neal Stephenson... not so much. I'm a _huge_ Stephenson fan and will read (actually listen) to anything he writes. Every novel is an adventure buy MY GOD he is merciless in weaving his words. Story threads explode from the page and demand you pay attention!

These people write fiction and I'm trying to write a technical tutorial - yet the same mechanics are at play.   _I need to create a story that moves things along_ so you don't get bored but I also need to make sure there's enough detail in there so you understand how things work.

This is tough to do, and yesterday I was haunted by yet [another Hemingway quote](https://www.goodreads.com/quotes/52073-the-first-draft-of-anything-is-shit) which is so spot on that it had to be the title of this post:

![](/2022/12/bip_1318.jpg)

Spinning this in a more positive way: _you can always improve your first effort at anything_. For this book and video, that's absolutely true.

## There's Always More To Say With Fewer Words

In this first draft I cover 80% of the moving parts of Vue and Nuxt that you would likely need to understand if you're going to build an application. But, as we all know, the details are where the fun is.

For instance: we build an entire site with Nuxt and Tailwind, including a checkout page that looks pretty groovy:

![](/2022/12/bip_1319.jpg)

This checkout page looks pretty good for a demo or tutorial, but it's nothing that I would ever ship in the real world! That, friends, is the problem. I need to keep things as real as I can and **creating a page that I wouldn't ever use is just... _shit_**.

I also show you how I do content management using Nuxt CMS but not how I hook up full text search or blog posts. I hook up Firebase for deployment too, **showing how you can deploy a full server-backed Nuxt application using a single Firebase function** \- which is crazy by the way - but I don't show you how to wire up Stripe to your API.

**I feel bad about that**. In fact I woke up at 3:22 am the other night and simply said "shit, I can't send this out like this".

There is so much already there, which is also a problem. I have to manage myself very carefully as I tend to find little rabbit holes everywhere with a need to fall into them. This is where careful editing comes in: _staying on track, omitting needless words (and code), delivering a coherent story_.

There's a solid story here and it needs care in the telling, so I'm going to give it the time and care it needs.

## The Next Steps

You can't have an ecommerce site without some type of gateway, so **I'll show you how to wire up Stripe** _and_ the strategies I use for storing that data in Firestore, the "next-gen" Firebase database. But once a charge goes through, then what?

You have to give the people what they bought, so **I'm also going to show you how to fulfill digital goodies** with a reactive function and expiring URLs with Firebase Storage. But even then we're not done...

My #1 support item, by a massive margin, comes from people losing their downloads and wanting access to them again. How do we do that? There are a few ways, and I'll show them all to you:

* Create a serverless function that sends an email with a link based on a form input.
* Create an email auto responder that does the same thing, triggered by an email receipt.
* Allow customers to login, easily, to your application using Firebase Authentication and then find their orders for them.

Just writing these things out is freaking me out. _That's a lot of work_. But it just wouldn't make sense otherwise, would it? So I'm going to divert my freaky energy into _just doing it_ and hopefully it's helpful to you.

## Great... So When, Then?

I'm back to writing now and will continue until I'm done. I'm pretty rigorous with my writing schedule (weekends and evenings and whenever I can fit it in) so ... hopefully I'll be through this in a month or two. I'll be making the videos at the same time.

You might be wondering why I chose to do all of this in the first place? The main reason is described briefly above: _it's a real thing I did last summer_. I think that if you build something interesting you should show people how you did it, rather than fabricating some nonsense for a Udemy video.

Oh, also: Nuxt is a lot of fun and it's changed the way I think about creating web sites! Fun is a good thing, don't you think?

Right - back to work with me!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https:/images.unsplash.com/photo-1517917822086-6988b4ca9b31" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https:/images.unsplash.com/photo-1517917822086-6988b4ca9b31" />
  </entry>
  <entry>
    <title>The Importance of Knowing Your Numbers</title>
    <link href="https://bigmachine.io/posts/the-importance-of-knowing-your-numbers" rel="alternate" type="text/html"/>
    <updated>2022-12-20T04:11:50.000Z</updated>
    <id>https://bigmachine.io/posts/the-importance-of-knowing-your-numbers</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2022/12/bip_1310.jpg" alt="The Importance of Knowing Your Numbers" /></p>
I'm on vacation for the next few weeks and one thing I have a hard time doing is sitting still. Yes, of course there's family stuff to do but my kids are older and I'm on my own these days, so I find that I have a lot more time to myself. 

This, to me, is a wonderful thing. I have a list a mile long of things I want to get done "when I have the time" and that time is _now._ But... what things, exactly, should I be digging into?

## The Importance of Focus with a Side Hustle

[I sell books and make videos on the side](https://bigmachine.io) and I've done so for years. Much of that time I was on my own, living happily off the income but for the last 4 years or so I went back to full time work. I enjoy being able to do both, but what it also means is that **I need to be rigorous when it comes to deciding what to focus on.**

That means _numbers_, lots of them. Sure I could just wing it and follow my passion, but when you actually depend on your side hustle to provide for you, winging it isn't your best option.

Anyway: I learned something over the last 10 days or so of vacation: **if your numbers aren't there, you're lost**.

Let me explain.

## When Your Email Service Is Failing You

I have two email services: the first is this here blog which uses Ghost to manage newsletter-y stuff. It has some pretty slick analytics for everything and I know who's here doing what and when. It's good and I love it.

My other "list" is with ConvertKit and has been for years. If you don't know: ConvertKit is a "marketing automation platform" which basically amounts to them sending emails to people based on certain events. It's taken me years to figure out how to do this without being massively obnoxious and I like it but here's the question: _how do I know I'm not being massively obnoxious?_

Moreover: how do I know if the monthly price tag of this service is worth it?

I pay close to $200/mo for the service and I _think_ it's worth it... which is a problem as I don't _know_ if it is. ConvertKit is famously light on reporting and analytics, but it's also incredibly easy to use, which is why I decided to use the service in the first place. Unfortunately, when I dug in this last week to try and figure out where to spend my time... I was left completely in the cold.

To be clear: they _do_ provide _some_ measurements for you and give you basic indicators of how well you're doing there... but that's about it. If you have deeper questions, such as "which people are buying things and how often" you'll need to find a way to track that on your own. I can see ConvertKit's reasoning here: _they're not an analytical platform_. At the same time, they hold all the interactivity data and if I want to tap into that I'll need to get a PhD in Zapier and pay even more money to find out if ConvertKit is worth the effort.

So, what did I do then?

## Back to Drip

I used [Drip](https://www.drip.com/) way back when it was shiny and new. I had been using Mailchimp forever but they didn't have automations which were becoming the rage - and Drip did. Plus they have a tight integration with Shopify, so I jumped in.

One thing that I utterly _love_ about Drip is that **everything you do is trackable**. Every email you send shows you who clicked, who left, and who read, which are incredibly important numbers. ConvertKit does this too, of course, but they don't tie it back to _revenue_, which is the biggest indicator of all.

**The trick to email marketing is to be your authentic self**, give people good things and then give some more, and then let them know about other things you do which could hopefully benefit them. **It's a fine line between being a cheeseball sales person vs. sharing something you're truly proud of** that could really help someone!

So here's the question: _how do you know if you're being cheesy vs. helpful_? It's all in the numbers. Let's dig deeper.

## A Real Example

Every year during the holidays I do _something_ sales-y_._ I have to - it's the biggest sales time of the year. I used to think "I don't want to fall into this corporate capitalist consumer money scam" but then I actually had people email me - a lot! - asking if I did gift stuff or discounts of any kind.

So in 2012 I decided to give it a go and had a _six-figure sales month_. People were very happy with the discounts I was offering and also very happy that I let them know about them. We all win!

Anyway: every year I do something so this year **I decided to do gift cards for the books and video courses** I sell:

[![](https://bigmachine.io/img/2022/12/bip_1308.jpg)](https://shop.bigmachine.io/products/gift-card)

Gift cards make things very easy

Why did I decide on gift cards? Because I had the sales numbers, thank \[Diety\]! I keep everything nice and tidy in Postgres so I was able to see what kind of gifty things people like to buy for each other, and gift cards won out by about 30%.

Last year I tried something a bit different that wasn't exactly a failure, but it required work on the giver's part. You could buy something for someone but to do so you had to tell me their email and when to send it. I never used their email, of course, but I _think_ people were hesitant to share someone else's email like that. 

So this year it's back to gift cards!

## Numbers, Numbers, Numbers

Analytics are one of those things that people don't consider until they need them and, unfortunately, it's often too late. On one hand I can shrug because it's just me and my side hustle and I probably won't lose my job. On the other hand, if I can optimize what I do based on solid numbers then I can save myself some valuable time!

I won't say I made a mistake by staying with ConvertKit, their service is wonderful and does exactly what they say it will. At the same time I _will_ say that I could have made better choices for myself if I would have stayed with Drip.

**It's critical to think about the question you'll be asking in 1, 3 and 5 years** down the line. What will you need to know to answer those questions? You can't know for sure, but you _can_ use tools that will think of this stuff for you!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2022/12/bip_1310.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2022/12/bip_1310.jpg" />
  </entry>
  <entry>
    <title>Data Structures and Algorithms</title>
    <link href="https://bigmachine.io/posts/data-structures-algos" rel="alternate" type="text/html"/>
    <updated>2022-12-15T07:07:39.000Z</updated>
    <id>https://bigmachine.io/posts/data-structures-algos</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2022/12/data-algos-2.jpg" alt="Data Structures and Algorithms" /></p>
🙌🏼 Thanks for signing up to the blog!

Here are 9 videos (out of 34) from the [_Computer Science Fundamentals_](//products/imposter-video/) course that you can find here on my blog. I made these videos to complement the book, and I also added a few things that weren't in the book... but that made me kind of happy.

Here's a tip: pay good attention to tree traversal stuff! I've interviewed at Google 3 times over my career, and the first two featured two tree questions apiece.

Enjoy!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2022/12/data-algos-2.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2022/12/data-algos-2.jpg" />
  </entry>
  <entry>
    <title>Handling Dates and Times Properly in Postgres</title>
    <link href="https://bigmachine.io/posts/handling-dates-and-times-properly-in-postgres" rel="alternate" type="text/html"/>
    <updated>2022-12-06T01:55:11.000Z</updated>
    <id>https://bigmachine.io/posts/handling-dates-and-times-properly-in-postgres</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2022/12/08-title.jpg" alt="Handling Dates and Times Properly in Postgres" /></p>
We've all had to deal with storing date information in a database, and I'm sure most of us follow the guidance of "**just use UTC**" - which I think is great advice for the most part. **Until it isn't**.

But when would it _not_ be the right thing to do? This, unfortunately, is something you only figure out when your ass is on fire... let me explain.

## Phantom Sales

When I ran Tekpub.com back in 2012, we had been in business for 4 years and our customer base was starting to take off. This, of course, was exciting so, for the holiday season, I put on a big Holiday Sale at 50% off.

I did _not_ expect the response! **We had a _six-figure sales month_**. This blew my mind completely! I was excited for our business, of course, but also very happy for the authors earning royalties from us - they were in for a nice bump.

Anyway, it was great fun until I got a call from my accountant when he was **trying to reconcile my books with my bank statements**. We were off by a significant amount.

A week later I was able to unsnarl my books, and I learned a valuable lesson when it comes to storing dates and times.

## When UTC Strikes Back

Here's the thing: **storing dates as UTC only works if you know how to pull those dates back out properly**, and have them mean something. The platform I was using at the time stored date values as `timestamp` in Postgres, which is a UTC time stamp without a time zone designation. Sounds good, right?

Unfortunately, reading those values back out causes all kinds of pain unless, of course, your business is located in England near GMT. Consider the following:

* You make a sale on December 31, 2012 at 23:59 PST. It goes into your `timestamp` date field as such.
* You query that value later on and see the value as you entered it. Your accountant reminds you, however, that your business is incorporated on the east coast of the US, so all sales must be relative to that. Meaning that your sale actually happened at January 1st, 2013 EST and belongs in next year's books.
* "No problem", you think, because you remember that you can add `at time zone 'America/New_York'` to your query to cast your date to east coast time.
* You start to cry when you see the query result is December 31, 2012 18:59\. Why... is that happening?

The answer to this problem is that Postgres (and I'm sure other platforms too) is very literal when it comes to storing dates at UTC using `timestamp`. Postgres thinks it literally happened at that time, GMT. When you ask for a conversion to EST from GMT, it will give it to you.

In our case, the date stored is _incorrect_ and off by 8 or so hours. Ouch. This would throw off every sales report, which might be a small amount and not matter so much in sales meetings - but it matters to book keepers and accountants!

## Storing Time Zone Information

We could, of course, store dates using `timestamptz`, which is a time stamp with a time zone offset. If I had done that, I would have been able to read the dates back out with the correct time stamp.

**Maybe**.

See **this only works if you know what time zone your server is in**. Do you? When working locally and using local Postgres, your server's time zone is the same as your local machine, which is wherever you're located in the world.

If your database is in the cloud, however, that's not the case, and it's likely your server is set to UTC, _even if your data center is located at us-west, us-east, asia, or whatever_. This is true for AWS, GCP, Azure and Digital Ocean. Might be worth confirming yours...

The worst part, however, is that you only realize the mess you're in when it comes time to generate reports, specifically sales reports, and the sales differ from your bank.

So, what should you do then? _Good question_. The first step is to be aware of the problem. The second step is to store the dates with the right offset for your business.

That's the subject of this week's video - dates and date handling in Postgres. Enjoy!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2022/12/08-title.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2022/12/08-title.jpg" />
  </entry>
  <entry>
    <title>Striking Gold in My Archives</title>
    <link href="https://bigmachine.io/posts/striking-gold" rel="alternate" type="text/html"/>
    <updated>2022-11-30T03:00:09.000Z</updated>
    <id>https://bigmachine.io/posts/striking-gold</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2022/11/bip_1256.jpg" alt="Striking Gold in My Archives" /></p>
I've had a Vimeo account since 2007, when I moved all of my video streaming "stuff" for my old business, Tekpub. I was paying about $1100/month to AWS, using their media streaming services and a crappy HTML 5 video player at the time... and then I stumbled onto Vimeo.

**Update:** _I sent this post out to the mailing list and said "it's free for members" but I failed to mention that you have to login to see it. No charge - you're already a member here if you got my email._

I couldn't believe it.

For $250/year I could host my videos securely, with a branded player, completely hidden from the Vimeo service. I could also control the access rights, disallowing embeds anywhere but specified domains. This saved me so much time and money... I was beside myself!

I still use them to this day. One of the rare services that hasn't let me down over the years. Stripe, Vimeo and Firebase - services I've been with seemingly forever.

## Browsing My Archives

It's fun to look through my video archives up there. Over the holidays last week I decided to do some organization and **I found a set of videos I did with Rob Sullivan _years_ ago** \- back in 2011\. I thought it would be a fun bit of nostalgia, to look back at our **inital fumblings with PostgreSQL**, but after a few minutes I realized that there is gold in there.

**99% of the content is still relevant**! Shocking! Given that, [I published it here](//video/hello-pg/) for all of you - no subscription required (but you still need to login!) 

I guess that's how good Postgres is. Widely hailed as **the pinnacle of open source**, the PostgreSQL project sets the standard for every other project out there. A dedicated team that just keeps delivering an amazing product.

[You can watch the video here](%5F%5FGHOST%5FURL%5F%5F/video/hello-pg/) \- I hope you enjoy!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2022/11/bip_1256.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2022/11/bip_1256.jpg" />
  </entry>
  <entry>
    <title>Fun Geeky Things To Do For the Holidays</title>
    <link href="https://bigmachine.io/posts/fun-things-to-do-for-the-holidays" rel="alternate" type="text/html"/>
    <updated>2022-11-25T06:18:52.000Z</updated>
    <id>https://bigmachine.io/posts/fun-things-to-do-for-the-holidays</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https:/images.unsplash.com/photo-1502519144081-acca18599776" alt="Fun Geeky Things To Do For the Holidays" /></p>
Hope you're having a great holiday season and if you celebrate Thanksgiving, which is today, I hope you're 1) having a wonderful time with friends and family and 2) **have something queued up to occupy your time**!

I'm having a great day - it's the first Thanksgiving I've spent on my own and **I love it**. I know that many people might think this would be a sad thing, but far from it. You can really focus on the things you're grateful for! _If you're curious: almost everyone I know came down with COVID so all my plans went out the window... which is OK because..._

## There's Nothing More Fun Than Coding During the Holidays!

Even if you work for yourself, which I've done for many years, there's something about **taking a break during the holidays** and focusing on _fun, nerdy stuff_. For instance: this site uses Ghost and I've always wanted to find a way to **turn Ghost into something that could host a video course**. Turns out, it's not that easy!

My first solution was to use Vimeo's "showcases", which is a bit of a janky JavaScript layer on top of their player. It works, but I don't care for it.

I then had the fun idea, since I'm polishing my book/videos on Vue 3, to **pop Vue into a custom template with Ghost**! It's just a page that I can dedicate, can't I?

## Experimenting With Vue and Ghost

It's... unfortunately not that easy, for a variety of reasons. The first is that Ghost's pages are written using Handlebars, which display templated content with the same delimiter as Vue: `{{stuff}}`.

You cat get around this in Vue by telling it to use something else:

```js
  Vue.createApp({
    delimiters: ["[[", "]]"],
    data(){
      return {
        videos: Vue.reactive([]),
        thisVideo: Vue.reactive(null),
        player: null
      }
    },
```

Here, I'm telling Vue to use square braces instead. This works pretty well, but then comes the next hurdle: **how do I know which videos to pull**? The custom template is just that - how's it going to know which video production I want to show?

The short answer is that I can use the slug of the URL, which means I have to pop that into JavaScript:

```html
<script>
  const slug="{{slug}}";
</script>
```

I've done worse. This will plant the slug serverside in a page variable so I can use it to query an API where my videos live. Fun times.

I have to use a bunch of `script` tags to pull in the libraries I want, which means I need to use Vue's `mounted` lifecycle hook to be sure the DOM is loaded with my script tags. 

Once all that's done, it's off to the races. I think it looks kind of nice, don't you?

![](https://bigmachine.io/img/2022/11/bip_1235.jpg)

Currently, **I'm doing this for member-only stuff**, but if you want to see a list of the courses I [have you can have a look here.](%5F%5FGHOST%5FURL%5F%5F/tag/courses)

A fun way to spend a few hours on Thanksgiving morning - and I hope it improves the experience for everyone!

## This Site vs. Big Machine

I decided to loosen things up a bit and give myself a little "playground" where I can make all kinds of content. If you've bought stuff from me before, you might be wondering if anything is going to change - and no, it's certainly not. All the videos you see here are available to subscribers (paid), or you can still buy them individually on [Big Machine](https://bigmachine.io) if you want.

## And Finally: Here's Your Black Friday Discount

You didn't think I'd leave you hanging, did you? [I'm offering a 50% discount this holiday season](%5F%5FGHOST%5FURL%5F%5F/holiday) to this here site, which has every video I've made in the last few years - and I'm going to keep adding more.

As I mention, I'm wrapping up a 2 hour video "thing" with Vue/Nuxt 3.0 which I hope to get out the door in a few weeks. I also have plans for other fun things, like Firebase, Go and more.

I also write long-form premium posts for members too. Things that don't quite fit into a video. So sign up already!

[Sign Up for 50% Off](%5F%5FGHOST%5FURL%5F%5F/holiday)

Oh - and happy holidays!

**Rob**]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https:/images.unsplash.com/photo-1502519144081-acca18599776" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https:/images.unsplash.com/photo-1502519144081-acca18599776" />
  </entry>
  <entry>
    <title>The Easiest Postgres Experience You&apos;ve Ever Seen</title>
    <link href="https://bigmachine.io/posts/the-easiest-postgres-experience-youve-ever-seen" rel="alternate" type="text/html"/>
    <updated>2022-11-22T04:38:03.000Z</updated>
    <id>https://bigmachine.io/posts/the-easiest-postgres-experience-youve-ever-seen</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https:/images.unsplash.com/photo-1475869568365-7b6051b1e030" alt="The Easiest Postgres Experience You&apos;ve Ever Seen" /></p>
A few months ago my friend [**Craig Kerstiens**](https://twitter.com/craigkerstiens) decided to see what's possible with Web Assembly, the thing that runs Code That's Not JavaScript in the browser. If you're a Microsoft dev, you might have heard of Blazor, which compiles C# code to Web Assembly which then gets handed to the browser to run in-process.

It turns out that people are taking Web Assembly pretty seriously, including Adobe, which [compiled PhotoShop and Acrobat to Web Assembly](https://web.dev/ps-on-the-web/) so you could run it in the browser!

![](https://bigmachine.io/img/2022/11/image-8.png)

Image from https://web.dev/ps-on-the-web/

Love the image of the elephant... which reminds me... this post is mostly about Postgres, so let's get back to Craig's story...

## Using Web Assembly Entirely the Wrong Way

Craig cofounded a company named [Crunchy Data](https://www.crunchydata.com/) which is focused on proper running and hosting of PostgreSQL databases in the cloud. Or, as I like to think of it: _Postgres done right_.

Anyway, the story goes like this: **Joey Mezzacappa**, a Crunchy engineer, was reading a blog post about "[runnable Markdown examples](https://wasmer.io/posts/markdown-playgrounds-powered-by-wasm)" from a company named _Wasmer_. The example that caught his eye had to do with a SQL query running in SQLite:

> There was one in particular that _really_ got my attention: [SQLite](https://wapm.io/sqlite/sqlite). On that page, there is a fenced code block with some SQL queries inside. If you click the "Run in Playground" button, it runs the query right there in the web browser with SQLite compiled to WebAssembly.

That's impressive. Being a Postgres person, his next thought was completely natural:

> After running that SQLite query in my browser, I thought, "Can I do this with Postgres?"

It turns out that no, you can't run PostgreSQL like that in the browser. SQLite is unique in the database world as it's a little C binary that creates an _embedded_ database (meaning "just a file on disk"). It doesn't need to access internal "system stuff" like Postgres does, so compiling it to Web Assembly was a straightforward proposition.

That's how Web Assembly works: _it's just a compilation target_ in the same way you might target Windows, ARM processors, 32-bit Linux and more. If you compile something to Web Assembly, you can run it in a browser.

## What If I Just...

That's where Joey decided compilation rules didn't apply to him, nor to Postgres:

> ... the modern web browser is a very powerful platform. Let's just change the target platform to something other than WebAssembly, then run it in WebAssembly anyway like a rebel. 😎... It's actually possible to emulate a PC _inside the web browser_! There have been quite a few implementations over the years... I ended up choosing v86 for this... 

Long story short: Joey setup an Alpine Linux virtual machine running Postgres and shoved it into a VM emulator using Web Assembly, **[and it worked](https://wasmer.io/posts/markdown-playgrounds-powered-by-wasm)**.

![](/2022/11/image-4.png)

If you click the link above to the [Postgres Playground](https://wasmer.io/posts/markdown-playgrounds-powered-by-wasm)'s `psql` tutorial, you'll see this exact page along with some commands to help you get to know the powerful Postgres binary.

That's Postgres running in your browser. Not on the cloud somewhere: **_right in your browser_ in a VM emulator**.

I think that's fascinating. But it doesn't stop there.

## Craig's Easter Egg

One of the things you'll notice in the output above is that it's pulling in a SQL file for the tutorials, which is a single table named `weather`:

![](/2022/11/image-5.png)

This is fine, of course, if you want to learn `psql` and just need a simple dataset. But the thrill of the moment is indeed _momentary_ as working with data you don't know and don't necessarily care about can provide an undewhelming experience. Craig knew this so decided to **give people the ability to load a SQL file of their own**.

That seems like a very, very bad idea doesn't it? **Talk about a massive security risk**! But then again: this server is running in your browser. You close the tab or navigate away and it's gone! So why not?

It just so happened that, at the very same time as Craig was announcing this on Twitter, I was creating my free ebook: _[The Little SQL Book](https://bigmachine.io/little-sql-book/)._ I wrote this book because I know a lot of people who want to learn SQL but yawn right out of it, so I went in search of a data set that many people could relate to. 

I chose Fantasy Football (American Football, that is). Many leagues were going through their drafts and so I decided to share what I had done in the past with Postgres, sifting/querying past league data looking for trends. I know football isn't everyone's jam but hey, you gotta start somewhere don't ya?

Anyway: Craig asked me for my data set [and next thing I know](https://www.crunchydata.com/developers/playground?sql=https://gist.githubusercontent.com/craigkerstiens/2297d5fce53832a73c975e94e6a7f0c8/raw/7d858bdb9ecd8bd1445425fa948197b655804e31/ff.sql)...

![](/2022/11/image-6.png)

The PSQL Playground for The Little SQL Book

![](/2022/11/image-7.png)

**Craig had added the ability to append a `sql` querystring key which points to a public SQL file**. That public SQL file is actually a [GitHub gist](https://gist.githubusercontent.com/craigkerstiens/2297d5fce53832a73c975e94e6a7f0c8/raw/7d858bdb9ecd8bd1445425fa948197b655804e31/ff.sql) which has all the SQL my readers need to get started.

Think on that! This is a book about learning SQL and Postgres in particular. Normally there would be an installation/setup/yak-shaving phase before we get started but not with this option! You just click a link and you're off and running in the browser.

I think that's one of the neatest things I've ever seen! Hats off to Craig and Joey for putting this all together.

## Web Assembly is Coming...

We work in an industry that is in a constant state of hype and disruption so distrust is natural. I first heard of Web Assembly years ago when I saw [Steve Sanderson's first demo of Blazor at NDC Oslo](https://www.youtube.com/watch?v=uW-Kk7Qpv5U). I thought it was interesting and I also love the idea that maybe, someday, we could get away from using JavaScript to create browser applications.

That last bit is happening with the rise of TypeScript, but I'm not too sure about the first bit. I'll be honest and say that using something like C#, Go, or Rust (the current "main" players in the WASM world) on the browser feels like overkill to me. I like the idea of a "scripting" approach because it keeps things simple... but then again frontend applications aren't growing simpler, are they?

So I'm sitting on the fence, waiting to see what happens in the frontend space. The _backend space_, however, is grabbing my attention.

Web Assembly is making a similar migration as JavaScript did with Node: it's moving from the browser to the server. This is fascinating to me for a variety of reasons, primarily because Web Assembly is _fast_ and it's also very lightweight. Unlike Docker, **Web Assembly can be run as a binary**. This means that **you don't need to have a base installation of Linux just to have your service run - it can be executed like any other binary application** on your system.

Think of your "services" as pure code living in subdirectories of a main project. Your root directory might have some kind of manifest and your individual services could be written in any language that can be compiled to WASM. Once compiled, you would have a set of binaries that could be run anywhere WASM could be run.

Web Assembly in the browser is completely sandboxed so writing full service-based applications is still something that's being worked out, which is where [WASI](https://hacks.mozilla.org/2019/03/standardizing-wasi-a-webassembly-system-interface/) comes in:

> WebAssembly is an assembly language for a conceptual machine, not a physical one. This is why it can be run across a variety of different machine architectures... Just as WebAssembly is an assembly language for a conceptual machine, WebAssembly needs a system interface for a conceptual operating system, not any single operating system. This way, it can be run across all different OSs... This is what WASI is — a system interface for the WebAssembly platform.

I think this is exciting!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https:/images.unsplash.com/photo-1475869568365-7b6051b1e030" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https:/images.unsplash.com/photo-1475869568365-7b6051b1e030" />
  </entry>
  <entry>
    <title>Five Things I Learned Building Bigmachine.io Using Nuxt and Firebase</title>
    <link href="https://bigmachine.io/posts/how-i-built-bigmachine-io" rel="alternate" type="text/html"/>
    <updated>2022-11-09T01:37:15.000Z</updated>
    <id>https://bigmachine.io/posts/how-i-built-bigmachine-io</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2022/11/bigmachine.jpg" alt="Five Things I Learned Building Bigmachine.io Using Nuxt and Firebase" /></p>
I have the habit of rebuilding [my publishing site](https://bigmachine.io) every six months, and it's annoying. **There's always _something_ that goes wrong** and requires a "drop everything and fix this" moment. I've bounced between Shopify, WordPress, SendOwl, Jekyll, Sinatra, and even Ghost at one point!

There are a lot of reasons for the changes, but it mostly comes down to flexibility and service charges. Sometimes complete service failures too.

Anyway: 5 months ago I decided to redo it again, but this time I would take my time, _do it just the way I wanted_. **I decided to use Nuxt 2.0 and Firebase** as I had been a Firebase customer since 2013 and I know and love the service and all of my data is over there too.

I learned a lot along the way, and I thought I would share it with you. Oh, before I forget, **I'm writing a book and hopefully going to create a video thing** based on what I learned - I'll post about it here in the next few months.

## Thing One: I Should Have Used Nuxt 3

I was trying to decide between using Nuxt 2 and Nuxt 3, which wasn't released but was getting very, very close. I figured things would be changing a lot and I knew Nuxt 2... so I just did that.

The reason I should have used Nuxt 3 is that it uses Vue 3 under the hood, which has a devotion to _terseness_. For instance, this code here is old school Vue 2:

![](https://bigmachine.io/img/2022/11/bip_931.jpg)

Here's Vue 3 :

![](/2022/11/bip_1137.jpg)

**Note**: _I don't have the `app` declaration in the Vue 3 sample, but hopefully you get the idea_.

Vue 3 is all about **composition vs. declaration**. In Vue 2, you created an object and sent that declaration to Vue, which would know how to use it. With Vue 3 you import the functions you need to use and that's it.

The real magic, however, comes from `script setup`, which will automatically "lift" your variables to your template, whether they're simple values or full functions. You don't have to use `script setup`, but I love it.

## Thing 2: Pinia is Delightfully Simple

I really should have used Nuxt 3, and as of a week or so ago: _I am!_ I'm rolling my site over and it's been a fun process - especially using Pinia.

Every multi-page app needs a state store of some kind and, if I'm honest, using Vuex (the Vue 2 state store) was... _wonky_. I know they were following the Redux pattern blah blah blah but it just seemed so... _ceremonial_.

Pinia is delighfully terse:

![](/2022/11/bip_1138.jpg)

That's my `sales` store, which is responsible for loading things from Nuxt Content. I'm using `ref`, which is a built-in function which allows you to compose reactive data when you need it. _I love this_. State was automatically reactive in Vuex (aka "two-way binding"), here I can dictate which things are and which are not.

When you `defineStore`, the functions and values you return are what the store "does". Each function is turned into an action, each value is loaded into the state. Clean!

## Thing 3: Nuxt Content is Radical

As I mention: I've moved sites a lot over the years and one of the major headaches I had was dealing with the damned content! Copy, paste, copy, paste. I mean... yeah I know I should use a headless CMS like Contentful or Prismic, but that just added a load of extra concepts I didn't want to think about.

And then I read about [Nuxt Content](https:/.nuxtjs.org/). I swear, do these people ever sleep? The idea is simple: you can put files in your `/content` directory and then query for them from your Vue templates. These files can be **Markdown, JSON, YAML or CSV.** 

The Markdown files are especially interesting because you can embed Vue components right into them, which is wild. I don't find that useful, however. What I do find useful is chunking out the content for my pages into multiple files:

![](/2022/11/bip_1139.jpg)

Sales pages have different elements that follow a general pattern (headline, agitation, problem, solution, CTA, etc) and being able to divide these out and query for them all at once was joyous!

The Markdown files also use frontmatter just like Jekyll or Hugo, which makes these extremely portable.

## Thing 4: The Headless Component

I actually prefer another name: _the logical component_, but "headless" works too. These are components that don't have any UI to them (or "view" in the Vue world) and they exist only to handle state or react to events.

The one I created was my `UserAuthentication` component, which tells me if a user has logged in using Firebase Authentication:

![](/2022/11/bip_1097.jpg)

That's a lot of code there, but the idea is that you can **use "slots" in a Vue** component and give them a name. You can also display them conditionally. Here, I have two slots which show things if a `user` is logged in or logged out. That's determined by a Firebase authentication event that fires in the component code.

In addition, **this component fires an event** whenever a user logs in or out - something I can wire up on the page where the component lives:

![](/2022/11/bip_1140.jpg)

Here, I'm wiring `v-on:logged:in` (my login event fired from my component) to `loadVideo`, which goes to Firebase and requests a video record based on the logged in user.

There are two templates here that correspond to the slots in my Vue component and they are displayed conditionally. The first one is given the logged in `user` object, the other just displays a prompt telling the user to login.

I use this component _all over the site_. It's extremely versatile. Here I'm using it to show different navigation elements:

![](/2022/11/image-1.png)

I love this!

## Thing 5: You Can Run Universal Mode on Firebase

This one blew my mind! I'm running my current site as a static web app backed by 8 or so Firebase functions (serverless). This works pretty well, but Nuxt likes to have a server behind it so it can do server-side rendering (SSR)... but how do you do that if you don't have a server?

It turns out you can wire up Nuxt 3's server (Nitro) to a Firebase function and you're good to go!

This is my deployment script:

![](/2022/11/image-2.png)

This is my `firebase.json` file, which I'm using for a test site currently:

![](/2022/11/image-3.png)

These two things together will create a function called `server` at Firebase, which exists to serve my site. That means I can create a full server API in my Nuxt app and run the whole thing virtually free on Firebase.

Wild!

## Interested? Let Me Know!

I'm writing up a full walkthrough of everything I did and I'm getting really close to being done... with the book part at least. I'm also creating a video that I'll be hosting [on my blog](%5F%5FGHOST%5FURL%5F%5F/) for my subs.

If you want to see something in particular, I'd love to know. Feel free to drop me an email (rob@bigmachine.io) or [ping me on Mastodon](https://mastodon.social/@robconery).

Hope this was helpful!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2022/11/bigmachine.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2022/11/bigmachine.jpg" />
  </entry>
  <entry>
    <title>Developing A Potent Voice</title>
    <link href="https://bigmachine.io/posts/developing-your-voice" rel="alternate" type="text/html"/>
    <updated>2022-11-03T03:42:53.000Z</updated>
    <id>https://bigmachine.io/posts/developing-your-voice</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https:/images.unsplash.com/photo-1483489571097-0ca295ba3d08" alt="Developing A Potent Voice" /></p>
One thing that gets me into trouble _constantly_ is when I write something _challenging_. The topic doesn't matter, but if there are feathers to be ruffled I tend to think "well... here comes a breeze friendo".

**Writing, to me, is all about _truth_**_._ Not "The Truth", but _my truth_. Things I believe and feel strongly about - things that I think are worth saying. So, if it's true to me and I want to say it, that's exactly what I'm going to do. Well... sort of. There are rules here.

One one hand there's what [Stephen King](https://www.amazon.com/Writing-10th-Anniversary-Memoir-Craft/dp/1439156816) suggests:

> If you expect to succeed as a writer, rudeness should be the second-to-least of your concerns. The least of all should be polite society and what it expects. If you intend to write as truthfully as you can, your days as a member of polite society are numbered, anyway.

On the other hand, my truth and your truth? It's pretty unlikely they'll align. That's OK! This is why we share ideas and write posts like this one. But I do need to offer you _some_ respect as a reader, don't I? **It's easy to be loud and obnoxious, creating incendiary click-bait titles full of noise**, amounting to nothing. If it wasn't, people wouldn't do it so much.

I hate that. I'm sure you do too... but **what are these rules**? How can you develop a voice that's true and welcoming at the same time? I suppose the answer is that you can't, but **you _can_ avoid a trainwreck** and make your best effort by observing your internal processes and needs, and knowing when they defeat you.

## Writing How You Speak

I have a newsletter that I send out from time to time and in one of my posts I discuss ways that I've found helpful for making solid, helpful videos, like the ones you see on this site. 

One of my suggestions was to **write yourself a script**:

> **People that don't know what to say will usually copy what other people say so they don't have to think for themselves**. It's hard to listen to.

> Take the time to think through what you're going to talk about, and then write yourself a little script. Work your demo at the same time, and write down the words you hear yourself saying. You would be surprised how tight your presentation gets, and how often you catch yourself saying nonsensical things!

One of my readers, Aaron, responded:

> I find I grapple with "writing how I speak" because I tend to get in the weeds... If I could master that, I'd be better at writing for things I intend to be spoken, vs read, and probably write a lot more, too. 

I get it. I really do! It's one thing to tell someone the famous writer's nudge: "**whatever you're trying to say... _just say it_**" and another to offer some tangible advice. **Our words go through a lot of filters** as they make their way from our brains to our mouths (or fingers) and, to me, the trick is to realize which of these filters is doing a little too much work.

## Knowing Your Filters

I think of The Writer's Voice as that thing deep inside you that has something to say. Formed by your dreams, ideas, inspirations, and frustrations: it wants to speak out!

Thankfully, humans have developed filters over time because speaking too freely tends to be a problem.

![](https://bigmachine.io/img/2022/11/napoleon-dynamite.gif)

Some of us have more filters than others, which finally brings me to the point of this whole post: **figure out which filters you need to throttle.**

Each of these filters serves a basic need, which we have to discuss if we're going to tweak them.

## Acceptance

The need for acceptance is the dominant need of anyone in a group. I don't need to explain this to you, you know it already. But what of this need when it comes to filtering our voice?

**This, friend, is where the good stuff lives** and understanding how you stop yourself from saying what you need to say, based on this need, is critical. There are so, so many factors at work but know that _it really is just one, single filter_ at work here and if you can adjust it ever soooo slightly, you'll find that your writing becomes massively more effective.

Easy to say, right? _Just care a little less about being canceled..._ Unfortunately, that's exactly what it amounts to.

In 2009 I wrote a post suggesting that ASP.NET developers learn ASP.NET MVC, which was a new framework back then. I worked on the ASP team and we weren't supposed to be offering "prescriptive guidance", yet I felt that was disengenuous. I wouldn't say we were outright lying to people - but I did think we should have done more to help people understand the benefits of the new framework.

So [I wrote a post](https://web.archive.org/web/20091211175324/http://blog.wekeroad.com/blog/i-spose-ill-just-say-it-you-should-learn-mvc) about it. This is from the Internet Archive as I've taken the post down. There was... some heat involved:

![](/2022/11/bip_1076.jpg)

Here's the thing: _I knew this post was going to blow up_. **I wrote it anyway**. It's filled with direct thoughts and what I consider a hefty dose of "my truth". The first two sentences of this paragraph are examples of this kind of writing: direct, no fluff... just the facts as I see them.

When I wrote this post I did it with extreme care, believe it or not. You might think that when I say "extreme care" I'm talking about trying not to hurt people's feelings but no, not at all. The care was this:

* Write a paragraph.
* Reread the paragraph and remove half the words.
* Deep breath, you're not insulting anyone, you're writing what you believe.
* Reread and look for snark. Remove snark.
* Reread and look for snark hiding in humor. Redo humor or just remove it.
* Believe.

This is how I'm writing this post. I reread every paragraph and I think of you, sitting there, reading these words. In fact this paragraph was twice as long when I first wrote it.

There's a balance to this. Writing with Strunk and White's Rule 17 can be terrifying!

> **Omit needless words**.  
> Vigorous writing is concise. A sentence should contain no unnecessary words, a paragraph no unnecessary sentences, for the same reason that a drawing should have no unnecessary lines and a machine no unnecessary parts. This requires not that the writer make all sentences short, or avoid all detail and treat subjects only in outline, but that every word tell.

I agree with this when it comes to stories. Casual blog or script writing... that's tough. **Terse writing has a punch so the question then becomes "are you trying to throw one?"** Sometimes yes, sometimes no. I was trying to throw a few with that blog post above, so I leaned on Rule 17, which means I also had to throttle my acceptance filter. 

**If you throw punches with your writing, expect a few to come right back at you**. That's really the trick: _be OK with getting hit_. One writer I know had a problem with this and knew it, so he took it on literally. He signed up for muay thai classes with the singular goal of losing the fear of being punched in the face!

I think that's a healthy fear to have, but I admire him for trying.

## Validation

It's a horrible feeling when you hit "publish" and **no one reads or cares about the thing you just spent days, weeks, and months preparing**. You think it'll be a hit, climb Hacker News or Reddit and then... _nada_.

As I mention: I'm writing this thinking about you. Does that mean I'm writing it _for_ you? In a way, yes. In a way, _no_. I'm writing for the both of us but, mostly, I'm writing because I enjoy it. It pushes me to explore things in my own mind and out there in the world. When I share it, I do so with the hope that you'll enjoy it...

**Nah, I'm lying**. You noticed it too, didn't you! While it is indeed true that I'm thinking about you, I also know that if it moves you, I'll feel good about that. The trick is to take a step back and see the boundary between the words on the screen and my need for you to validate my existence. That last bit is toxic!

When I wrote _[The Imposter's Handbook](https://bigmachine.io/products/the-imposters-handbook/)_ I knew I was going to take heat because I wrote about things I was trying to learn, which you can read another way: _I **wrote about things I didn't know**_. That's a sin when it comes to writing.

The only way I got through the process was to throttle my need for you to validate my effort. The way I did that was to ultimately respect the work for what it would become.

This post, for instance. Many will hate it just because of the law of averages. Many will love it - hopefully there won't be too many in between as that's where the real shame is: _mediocrity_.

How do you go about setting a boundary, then? This process is related to acceptance, but not entirely. Here's what I think about with each paragraph, and what I'm thinking about now:

* Do I believe what I've just written?
* Do I care if you don't? (usually the answer is "yes")
* Am I willing to delete this paragraph if I knew, for sure, that you would think it horrible?
* Am I gazing deeply into my reflection in a pond?

I originally wrote this paragraph right here with some self-congratulatory bullshit but I deleted it. It was egotistical and self-serving... it's so easy to do. I mean... I _have_ written a lot and people _do_ ask my advice but... wow is this a balancing act!

So: how much should I care about whether you like what I'm writing? Stephen King would say "[not at all](https://www.amazon.com/Writing-10th-Anniversary-Memoir-Craft/dp/1439156816)":

> What are you going to write about? And the equally big answer: Anything you damn well want. Anything at all . . . _as long as you tell the truth_... What you know makes you unique . . . Be brave.”

Ah yes, _be brave_. That's good advice unless you're finding that your bravery is in service to your ego. I go there sometimes. As I said, it's hard not to. But you can check yourself as I'm doing right now, by reading over your work and pondering if your theme has switched to:

* How I live my amazing life
* You're doing it wrong, here's why
* There's no need for Thing I Don't Use. You Don't Need It Either.
* Here's how I helped thousands of people who didn't know what they knew
* Here's how I handled a weird situation and taught someone a lesson

Let me be clear: **there are plenty of people who can crow about themselves and I'll read every bit of it**. We all have our heroes and if you couldn't tell, Stephen King is one of mine and I'll read _anything_ he has to say about writing. But that's just the thing: _recognizing when you get self important_. **Did you earn the right to write what you just wrote**?

This post is about you developing a voice - specifically a _writer's voice_. I've done a lot of work on that subject and I want to share the _work_, not how amazing I am. It's a tightrope but I trust you'll tell me if I'm veering into self-importance. I'm dancing around the assertion that _yes, I have earned the right_. I've published books and people like what I write and I need to be OK with that if I'm going to keep going... which I really should so let's move on.

## Persuasion

**People love being persuaded but they hate being sold**. There's a difference: the first is when you help someone change their mind and see your point of view, the last is telling the person what they want to hear until they ultimately agree with you, truth be damned.

I find that **I get persuasive when I think no one will listen to me**. My voice isn't strong enough, so I need to fill it in with fluff and "I promise if you just try this thing it'll be amazing and you'll love it" kind of crap.

The throttle we're working with here is your need to be heard. If you _believe_ you have a voice, you'll _believe_ that people will listen to you. Seems overly simplified, but I find it to be true.

Once again: _I'm thinking of you, reading this_. I'm rereading my paragraphs as I go, twiddling three dials as I adjust my filters. I'm sharing something I find truthful, a process that has worked for me and brought me great joy (I love writing). Will it work for you? I honestly have no idea - but I hope so!

As you are sensing, **all of these needs are related to your core sense of self**. This is where your voice comes from, emanating from a truth deep inside you. Everything I've written here amounts to a singular goal: _understanding your truth and writing it with conviction_. Easier said then done! Let's get tangible.

Once again, I'll fall back to [Stephen King's advice](https://www.amazon.com/Writing-10th-Anniversary-Memoir-Craft/dp/1439156816) on this: **read other people's work**. This time, however, think about it in terms of mechanics. Where are their filters? Are they trying to sell you on an idea, or congratulate themselves too freely? Do they care if they anger you...

> The real importance of reading is that it creates an ease and intimacy with the process of writing; one comes to the country of the writer with one’s papers and identification pretty much in order. Constant reading will pull you into a palace (a mind-set, if you like the phrase) where you can write eagerly and without self-consciousness. It also offers you a constantly growing knowledge of what has been done and what hasn’t, what is trite and what is fresh, what works and what just lies there dying (or dead) on the page. The more you read, the less apt you are to make a fool of yourself with your pen or word processor.

## Your Voice Is There, Waiting

Let's go back to the beginning, considering Aaron's question once again:

> I find I grapple with "writing how I speak" because I tend to get in the weeds... If I could master that, I'd be better at writing...

Aaron's problem is "the weeds", which I think means that he becomes overly-detailed. Now that we've discussed the filters: which one do you think is active here?

My thought is this: people go into detail or repeat themselves because they don't think you'll care or pay attention. I do it all the time, especially when speaking verbally. Writing, on the other hand, allows me to go back and ponder my throttles, adjusting as I go.

If you find yourself in a similar situation, without an internal voice to guide you through a writing process: try examining how good writers write. I keep bringing up Stephen King because I love what he does, but I'm sure you have writers that you enjoy as well. Try rereading one of your favorite books by them and ponder their commitment to the story.

I think there are outstanding bloggers out there too. I've always enjoyed [Scott Hanselman's writing](https://www.hanselman.com/blog/). If I had to point to an influence when it comes to writing a blog, I would say "read Scott". Here's a clip from [a post I love](https://www.hanselman.com/blog/standing-on-their-shoulders-and-paying-it-forward) \- go read it and see how he manages his throttles to bring out his voice:

> It's my birthday! I turn 0x22 today, beginning the downward slide to 0x28, and then death. ;) Seriously, it's an interesting birthday because I'm definitely not a "young hotshot" any more. (It's possible I haven't been for 10 years, but I can dream, right?)

> It's funny how these things happen. I didn't think I'd be a Computer Person. In high school I was into Theatre, doing a number of plays, a few as the lead or co-lead, and I'd always assumed I'd be on TV by now. Of course, [Ryan Reynolds](http://www.ryanreynoldsonline.com/) has my career, so I can't do much about that. Heh, maybe I'm still in theater and I don't realize it?

Scott's writing has _rhythm_, which is effective use of commas and periods to break things up in to a "song". If you've seen him on stage before, that's exactly how he speaks.

Hope you found this post helpful but, if you don't, I'm OK with that too :).]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https:/images.unsplash.com/photo-1483489571097-0ca295ba3d08" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https:/images.unsplash.com/photo-1483489571097-0ca295ba3d08" />
  </entry>
  <entry>
    <title>Fast and Simple Web Apps with Nitro</title>
    <link href="https://bigmachine.io/posts/fast-and-simple-web-apps-with-nitro" rel="alternate" type="text/html"/>
    <updated>2022-10-31T23:48:44.000Z</updated>
    <id>https://bigmachine.io/posts/fast-and-simple-web-apps-with-nitro</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2022/10/nitro_title.jpg" alt="Fast and Simple Web Apps with Nitro" /></p>
I've been **writing a fun book** over the last few months on **Nuxt 3** and it's based on my experience building out bigmachine.io as well as the individual course sites I have. I've been using Nuxt for years and love it - and Nuxt 3 is particularly exciting because it's both **faster and simpler**.

One of the things I wanted to dig into was the server that powers Nuxt: [Nitro](https://nitro.unjs.io). Nuxt 3 behaves a bit differently than Nuxt 2, and I wanted to know why. Specifically:

* It's faster. I know a lot of this has to do with Vite, but the server starts instantly.
* The routing is different. You used to use an `_` for the dynamic routes, now you use square braces `[]`.
* The serverside routes are _ridiculously terse_, which I absolutely love.

So I spent the last few nights and weekends digging in and had a ton of fun. The result was a **quick 5 minute video** for you to enjoy!

If you're into terse simplicity, here you go...]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2022/10/nitro_title.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2022/10/nitro_title.jpg" />
  </entry>
  <entry>
    <title>Hello PostgreSQL</title>
    <link href="https://bigmachine.io/posts/hello-pg" rel="alternate" type="text/html"/>
    <updated>2022-10-27T02:37:28.000Z</updated>
    <id>https://bigmachine.io/posts/hello-pg</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https:/images.unsplash.com/photo-1557050543-4d5f4e07ef46" alt="Hello PostgreSQL" /></p>
In this video production, created back in 2011, Rob Sullivan (a SQL Server DBA) and I discover PostgreSQL from the perspective of a DBA (Rob) and a developer (me).

This is when I first got to know Postgres and it blew my mind. Keep in mind that Postgres has changed over the years, but so many of the features here are still around. If you're hoping for a quick primer, these videos are for you! **They're free to all, as long as you're a member**!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https:/images.unsplash.com/photo-1557050543-4d5f4e07ef46" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https:/images.unsplash.com/photo-1557050543-4d5f4e07ef46" />
  </entry>
  <entry>
    <title>Get Involved!</title>
    <link href="https://bigmachine.io/posts/get-involved" rel="alternate" type="text/html"/>
    <updated>2022-10-25T00:46:10.000Z</updated>
    <id>https://bigmachine.io/posts/get-involved</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https:/images.unsplash.com/photo-1587825140708-dfaf72ae4b04" alt="Get Involved!" /></p>
Back in 2011 [Scott Hanselman](https://hanselman.com) and I recorded a video for my old video company, **Tekpub.com**, all about elevating your developer career. In 2013 I sold Tekpub to [Pluralsight](https://pluralsight.com) with the promise that **this video would always be free and available**.

With my deepest and sincerest thanks for that promise - here is the original video that Scott and I created many, many years ago. I think it holds up extremely well and if you're curious about getting involved in _your_ community - go for it! Here's your blueprint...]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https:/images.unsplash.com/photo-1587825140708-dfaf72ae4b04" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https:/images.unsplash.com/photo-1587825140708-dfaf72ae4b04" />
  </entry>
  <entry>
    <title>People Who Create Tutorials Need to Try Harder</title>
    <link href="https://bigmachine.io/posts/try-harder-please" rel="alternate" type="text/html"/>
    <updated>2022-09-20T00:04:32.000Z</updated>
    <id>https://bigmachine.io/posts/try-harder-please</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2022/09/bip_806.jpg" alt="People Who Create Tutorials Need to Try Harder" /></p>
A few weeks ago I asked a simple question on Twitter:

> If you’ve been thinking about learning SQL but didn’t or haven’t, what’s stopping you? Would love to know more :) light me up!

The responses were mixed, but a friend of mine confided in me that SQL is just plain boring. The syntax is weird and they just couldn't see why they would use it vs. an ORM.

![I see you](https:/images.unsplash.com/photo-1631259307720-3bf59418c31a"

Photo by [Oleg Didenko](https://unsplash.com/@o%5Fdid?utm%5Fsource=ghost&utm%5Fmedium=referral&utm%5Fcampaign=api-credit) / [Unsplash](https://unsplash.com/?utm%5Fsource=ghost&utm%5Fmedium=referral&utm%5Fcampaign=api-credit)

## This is a Real Problem

This is a real bug of mine: people who want to teach other people something seem content to use the most meaningless, ridiculous examples!

I remember trying to learn React from a popular paid tutorial site and the author completely phoned in the demo application, calling it exactly that! With super exciting component names like `MyComponent` that `foo` property and `bar` method.

If you're a content producer: please take a little extra time to find something relevant and meaningful. Draw on your own experience and show how you solved something! 

## It's Keeping People From Learning

This lack of effort when it comes to demo material is harmful to people who try to learn. Take SQL for instance - the most commonly used data set (that I've seen) is [DVD Rental](https://www.postgresqltutorial.com/postgresql-getting-started/postgresql-sample-database/). This is crazy-making because:

* Who rents DVDs anymore? I'm sure there are some people, sure, but come on! We can do better.
* The schema is so completely unreal it boggles my mind. I could go off on everything wrong with it but please, trust me, it's completely made up and ridiculous.
* Did you even try to find something relevant and real?

That last one gets me and yes, I do realize it's a peave I have that I should probably let go of.

But here, let's try it together. Google "[Real World Datasets](https://www.google.com/search?q=real+world+datasets)." The results are pretty good:

![](https://bigmachine.io/img/2022/09/image.png)

One of the links you'll end up at is Kaggle, one of my favorite data sites:

[Find Open Datasets and Machine Learning Projects | KaggleDownload Open Datasets on 1000s of Projects + Share Projects on One Platform. Explore Popular Topics Like Government, Sports, Medicine, Fintech, Food, More. Flexible Data Ingestion.![](https://www.kaggle.com/static/images/favicon.ico)Kaggle](https://www.kaggle.com/datasets)

## There Is a Wealth of Real World Stuff Out There In The Real World

OK, I'm done ranting! I have been meaning to create a SQL resource for people who want to get started with database "stuff". As mentioned above, people just won't care if they can't see the value of a tool to the things they actually care about.

Given that it's the start of the American football season and that I have quite a few friends trying to figure out who to start on their fantasy football team - I figured that this would make a pretty [damn good SQL tutorial](https://bigmachine.io/little-sql-book/).

Turns out it did! I've had over 120 downloads in the last 24 hours, and that's just announcing it on Twitter.

[![](/2022/09/littlesql-1.jpg)](https://bigmachine.io/little-sql-book/)

I found [my data set](https://www.kaggle.com/code/mur418/2020-fantasy-football) on Kaggle, which I liked as it was concise and simple to work with. As you might imagine, this data can be intense! I just wanted something in summary for that could be used for a draft cheatsheet.

If you're interested in SQL, [go have a look](https://bigmachine.io/little-sql-book/)! It's 70 pages long and absolutely free.

##]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2022/09/bip_806.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2022/09/bip_806.jpg" />
  </entry>
  <entry>
    <title>Why Blogs Make Outstanding Books</title>
    <link href="https://bigmachine.io/posts/why-blogs-make-the-best-books" rel="alternate" type="text/html"/>
    <updated>2022-09-08T23:46:09.000Z</updated>
    <id>https://bigmachine.io/posts/why-blogs-make-the-best-books</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https:/images.unsplash.com/photo-1586339393565-32161f258eac" alt="Why Blogs Make Outstanding Books" /></p>
Four years ago I was reading [Troy Hunt's blog](https://www.troyhunt.com/pwned-the-book-is-finally-here/) thinking: "this guy can _write_". **Good writing is all about being truthful** \- true to yourself, the story you're writing, and most of all true to your readers. **Troy's posts are a masterclass** in _laying it out there_ in the most honest, unvarnished way possible.

Anyway, the next thing that hit me was _**this dude should write a book**_. Of course I thought that, I like writing. But so must Troy because some of his posts are epic!

Some of his posts... are... epic... 🤔.   _I wonder if I could turn his blog into a book somehow..._

I'm not the first person with this idea - in fact there are entire businesses devoted to doing just that. In fact, _The Martian_ started out life as a series of blog posts by Andy Weir and was later turned into a book which, I hear, did pretty well.

## What If We Made Something More?

For anyone but the author, **the best part of a blog post is the comments section**. I don't need to explain this to you - there's always drama, criticism and, occasionally, a nugget of gold. 

What if we wrote a book that added the comments but scrubbed away the nonsense? I could curate that I think.

The idea took hold in me and I couldn't stop thinking about it. I started combing through Troy's blog for posts that I thought would fit an interesting story arc, which could be the rise of his career, the start of [HaveIBeenPwned](https://haveibeenpwned.com/), and his new life to come.

Then I read his post about [leaving Pfizer](https://www.troyhunt.com/today-marks-two-important-milestones/), to go out on his own. This is how the post ended:

> I’m enormously excited by what’s happening next. The experiences at Pfizer have shaped that future and exiting in this fashion is the best result I could ever have hoped for. I’ll follow up with another post as the dust settles on this outgoing phase of my life, I’m _enormously_ excited about what’s coming next!

People knew who Troy was in 2015 - he released HaveIBeenPwned 2 years prior. But he wasn't the Troy of today, which got me thinking...

## Let's Add Some Perspectives

**I picked 36 or so posts that I thought told a great story** and read well, and then I proposed the idea to Troy: _let's compile your posts into a book. I'll curate the comments and add them too - you write introductions and epilogues for each post, giving a "behind the scenes" feel to it_.

This is Troy's response, verbatum:

> You’ve done a great job with the Imposter’s Handbook and whilst this is a very different beast, I know the TLC you’ve applied to that. If it was anyone else suggesting this, I probably wouldn’t even reply to the email!

Wow. It's true - I put _everything_ into the books and videos I create - podcasts too. I better not mess this up...

## And It's Alive...

After four years, we launched the book.

[PWNEDIn 2015 Troy Hunt left a well-paying career at one of the largest companies in the world. He created a blog and set about building a career in online security, working for himself![](https://bigmachine.io/images/logos/cs.png)Big Machine![](https://bigmachine.io/images/pwned/cover.jpg)](https://bigmachine.io/products/pwned/)

Right now, as I write this, **I'm a bit numb**. I mean - it's not like I spent every waking moment on this thing, but it's always been there. Something I would spend an hour on here and there. Troy, Charlotte and I would get on a video call periodically to talk about this post or that, how to order things, formatting issues and so on.

I miss that. It's like watching your kid go off to college which I had to do two years ago. You're so excited for their new life - one that's going to be so much fun and open so many doors... but then... that's your baby heading out the door and into their life.

Troy's book means a lot to me and you can read why in the first few pages, where I took the liberty of explaining the intensely personal reasons as to why I put so much care into this thing. We all did, and I think this review captures it:

> **Love, love, LOVE the intros** ... in particular Rob Conery ... fantastic and really anchor an emotional aspect to the book, that i was not expecting. Great to see a book deliver this authenticity - we're all only human after all!. I "cheated" and also skipped to read Charlotte's epilogue and again was blown away by the depth and genuine nature of the emotion on display. **I honestly was not expecting there to be so much heart on display, but am very glad there is**.

Troy and I faced the biggest challenges of our lives, oddly at the same time. Creating this book was a necessary thing, for both of us. 

## Not Just Any Old Blog

You can't run a scraper over a random blog, output as a PDF and call it a day. The person behind the blog needs to be authentic and "write true". They also should have something to say and not be afraid to say it. If I'm honest, I can't think of many blogs that fit that profile.

If you do find one, however, it's a treasure. Each post anchors a larger story that, to me, makes the blog come alive with historical meaning. This comment from Henk Brink captures that:

> **I haven't been able to put the book down**. The added intros and epilogue on each post in particular and the retrospectives from today's perspective are particularly interesting...

If you're up for a fun read into the lively career of Troy Hunt, [well here you go](https://bigmachine.io/products/pwned/). We put a lot of life in there, and I do hope you enjoy it.

I do hope to do another project like this, and I have a few names in mind - but if you have a suggestion, hit me up on Twitter or, if you're a subscriber here, leave a comment below.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https:/images.unsplash.com/photo-1586339393565-32161f258eac" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https:/images.unsplash.com/photo-1586339393565-32161f258eac" />
  </entry>
  <entry>
    <title>Losing Yourself</title>
    <link href="https://bigmachine.io/posts/losing-yourself" rel="alternate" type="text/html"/>
    <updated>2022-08-22T02:29:41.000Z</updated>
    <id>https://bigmachine.io/posts/losing-yourself</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https:/images.unsplash.com/photo-1503256575996-7cbe509190b7" alt="Losing Yourself" /></p>
I was sitting outside my little apartment in Marin County where I'll be staying for most of this year and I was thinking about work and what I like to do. And then I was thinking _what I'm actually doing_.

I really enjoy writing and making videos, but it's become more difficult and I'm not sure why. Over the years the joy has waned and I find that each post or video requires more and more courage to create. Not because I'm lazy, because _I'm scared and have a hard time writing the truth_.

Any author will tell you: if you're not writing something true, you're not writing. God... what happened to me?

## Inspiration

Me being scared is my problem and I like to think that maybe I'm making too much of things. I let myself daydream about a "what if" scenario and I thought about creating this blog. I then found myself on the Ghost website and the showcases and then [John O'Nolan's blog](https://rediverge.com) and eventually this video John made many years ago:

John's a digital nomad? _I had no idea_. I did that for a year and I plan on doing it again - sporadically this year as my youngest is in her last year of high school and I very much want to be here as much as I can - but **next year, I'm out**.

Anyway, at the 1:00 mark John describes something all too familiar to me (emphasis mine):

> I've gone from being this sort of free-thinking, open minded person who wrote a lot on the internet to someone who's almost ... just afraid of doing any of that and... **scared of what the response might be.** I've just come to realize that I don't want to do that anymore.

John read my mind exactly. EXACTLY.

## It's Not Us vs. Them, It's Me vs. Me

See even as I write this I hear little voices in my head saying _oh poor cis white tech bro can't bleat all over the internet_. I'm not joking about that - it's exactly what I hear, right now in my mind. In fact I'm fighting the urge to delete this entire paragraph because I'm worried that it will sound whiny and privileged.

It's easy to see things in black and white and, in fact, it's a common problem for people who have been through an emotional wringer. They isolate themselves and go into survival mode, evaluating everything as threat/non-threat and fight or flight. Let's just say I've learned a lot about this stuff over the years.

The only way out, is _through_ and it's not me valliantly standing up to the voices on Twitter, the negative comments on YouTube or the shitty emails I might get - it's me standing up to me. Breaking the construct down, believing that what I have to say is valuable.

I know I'm not alone feeling this and I don't think it matters how you identify. There's a lot of shit that's happened over the years and it's _traumatic_. A lot of wonderful things have happened too. 

I'd like to think they're going to keep happening.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https:/images.unsplash.com/photo-1503256575996-7cbe509190b7" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https:/images.unsplash.com/photo-1503256575996-7cbe509190b7" />
  </entry>
  <entry>
    <title>Just Paddle</title>
    <link href="https://bigmachine.io/posts/just-paddle" rel="alternate" type="text/html"/>
    <updated>2022-08-21T11:09:40.000Z</updated>
    <id>https://bigmachine.io/posts/just-paddle</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https:/images.unsplash.com/photo-1616449973117-0e1d99c56ed3" alt="Just Paddle" /></p>
I have the hardest time starting things. Like this post, for instance. I'll just sit there and think... _and think_... which is funny because if you know me personally you might say something like "really, Rob? You... _think_ too much?"

Riding waves is so much like life. I don't think I need to explain this too much - but often the hardest thing to do when surfing is _paddling_. Making the decision to go and then committing 100% to whatever happens. 

**That's the irony**. I love to try things, change my mind, race around, zig and zag... you get the idea. I'm an optimizer. It's my thing.

![This is my son finishing the last lap of a 6 race, three city racing series.  He finished first through a combination of effort, tenacity and results.](https:/images.unsplash.com/photo-1505570554449-69ce7d4fa36b"

Photo by [David Armstrong](https://unsplash.com/@armstrong99?utm%5Fsource=ghost&utm%5Fmedium=referral&utm%5Fcampaign=api-credit) / [Unsplash](https://unsplash.com/?utm%5Fsource=ghost&utm%5Fmedium=referral&utm%5Fcampaign=api-credit)

Unfortunately I also tend to overthink things and don't even start the race. Take this blog, for instance. **I've wanted to create something simple, where I can post ideas and long-form "tutorials"** if you will, but every time I think about it I lock up, trying to optimize what I'll create, how it will be presented, etc.

## People Like What I Create

Which is a problem. I've been making videos for years - [even sold a company once](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwja0qax0Nb5AhX7HTQIHSljA10QFnoECAgQAQ&url=https%3A%2F%2Fwww.crunchbase.com%2Forganization%2Ftekpub&usg=AOvVaw2jyToKdVO1oOYh4moi5lhX). I write books, [one of which has sold almost 30,000 copies](//products/the-imposters-handbook/), self published!  
I research something, get excited when it works and then get even more excited when I see connections to other things I've learned. I take notes, make some demos and in 3-6 months I might have a video I can sell. Or maybe I'll take a year and write a book!

I like writing. I like making videos. **I don't like how long it takes so I end up just giving up** or doing less so it doesn't take a ton of time... with the videos ending up on YouTube.

There has to be a happy middle ground where I can do good, valuable work that people are willing to pay for without becoming a cheesy cloud coach guru. I don't mind giving things away, but if I'm getting paid to do it I'll make 10x the content in terms of quality and quantity.

But how do I go about that? What types of things will I create... how will I present this to people? _I need to think about this..._

## Or Maybe I Don't

The domain for this site, _robconery.com_, seems pretty obvious for someone like me because that's my name. The problem is that I have a hard time with attention and I very much don't want to give the impression of trying to be a "guru". **So I spent 3 days (literally) pondering domain names.** It had to be _just right_ so people didn't get the impression I was full of myself.

_What a load of shit_. I mean... I do good work! People like what I do (see above) so why shouldn't I be OK with doing more of it in a place with my name on it? I'm not a guru, I'm an explorer and hopefully that will come across in everything I do here.

Or maybe I just stop thinking about it and _just do it already._

**I want a place where I can stop thinking and just do things for fun**. A long trek through the world of travel, software, whatever. I plan on discovering some fun stuff along the way and if you decide to join, I hope you find it valuable.

If not, that's OK too. I'll keep writing nonetheless and I can think this thing to death and sit here and tell you what my plans are but honestly I have no idea.

Big wave, might be too big, or maybe close out. Let's find out and _just paddle_.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https:/images.unsplash.com/photo-1616449973117-0e1d99c56ed3" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https:/images.unsplash.com/photo-1616449973117-0e1d99c56ed3" />
  </entry>
  <entry>
    <title>Creating a Full Text Search Engine in PostgreSQL, 2022</title>
    <link href="https://bigmachine.io/posts/creating-a-full-text-search-engine-in-postgresql-2022" rel="alternate" type="text/html"/>
    <updated>2022-07-17T23:55:00.000Z</updated>
    <id>https://bigmachine.io/posts/creating-a-full-text-search-engine-in-postgresql-2022</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https:/images.unsplash.com/photo-1572375992501-4b0892d50c69" alt="Creating a Full Text Search Engine in PostgreSQL, 2022" /></p>
A few years ago I wrote about [how to “fine tune” a full text index in PostgreSQL](https://file+.vscode-resource.vscode-cdn.net/2019/10/29/fine-tuning-full-text-search-with-postgresql-12/) 12, but that was a few years ago and things have changed a bit. The current version of PostgreSQL is 14 and Postgres just keeps getting better and better.

In this video I show you how the process a DBA might take when creating a full text index in Postgres. It’s not enough to throw a `tsvector` field onto a table, create a trigger and call it a day. You have to know what your users are searching for and how they’re searching for it.

These days we have `generated` columns and don’t need triggers. We also have `websearch_to_query` instead of the old `plainto_tsquery` or it’s languagy big brother, `phrase_to_tsquery`.

We can use this power to do all we need without having to use a third-party system like Sphinx or Elastic (as good as they are).

Hope you enjoy the video!

## The Code and data

If you want to play along you can download the data set here. It’s about 3Mb and is a single SQL file that contains the table definition and structure. To run it, unzip the file and pop it into a database:

```bash
createdb scifi
psql scifi < questions.sql
```

There’s a lot of code in the video, but the main bits are:

```sql
--add the search index
alter table questions
add search tsvector
generated always as (
  setweight(to_tsvector('simple',tags), 'A')  || ' ' ||
  setweight(to_tsvector('english',title), 'B') || ' ' ||
  setweight(to_tsvector('english',body), 'C') :: tsvector

) stored;

-- add the index
create index idx_search on questions using GIN(search);

-- the search query
select title, body,
  ts_rank(search, websearch_to_tsquery('english','vader tie fighter star-wars')) + 
  ts_rank(search, websearch_to_tsquery('simple','vader tie fighter star-wars')) as rank
from questions
where search @@ websearch_to_tsquery('english','vader tie fighter star-wars')
or search @@ websearch_to_tsquery('simple','vader tie fighter star-wars')
order by rank desc;

-- turning it into a function
create or replace function search_questions(term text) 
returns table(
  id int,
  title text,
  body text,
  rank real
)
as
$$

select id, title, body,
  ts_rank(search, websearch_to_tsquery('english',term)) + 
  ts_rank(search, websearch_to_tsquery('simple',term)) as rank
from questions
where search @@ websearch_to_tsquery('english',term)
or search @@ websearch_to_tsquery('simple',term)
order by rank desc;

$$ language SQL;
```]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https:/images.unsplash.com/photo-1572375992501-4b0892d50c69" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https:/images.unsplash.com/photo-1572375992501-4b0892d50c69" />
  </entry>
  <entry>
    <title>The Fabulous Linked List</title>
    <link href="https://bigmachine.io/posts/the-fabulous-linked-list" rel="alternate" type="text/html"/>
    <updated>2022-06-24T23:30:00.000Z</updated>
    <id>https://bigmachine.io/posts/the-fabulous-linked-list</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2022/08/06_title.jpg" alt="The Fabulous Linked List" /></p>
The second time I interviewed at Google I was asked to write a linked list from scratch. I was expecting something a little more “difficult” and thought I would breeze through the question. I had been studying for months for this interview, after all, so this should be easy… right?

It didn’t take long before I completely locked up. The logic for inserting and removing was wild… and then I was asked to reverse the linked list, another ridiculous and annoying question.

And yet I couldn’t do it on the first try.

**I failed that interview**, which was humiliating. I had 15 years of experience and I couldn’t reverse a linked list?

## Know Your Basics

I know I’m not the only one to get blindsided by a basic question like this. Turns out, this is a tactic used by a lot of interviewers when interviewing senior people: ask a basic question and see how they handle it. A friend of mine does this kind of thing routinely for a larger tech company. Their thinking is straightforward: if you get grumpy when asked to do something simple or “beneath you”, you might not be the best candidate.

I can see the logic in that.

That’s what today’s video is all about: doing the very basics with a Linked List. Hope you enjoy! If you want to play along, here is some code to get you started:

```javascript
//This is the simplest possible linked list
class Node{
  constructor(val){
    this.value = val;
    this.next = null;
  }
  append(val){
    this.next = new Node(val);
    return this.next;
  }
}
```

---]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2022/08/06_title.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2022/08/06_title.jpg" />
  </entry>
  <entry>
    <title>What&apos;s the Difference Between Vue 2.0 and Vue 3.0?</title>
    <link href="https://bigmachine.io/posts/whats-the-difference-between-vue-2-0-and-vue-3-0" rel="alternate" type="text/html"/>
    <updated>2022-05-30T02:05:00.000Z</updated>
    <id>https://bigmachine.io/posts/whats-the-difference-between-vue-2-0-and-vue-3-0</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2022/08/vue3_title.jpg" alt="What&apos;s the Difference Between Vue 2.0 and Vue 3.0?" /></p>
In this video I look at the main differences between Vue 3 and Vue 2\. I’m happy to report that they are, for the most part, aimed at making things faster, smoother and more straightforward.  
The best changes that I have seen are:

* **Using** Vite as the build tool. This thing is blazing fast which makes all the difference in the world.
* **The creation of Pinia** as a replacement for Vuex. Bigger apps need a centralized state store and I, for one, am happy to see Vuex go away. The Redux-y stuff was just a bit too much.
* **The Composition API**. Almost everything in Vue is now done by importing and executing specific functions that do a single thing. You can still pass objects in the old declarative style, but working with the Composition bits is refreshing.
* **Teleport**! You can now “shove” a component into any DOM element on the page as long as it has a selector.

Have a look:

---

## Learn The Core CS Concepts Every Programmer Should Know - Free 

Every day programmers like you and me have to solve complex problems. In this **free, 52 page PDF** I'll share with you some of the skills and techniques I use on a **daily basis**. Pop your email below and I'll send it right off!

Send me the free book!

I respect your privacy. Unsubscribe at any time.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2022/08/vue3_title.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2022/08/vue3_title.jpg" />
  </entry>
  <entry>
    <title>What&apos;s Your Exit Plan?</title>
    <link href="https://bigmachine.io/posts/whats-your-exit-plan" rel="alternate" type="text/html"/>
    <updated>2022-04-01T23:27:00.000Z</updated>
    <id>https://bigmachine.io/posts/whats-your-exit-plan</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https:/images.unsplash.com/photo-1552576949-04e3a68b0a9f" alt="What&apos;s Your Exit Plan?" /></p>
It doesn’t matter if you work for yourself, at a startup, or in a massive enterprise environment: _knowing what you’re going to do when it’s over is incredibly important to your career_. I work at Microsoft currently and before I started there in 2018 I pondered my “end game” – where do I want to be when it’s over and what do I hope to gain?

Let’s dig in to this.

## BIG TECH/BIG ENTERPRISE

For a lot of people in a big tech company, the goal is to **accumulate enough stock** so you can live comfortably on top of your retirement savings plan. If the company is private then you take a bit of a risk here as the only way your stock is worth anything is if you can sell it.

Working for a publicly-traded FAANG(M) (Facebook, Apple, Adobe, Netflix, Google and Microsoft) is a sound plan. If you work at a company like Google or Microsoft for decades you will accumulate a giant chunk of stock that will stay relatively stable. Many of my friends have **portfolios in the millions** but that, of course, depends a lot on the performance of the company stock in the market.

How do you exit this kind of job? For many people it means retirement – often at an age much younger than “retirement age” used to be.

Let’s say you’re this person and at some point in the future you’ve been at Google for 25 years. You’ve received promotions and a yearly stock grant, and you’ve managed your investments well, retiring with X million dollars and a retirement plan worth Z.

In this scenario, what are X and Z? Moreover, what do you plan to do in your mid 50s? It might seem like a simple answer and who knows, maybe it is! Traveling is fun, but so is building something useful to the world and having a sense of meaning. You’ve likely thought these things through – but it might be worth discussing them with people who are facing retirement themselves, or who have just been through it.

Both of my brothers are at this point in their careers. My oldest brother is a retired college professor who is trying to figure out what he’s going to do now that he doesn’t have to create lesson plans. I asked him directly, and he didn’t have an answer and instead shrugged in an “**I’m sure I’ll find something**” kind of way, but it’s not hard to see that he feels a sense of loss.

My other brother drives a bulldozer and hauls dirt. He keeps saying he’s going to stop and **“this is my last year, for sure“…** but he hasn’t. He enjoys what he does and keeps taking jobs. Working for yourself has that effect!

## SOLO CONTRACTING/HIRED GUN

I’ve worked on my own for many, many years prior to working at Microsoft and **I like the freedom of it – but I dislike the stress**, especially when a contract is over and you don’t have another one lined up.

After a few years things tend to smooth out and, hopefully, you’re making more money than you need and you can save but let’s face it: you’ll be hard-pressed to make as much as a big tech company can offer in the form of stock + salary, unless you’re very, very good. And maybe you are!

Either way: **is this something you plan to do until you can’t do it any more, or just don’t want to**? Work is harder to come by as you get older in this industry – it’s simply a painful truth. I used to get calls from recruiters constantly; this year I’ve had 2.

If you’re very good and very niche, the contract world can be extremely lucrative, but someday you’ll complete your last contract and send your last invoice. What does that day look like to you, and what’s next? THE STARTUP GRIND

I have created 3 companies so far. The first was a consulting company I founded with a friend in 1998\. We did very well for a few years and went out of business in 2002 due to the whole dotcom implosion thing.

The second one, Tekpub.com, was [sold to Pluralsight](https://www.pluralsight.com/newsroom/press-releases/pluralsight-acquires-tekpub--third-acquisition-in-3-months-) in 2013\. I’m still running [the third one](https://bigmachine.io/) in my spare time – which is where you’re reading this post.

This is, perhaps, the hardest thing to consider when starting a company of your own: how will I leave this? More importantly: **how do I want to leave this**?

I know that I will either leave or dissolve my company at some point and I’ve made the decision it will be when I’m no longer here. **I don’t plan to retire**! I want to keep writing books, articles and making videos until I absolutely can’t do it anymore which means I’m probably in the ground.

With Tekpub, my cofounder James Avery and I had this conversation in the very first week. We didn’t think Tekpub would ever go public, nor did we want it to. We didn’t want to deal with venture capital nor running a massive company. We felt that doing so would **dilute the quality of our content** and I still feel this way!

We knew that acquisition was a very likely outcome so we laid out some numbers we would entertain, how we would get to that point and also when. It basically worked out the way we thought it would but I’ll be honest: **selling my business was traumatic**.

James and I built Tekpub from nothing and were doing very well for the most part. When I sold it I immediately missed it and wanted to start it all over again, which I kind of have with Big Machine.

For me acquisition was a nice windfall with short-term excitement, but I love what I do and I stopped being able to do it every day. I found out the hard way that money doesn’t buy happiness and **my exit plan sucked**.

## SO, WHERE YOU GOIN THEN?

We’re so lucky to be in this industry, with so many opportunities in front of us. One person can build a business from nothing and [sell it to Microsoft for billions](https://en.wikipedia.org/wiki/Mojang%5FStudios) of dollars. How insane is that?

You can also build a business like my friend [Frans Bouma](https://www.llblgen.com/), who sells developer tools that enable him to take fun pictures in video games. Another friend, [Rick Strahl](https://west-wind.com/), also sells developer tools that enable him to chase the wind between Hawaii and Oregon. These people are just fine without company stock, an IPO or acquisition and, instead, **enjoy the work they do every day**.

Other friends enjoy the security of a job at a FAANG(M) and know they’ll be taken care of until the day they retire with their fat nest egg. They interact with coworkers every day, leave work behind them (for the most part) when they go home and don’t stress out about their next paycheck.

The important thing is to **know what you want to be doing and where you want to be when your current gig is done**.

---

## Learn The Core CS Concepts Every Programmer Should Know - Free 

Every day programmers like you and me have to solve complex problems. In this **free, 52 page PDF** I'll share with you some of the skills and techniques I use on a **daily basis**. Pop your email below and I'll send it right off!

Send me the free book!

I respect your privacy. Unsubscribe at any time.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https:/images.unsplash.com/photo-1552576949-04e3a68b0a9f" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https:/images.unsplash.com/photo-1552576949-04e3a68b0a9f" />
  </entry>
  <entry>
    <title>Importing a CSV Into PostgreSQL Like a Pro</title>
    <link href="https://bigmachine.io/posts/importing-a-csv-into-postgresql-like-a-pro" rel="alternate" type="text/html"/>
    <updated>2022-02-23T04:07:00.000Z</updated>
    <id>https://bigmachine.io/posts/importing-a-csv-into-postgresql-like-a-pro</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2022/08/03-csv.jpg" alt="Importing a CSV Into PostgreSQL Like a Pro" /></p>
## USING HEAD

It all starts with using the head command in the shell in order to pull out the column names:

```sh
head -1 master_plan.csv

```

This will pop out the very first line of the CSV, which is typically the header row. I’m working with Cassini’s mission plan data, so this is what I see:

![](https://bigmachine.io/img/2022/08/head-command.png)

Now I just copy/paste that into VS Code and run a simple replacement using “Change all Occurrences” to build my create table statement.

The final step is to use the copy from command to pull data out of the CSV and into the database. There’s a whole lot more to this (like data types to use and creating an isolated schema) – just watch the video already!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2022/08/03-csv.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2022/08/03-csv.jpg" />
  </entry>
  <entry>
    <title>Big O Notation</title>
    <link href="https://bigmachine.io/posts/big-o-notation" rel="alternate" type="text/html"/>
    <updated>2022-02-10T07:51:00.000Z</updated>
    <id>https://bigmachine.io/posts/big-o-notation</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2020/07/screenshot_65.jpg" alt="Big O Notation" /></p>
When I started writing __[The Imposter’s Handbook](//products/the-imposters-handbook/)_, this was the question that was in my head from the start: __what the f\*\*\* is Big O and why should I care?_ I remember giving myself a few weeks to jump in and figure it out but, fortunately, I found that it was pretty straightforward after putting a few smaller concepts together. 

__**Big O is conceptual**_. Many people want to qualify the efficiency of an algorithm based on the number of inputs. A common thought is _if I have a list with 1 item it can’t be O(n) because there’s only 1 item so it’s O(1)_. This is an understandable approach, but **Big O is a __technical adjective_**, it’s not a benchmarking system. It’s simply using math to describe the efficiency of what you’ve created.

__**Big O is worst-case**_, always. That means that even if you think you’re looking for is the very first thing in the set, Big O doesn’t care, a loop-based find is still considered O(__n_). That’s because Big O is just a descriptive way of thinking about the code you’ve written, not the inputs expected.

## THERE YOU HAVE IT

I find myself thinking about things in terms of Big O a lot. The cart example, above, happened to me just over a month ago and I needed to make sure that I was flexing the power of Redis as much as possible.

I don’t want to turn this into a Redis commercial, but I will say that it (and systems like it) have a lot to offer when you start thinking about things in terms of __time complexity_, which you should! ****It’s not premature optimization to think about Big O upfront, it’s** __****programming**_ and I don’t mean to sound snotty about that! If you can clip an O(__n_) operation down to O(__log n_) then you should, don’t you think?

So, quick review:

* Plucking an item from a list using an index or a key: O(1)
* Looping over a set of __n_ items: O(__n_)
* A nested loop over __n_ items: O(__n^2_)
* A divide and conquer algorithm: O(__log n_)

---

## Learn The Core CS Concepts Every Programmer Should Know - Free 

In this **free, 52 page PDF** I'll share with you some of the skills and techniques I use on a daily basis. 

Send me the free book!

We respect your privacy. Unsubscribe at any time.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2020/07/screenshot_65.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2020/07/screenshot_65.jpg" />
  </entry>
  <entry>
    <title>What’s the Best Hashing Algorithm for Storing Passwords?</title>
    <link href="https://bigmachine.io/posts/whats-the-best-hashing-algorithm-for-storing-passwords" rel="alternate" type="text/html"/>
    <updated>2022-01-11T04:13:00.000Z</updated>
    <id>https://bigmachine.io/posts/whats-the-best-hashing-algorithm-for-storing-passwords</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2022/08/01-hashing.jpg" alt="What’s the Best Hashing Algorithm for Storing Passwords?" /></p>
If you’ve had to store sensitive user information in a database, you’ve probably heeded the advice to “**just use bcrypt**”. But do you know why? What other choices are there? In this video we take a deep look at **bcrypt, pbkdf2, scrypt** and **argon2**!

Crypto is a major weakness of mine and a subject I’ve put off learning about for ages. I’ve spent a few months with it now and it’s so much fun to learn about – specifically hashing. Here’s what I founMost developers just let their authentication library (or service) dictate which hashing algorithm to use, and normally that’s just fine. Well… until you get hacked and lose your user’s sensitive data.

Understanding hashing algorithms means understanding their resilience against certain kinds of attacks. That resilience is brought about by how difficult it is to calculate the hash. Algorithms like MD5 and SHA-x are all about speed, because that’s how they’re used! When you commit to Git, a SHA-1 hash is created for you and you certainly don’t want to be slowed down.

But when an attacker tries to brute force a rainbow table attack on your stolen data, you want that hashing algo to be damn slow!

In this video we’ll take a look at the most popular algorithms, including my new favorite, [Argon2](https://github.com/P-H-C/phc-winner-argon2).]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2022/08/01-hashing.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2022/08/01-hashing.jpg" />
  </entry>
  <entry>
    <title>Postgres For Those Who Can&apos;&apos;t Even, Part 3 - In The Real World</title>
    <link href="https://bigmachine.io/posts/postgres-for-those-who-cant-even-part-3-in-the-real-world" rel="alternate" type="text/html"/>
    <updated>2020-04-17T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/postgres-for-those-who-cant-even-part-3-in-the-real-world</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2020/04/shot_182-1.jpg" alt="Postgres For Those Who Can&apos;&apos;t Even, Part 3 - In The Real World" /></p>
This is part 3 of a series of posts I'm writing for Friendo, a web person who wants to get their hands a lot dirtier with Node and Postgres. You can [read part 1](/2020/01/24/postgresql-for-those-who-cant-even-part-1/) here, and [part 2 here](/2020/02/05/postgres-for-those-who-cant-even-part-2-working-with-node-and-json/), where we say hello to Postgres and learn how to use it with Node.

In this post we'll depart from the fun happy demo world where everything "just works super cool isn't that awesome?" and into the weeds, trying to build an application foundation that won't utterly suck in 6 months. If you want to look over/play with [the code it's right here](https://github.com/robconery/node-pg-start) although it's still super preview.

The short summary here is that I wanted to build something reasonably real and sort of went off and built ... something kinda real that I would use in production today. It was fun to put together and I thought I would share it with you.

Lots to get into, let's go.

## SQL... Leave It To The Pros

Poeple don't like seeing SQL in their code for a variety of mostly silly reasons:

- "SQL doesn't scale"
- "SQL === Injection Attacks"
- "Junior devs won't understand it"
- "It's ugly and hard to work with"

Injection attacks are _definitely_ something you'll want to worry about _no matter what tool you use_. Also: if you don't know SQL then yes, it can appear verbose and daunting. To me, however, there's a major reason you should use a simple abstraction: _future you will be confused_.

It can be really difficult to transition your brain from reading application code and tests to SQL, trying to reconcile what a query might do or return. Code is indeed a bit more expressive in this way, and it's easier to refactor.

### Let's Use MassiveJS

My favorite DB tool is one that I created many years ago and has since been taken over by the ultra freaky amazing [Dian Fay](https://twitter.com/dianmfay) - [MassiveJS](https://massivejs.org). There's a lot to explain about how it works, so I'll just cut to the reasons I like it:

- It's a dedicated Postgres tool that flexes the amazing features of Postgres and
- It has built-in support for JSONB. We like this.

The idea here is that we can start using JSONB, saving our models as documents and then, if we want, we can move to using a relational structure with very few code changes. Best of both worlds!

## Booting MassiveJS In An Express App

Finally, some code! When MassiveJS boots up it scans your database and reads in your table information - column names, keys, etc. This is an asynchronous process which means we need to do change the way our app is booting.

I like to have everything in an \``app.js` file so it's right in front of me, and I set everything inside of an `async start` function:

```js
var createError = require("http-errors");
var express = require("express");
var path = require("path");
var cookieParser = require("cookie-parser");
var logger = require("morgan");
var session = require("express-session");
var http = require("http");
const bodyParser = require("body-parser");
const Massive = require("massive");

const start = async function () {
  var app = express();

  const db = await Massive(process.env.DATABASE_URL);
  app.set("db", db);
  //...
};

start.then((app) => {
  const port = app.get("port");
  console.log(`App is running on port ${port}`);
});
```

This allows us to `await` any boot up stuff that is asynchronous - like Massive.

By default, the server boot stuff in Express is in `bin/www` but I like the app bits right in front of me so I know what's happening.

The App Stuff, however, goes in it's own boot file.

## The Boot File

A practice in Sinatra land (Ruby) is to have a `config/boot` file where everything is loaded up. This can be a single file or, if you're kicking up a lot of stuff, multiple files.

I decided to do a single boot file `config/boot.js` and in there I do _app specific_ initialization. Not the plumbing of Express or Express middleware - the things I care about with the app itself:

```js
const Auth = require("../lib/auth");
const Mail = require("../mail");
const Passport = require("./passport");
const Massive = require("massive");
const consola = require("consola");
const settings = require("../package.json");
require("dotenv").config();

exports.theApp = async function (app) {
  let rootUrl = `http://localhost:${app.get("port")}`;
  if (process.env.NODE_ENV === "production" && settings.azure) {
    rootUrl = settings.azure.siteUrl;
  }

  //set the root URL for use throughout the app
  app.set("rootUrl", rootUrl);

  consola.info(`Connecting to ${process.env.DATABASE_URL}`);

  //spin up massive... yay!
  const db = await Massive(process.env.DATABASE_URL);
  app.set("db", db);

  consola.info("Initializing Auth service...");
  Auth.init({ db: db });

  consola.info("Initializing Passport service...");
  let passport = Passport.init({
    Auth: Auth,
    GoogleSettings: {
      clientID: process.env.GOOGLE_ID,
      clientSecret: process.env.GOOGLE_SECRET,
      callbackUrl: `${rootUrl}/auth/google/callback`,
    },
    GithubSettings: {
      clientID: process.env.GITHUB_ID,
      clientSecret: process.env.GITHUB_SECRET,
      callbakUrl: `${rootUrl}/auth/github/callback`,
      scope: ["user:email"],
    },
  });

  consola.info("Initializing email...");
  Mail.init({
    host: process.env.SMTP_HOST,
    user: process.env.SMTP_USER,
    password: process.env.SMTP_PASSWORD,
  });

  app.use(passport.initialize());
  app.use(passport.session());
};
```

There are a number of things going on here - enough so that I will definitely need a Part 4 (so I can explain what `settings.azure` is). Hopefully this looks like what it is: _where everything is configured_. Some people put configuration stuff inside of a module... that's OK but I think it's much easier to put it one place where you have access to your `app`, your `db` and other services.

If you squint your eyes it's almost like poor-person's IoC, where you initialize your stuff in a central spot using environment variables instead of some wonky XML configuration or other nonsense.

So there's a lot going on here, for now let's focus on `Auth`. Once Massive is instantiated, you'll notice that I'm passing that instance to an `Auth` module. This is because Node's modules are singletons by default, and that's exactly what we want because Massive creates a connection pool to Postgres. If we accidentally create multiple instances of Massive, we'll have multiple pools which can cause our app to crash as well as make Massive complain constantly.

I know there will always be only one instance because I can see it right here in my config. If other classes or services need it, I'll pass it on on boot. I'll get more into this below.

## Authentication, Built In

![](https://i1.wp.com/bigmachine.io/img/2020/04/shot_183.jpg?fit=640%2C381&ssl=1)

The Tailwind Starter CSS Kit from Creative Tim

I wanted a specific use case for working with PostgreSQL on Azure and that grew into something I've wanted for a really long time: **a super simple starter site without all the cruft and noise**. I'm a little bit opinionated on things and I like it when an application design is as simple and straightforward as possible with room to grow.

As you can see above, I'm using [PassportJS](http://www.passportjs.org/) to handle OAuth. There's enough going on with Passport that I decided to give it its own boot file, which you can see in the repo.

Here's where we get to the meat of the matter, however: _where does the auth logic live?_ The simplest approach is to just drop it on a `User` model:

```js
class User {
  constructor(args) {
    //init stuff
  }
  register({ name, email, password }) {
    //...
  }
  login({ email, password }) {
    //...
  }
}
```

This seems like a straightforward thing but it's not because I need to work with the database at some point, which means I either need to `require` a database instance somewhere or pass it in through the constructor.

That's becomes a mess, fast. I could try and to an ActiveRecord type of base class, which is where I was headed before because it's simple - but (in my experience) this is the EXACT kind of emerging technical debt that I don't want to deal with. This is my opinion based on my experience, but orchestrating logic with data-aware models leads to a giant mess.

Probably because I'm not disciplined enough to find rugs to shove my code mess under - but I'd rather do something a bit cleaner.

## The Auth Service

The next logical step is to have a "service" class that deals solely with "Auth stuff", like registering, logging in, changing passwords, etc. You could spread this around multiple files or, to start, just a single file as I have.

This is where I constantly find myself when working with Node: _do I keep this module a singleton or export a class?_ I almost always go with the class option so I can pass in whatever config stuff I need to:

```js
class Auth {
  constructor(args) {
    //set things
  }
}

exports.init = function ({ db }) {
  assert(db, "Need a db instance here");
  return new Auth({ db: db });
};
```

This is a typical factory pattern where you don't allow direct access to a class's constructor and, instead, use a method that describes what you're trying to do.

This works, but it has a drawback: _you can't get to the Auth instance unless you call init()_. That's a pain! I need this service in at least three spots:

- The boot file
- The auth route file
- The passport config file

Ideally, I could just require the Auth service and it shows up and works! This is where Node's singleton thing comes in ultra handy. Instead of exporting a class instance, I'll just set some variables on the singleton:

```js
let db = null;

exports.authenticate = function ({ email, password }) {
  //... use the db in here
};

exports.register = function ({ name, email, password }) {
  //... use the db in here
};

exports.init = function (args) {
  assert(args.db, "Need a db instance here");
  db = args.db;
};
```

This works great _but only_ if you're making sure the boot file is called before any app code. But you would do that anyway, wouldn't you?

Here's what it looks like in my editor:

![](https://i1.wp.com/rob.conery.io/img/2020/04/shot_177-1.jpg?fit=640%2C346&ssl=1)

Notice that last line there circled in red? That's Massive doing it's thing with a document: `db.users.saveDoc(user)`\`. That's one of the main reasons I love working with Massive - you can start out saving your models as documents and, later on, if you want to get relational about it go ahead! I really dislike migrations, but I LOVE Postgres for it's data rules and speed. The best of both worlds.

## Next Time: Deployment

One of the things I also added to this starter site is an "in-house" deployment setup using Azure. I love the way Heroku works - cuddling up to your app and helping you seamlessly deploy it - so I added a version of that experience to this starter app.

I added [Commander](https://github.com/tj/commander.js) from TJ because I think every app - even a web app - should have a CLI. I added a bunch of commands and a few other things to streamline the deployment experience and it all worked pretty well!

![](https://bigmachine.io/img/shot_178.jpg)

You start with some Q&A, asking about where, what size web and DB servers, and your configuration is set for you in your package.json:

![](https://i0.wp.com/rob.conery.io/img/2020/04/shot_180.jpg?fit=640%2C380&ssl=1)

There are no passwords in here - they're stored (for now) in a local DB in the root of the CLI which DOES NOT get committed. You can view everything, if you want, using a simple CLI command:

![](https://blog.bigmachine.io/img/shot_179-1.jpg)

All of your deployment users and passwords are generated for you, as well as you app's name, app servic plan name and so on. Again - I'll blog more about this in the next post but the idea here is to make this as seamless and simple as possible.

And guess what! It works pretty well...

![](https://i1.wp.com/rob.conery.io/img/2020/04/shot_181.jpg?fit=640%2C280&ssl=1)

As you can see in the output, a remote git repo is setup for you locally so you can push/deploy using Git. A resource group, service plan, app settings, database along with database settings and even initialization with a users table - it's all done and ready.

Within 8 minutes you can browse your site or, if you want, explore your production database using `psql` locally.

I threw in a bunch more stuff too - things that I've always wanted at the ready when working with a cloud provider (like logging, open my site for me please, back up my db here please, etc). I'll share more in the next post.

## Summary

This post had less to do with Postgres I suppose - more with how you work with it.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2020/04/shot_182-1.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2020/04/shot_182-1.jpg" />
  </entry>
  <entry>
    <title>Postgres For Those Who Can’t Even, Part 2 - Working with Node and JSON</title>
    <link href="https://bigmachine.io/posts/postgres-for-those-who-cant-even-part-2-working-with-node-and-json" rel="alternate" type="text/html"/>
    <updated>2020-02-05T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/postgres-for-those-who-cant-even-part-2-working-with-node-and-json</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2020/02/burke_2.jpg" alt="Postgres For Those Who Can’t Even, Part 2 - Working with Node and JSON" /></p>
This is part 2 of a series of posts I’m doing for a friend who’s a JavaScript developer that, according to him, knows next to nothing about Postgres. [You can read part 1 right here](/2020/01/24/postgresql-for-those-who-cant-even-part-1/).

I [write a lot about Postgres](/category/postgres/), but I don’t think I’ve written enough about how to get started from the absolute beginning, so that’s what we’re doing here.

In this post, I’m continuing with his questions to me - but this time it has less to do with the database side of things and more to do with Node and how you can use Postgres for fun and profit. Let’s roll.

## How should I structure my code?

This question has more to do with your preferences or what your company/boss have set up. I can show you how _I_ do things, but your situation is probably a lot different.

OK, enough prevaricating. Here’s what I’ve done in the past with super simple projects that where I’m just musing around.

### Give PG It’s Own Module

I like putting all my code inside of a `lib` directory, and then inside there I’ll create a a `pg` directory with specific connection things etc for Postgres. It looks like this:

![](https://bigmachine.io/img/shot_49.jpg)

You’ll also notice I have a `.env` file, which is something that goes into every single project of mine. It’s a file that holds environmental variables that I’ll be using in my project. In this case, I _do not want_ my connection string hardcoded anywhere - so I pop it into a `.env` file where it’s loaded automatically by my shell (zshell and, for those interested, I use the `dotenv` plugin with [Oh-My-Zsh)](https://ohmyz.sh/).

There’s a single file inside of the `lib/pg` directory called `runner.js`, and it has one job: _run the raw SQL queries_ using pg-promise:

```js
const pgp = require("pg-promise")({});
const db = pgp(process.env.DATABASE_URL);

exports.query = async function (sql, args) {
  const res = await db.any(sql, args);
  return res;
};
exports.one = async function (sql, args) {
  const res = await db.oneOrNone(sql, args);
  return res;
};
exports.execute = async function (sql, args) {
  const res = await db.none(sql, args);
  return res;
};
exports.close = async function () {
  await db.$pool.end();
  return true;
};
```

I usually have 3 flavors of query runners:

- One that will return 0 to _n_ records
- One that will return a single record
- One that executes a “passthrough” query that doesn’t return a result

I also like to have one that closes the connections down. Normally you wouldn’t call this in your code because the driver (which is pg-promise in this case) manages this for you and you want to be sure you draw on its pool of connections - don’t spin your own. That said, sometimes you might want to run a script or two, maybe some integration tests might hit the DB - either way a graceful shutdown is nice to have.

We can use this code in the rest of our app:

```js
const pg = require("./lib/pg/runner");

pg.query("select * from master_plan limit 10")
  .then(console.log)
  .catch(console.error)
  .finally(pg.close);
```

Neat! It works well but yes, we’ll end up with SQL all over our code so let’s fix that.

### A Little Bit of Abstraction

The nice thing about Node is that your modules can be single files, or you can expand them to be quite complex - without breaking the code that depends on them. I don’t want my app code to think about the SQL that needs to be written - I’d rather just offer a method that gives the data I want. In that case, I’ll create an `index.js` file for my `pg` module, which returns a single method for my query called `masterPlan`:

```js
const runner = require("./runner");
exports.masterPlan = function (limit = 10) {
  return runner.query(`select * from master_plan limit ${limit}`);
};
exports.shutDown = function () {
  runner.close();
};
```

The `runner` here is the same runner that I used before, this time it’s in the same directory as the calling code. I’ve exposed two methods on the index as that’s all I need for right now. This is kind of like a [Repository Pattern](https://dev.to/kylegalbraith/getting-familiar-with-the-awesome-repository-pattern--1ao3), which comes with a few warnings attached.

People have been arguing about data access for decades. What patterns to use, how those patterns fit into the larger app you’re building, etc, etc, etc. It’s really annoying.

_Applications always start small_ and then grow. That’s where the issues come in. The Repository Pattern looks nice and seems wonderful until you find yourself writing `Orders.getByCustomer` and `Customer.getOrders`, wondering if this is really what you wanted to do with your life.

This is a rabbit hole I don’t want to go down further so, I’ll kindly suggest that if you have a simple app with 10-20 total queries, this level of control and simplicity of approach might work really well. If your app will grow (which I’m sure it will whether you think so or not), it’s probably a good idea to use some kind of library or relational mapper (ORM), which I’ll get to in just a minute.

## How do I put JSON in it?

One of the fun things about Node is that you can work with JSON everywhere. It’s fun, I think, to not worry about data types, migrations, and relational theory when you’re trying to get your app off the ground.

The neat thing about Postgres is that it supports this and it’s blazing fast. Let’s see how you can set this up with Postgres.

### Saving a JSONB Document

Postgres has native support for binary JSON using a datatype called “JSONB”. It behaves just like JSON but you can’t have duplicate keys. It’s also super fast because you can index it in a variety of ways.

Since we’re going to store our data in a JSONB field, we can create a “meta” table in Postgres that will hold that data. All we need is a primary key, a timestamp and the field to hold the JSON:

```js
create table my_document_table(
  id serial primary key,
  doc jsonb not null,
  created_at timestamp not null default now()
);
```

We can now save data to it using a query like this:

```js
insert into my_document_table(doc)
values('{"name":"Burke Holland"}');
```

And _yuck_. Why would anyone want to do something like this? Writing delimited JSON by hand is gross, let’s be good programmers and wrap this in a function:

```js
const runner = require("./runner");
//in pg/index.js
exports.saveDocument = async function (doc) {
  const sql = "insert into my_document_table (doc) values ($1)";
  const res = await runner.one(sql, [doc]);
  return res;
};
```

This works really well, primarily because our Node driver (pg-promise) understands how to translate JavaScript objects into something Postgres can deal with. We just pass that in as an argument.

But we can do better than this, don’t you think?

### Sprinkling Some Magical Abstraction

One of the cool things about using a NoSQL system is that you can create a document table on the fly. We can do that easily with Postgres but we just need to tweak our `saveDocument` function a bit. In fact we need to _tweak a lot of things_.

Let’s be good programmers and create a brand new file called `jsonb.js` inside our `pg` directory, right next to our `runner.js` file. The first thing we’ll do is to create a way to save _any_ document and, if we get an error about a table not existing, we’ll create it on the fly!

```js
exports.save = async function (tableName, doc) {
  const sql = `insert into ${tableName} (doc) values ($1) returning *`;
  try {
    const newDoc = await runner.one(sql, [doc]);
    doc.id = newDoc.id;
    return doc;
  } catch (err) {
    if (err.message.indexOf("does not exist") > 0) {
      //create the table on the fly
      await this.createDocTable(tableName);
      return this.save(tableName, doc);
    }
  }
};

exports.createDocTable = async function (tableName) {
  await runner.query(`
    create table ${tableName}(
    id serial primary key,
    doc jsonb not null,
    created_at timestamp not null default now()
  )`);
  await runner.query(`
    create index idx_json_${tableName} 
    on ${tableName} 
    USING GIN (doc jsonb_path_ops)
  `);
};
```

We have two groovy functions that we can use to save a document to Postgres with the sweetness of a typical NoSQL, friction-free experience. A few things to note about this code:

- We’re catching a specific error when a table doesn’t exist in the database. There’s probably a better way to do that, so feel free to play around. If there’s an error, we’re creating the table and then calling the `save` function one more time.
- The `createDocTable` function also pops an index on the table which uses `jsonb_path_ops`. That argument tells Postgres to index _every key_ in the document. This might not be what you want, but indexing is a good thing for smaller documents.
- We’re using a fun clause at the end of our `insert` SQL statement, specifically `returning *` which will return the entire, newly-created record, which we can then pass on to our calling code.

Let’s see if it works!

```js
//index.js of our project
docs
  .save("customers", { name: "Mavis", email: "mavis@test.com" })
  .then(console.log)
  .catch(console.err)
  .finally(pg.shutDown);
```

![](https://blog.bigmachine.io/img/shot_51.jpg)

Well look at that would ya! It works a treat.

But what about updates and deletes? Deleting a document is a simple SQL statement:

```js
exports.delete = async function (id) {
  const sql = `delete from ${tableName} where id=$1`;
  await runner.execute(sql, [id]);
  return true;
};
```

You can decide what to return from here if you want, I’m just returning `true`. Updating is a different matter, however.

### Updating an existing JSONB document

One of the problems with JSONB and Postgres in the past (< 9.5) was that in order to update a document you had to wholesale update it - a “partial” update wasn’t possible. With Postgres 9.5 that changed with the `jsonb_set` method, which requires a key and a JSONB element.

So, if we wanted to change Mavis’s email address, we could use this SQL statement:

```sql
update customers
set doc = jsonb_set(doc, '{"email"}', '"mavis@example.com"')
where id = 1;
```

That syntax is weird, don’t you think? I do. It’s just not very intuitive as you need to pass an “array literal” to define the key and a string value as the new value.

To me it’s simpler to just concatenate a new value and do a wholesale save. It’s nice to know that a partial update is _possible_ if you need it, but overall I’ve never had a problem just running a complete update like this:

```js
exports.modify = async function (tableName, id = 0, update = {}) {
  if (!tableName) return;
  const sql = `update customers SET
              doc = (doc || $1) 
              where id = $2 returning *; `;
  const res = await runner.one(sql, [update, id]);
  return res;
};
```

The `||` operator that you see there is the JSONB concatenation operator which will update an existing key in a document or add one if it’s not there. Give it a shot! See if it updates as you expect.

### Querying a JSONB document by ID

This is the nice thing about using a relational system like Postgres: querying by `id` _is just a simple SQL statement._ Let’s create a new function for our `jsonb` module called `get`, which will return a document by ID:

```js
exports.get = async function (tableName, id = 0) {
  const sql = `select * from ${tableName} where id=$1`;
  const record = await runner.one(sql, [id]);
  const doc = record.doc;
  doc.id = record.id;
  return doc;
};
```

Simple enough! You’ll notice that i’m adding the `id` of the row in Postgres to the document itself. I could drop that into the document itself, if I wanted, but it’s simple enough to tack it on as you see. In fact, I think I’d like to ensure the `created_at` timestamp is on too, so let’s formalize this with some transformations:

```js
const transformRecord = function (record) {
  if (record) {
    const doc = record.doc;
    doc.createdAt = record.created_at;
    doc.id = record.id;
    return doc;
  } else {
    return null;
  }
};
const transformSet = function (res) {
  if (res === null || res === []) return res;
  const out = [];
  for (let record of res) {
    const doc = transformRecord(record);
    out.push(doc);
  }
  return out;
};
```

This will take the raw record from Postgres and turn it into something a bit more usable.

### Querying a document using criteria

We can pull data out of our database using an id, but we need another way to query if we’re going to use this properly.

You can query documents in Postgres using a special operator: `@>`. There are other operators, but this is the one we’ll need for 1) querying specific keys and 2) making sure we use an index. There are all kinds of operators and functions for JSONB within Postgres and you can [read more about them here](https://www.postgresql.org/docs/12/functions-json.html).

To query a document for a given key, you can do something like this:

```sql
select * from customers
where doc @> '{"name":"Burke Holland"}';
```

This query is simply for documents where the key/value `{name:"Burke Holland"}` exists. That critieria is simply JSON, which means we can pass that right through to our driver… and behold:

```js
exports.find = async function (tableName, criteria) {
  const sql = `select * from ${tableName} where doc @> $1`;
  const record = await runner.query(sql, [criteria]);
  return transformSet(record);
};
```

Let’s run this and see if it works:

```js
docs
  .find("customers", { email: "mavis@test.com" })
  .then(console.log)
  .catch(console.err)
  .finally(pg.shutDown);
```

![](https://blog.bigmachine.io/img/shot_52.jpg)

Hey that’s pretty swell! You don’t need to use dedicated JSON operators to query a JSONB document in Postgres. If you’re comfortable with SQL, you can just execute a regular old query and it works just fine:

```sql
select * from customers
where (doc ->> 'name') ilike 'Mav%'
```

Here, we’re pulling the `name` key from the document using the JSON text selector (`->>`), and then doing a fuzzy comparison using `ilike` (case-insensitive comparison). This works pretty well but it can’t use the index we setup and that might make your DBA mad.

That doesn’t mean you can’t index it - you can!

```sql
create index idx_customer_name
on customers((doc ->> 'name'));
```

Works just like any other index!

### Play around, have some fun…

I [made a gist](https://gist.github.com/robconery/93aaec861fdf0cf3f5ff3f30f1cf11d5) out of all of this if you want to goof around. There are things to add, like updates/partial updates, and I encourage you to play and have a good time.

If you’re wondering, however, if someone, somewhere, might have baked this stuff into a toolset… indeed! They did…

## Are there any ORM-like tools in it? What do you recommend?

So here’s the thing: if you’re coming to this post from a Java/C#/Enterprise-y background, the “ORM” tools in the Node world are going to look … well a bit different. I don’t know the reason why and I could pontificate about Node in the enterprise or how Node’s moduling system pushes the idea of isolation… but… let’s just skip all of that OK?

The bottom line is this: you can do data access with Node, but if you’re looking for an industrial strength thing to rival Entity Framework you might be dissapointed. With that said - let’s have a look…

### My favorite: MassiveJS

I am 100% completely biased when it comes to [MassiveJS](https://massivejs.org/) because… well _I created it_ along with my friend [Karl Seguin](https://www.openmymind.net/) back in 2011 or so. The idea was to build a simple data access tool that would help you avoid writing too much SQL. It morphed into something much, much fun.

With version 2 I devoted Massive to Postgres completely and was joined by the current owner of the project, [Dian Fay](https://di.nmfay.com/about). I can’t say enough good things about Dian - she’s amazing at every level and has turned this little project into something quite rad. Devoting Massive 100% to Postgres freed us up to do all kinds of cool things - including one of the things I love most: [document storage](https://massivejs.org/docs/working-with-documents).

The code you read above was inspired by the work we did with JSONB and Massive. You can have a fully-functioning document storage solution that kicks MongoDB in the face in terms of speed, fuzzy searches, full-text indexing, ACID guarantees and a whole lot more. Massive gives you the same, simple document API and frictionless experience you get with Mongo with a much better database engine underneath.

To work with Massive, you create an instance of your database which reads in all of your tables and then allows you to query them as if they were properties (the examples below are taken from the documentation):

```js
const massive = require("massive");

const db = await massive({
  host: "localhost",
  port: 5432,
  database: "appdb",
  user: "appuser",
  password: "apppwd",
  ssl: false,
  poolSize: 10,
});

//save will update or insert based on the presence of an
//ID field
let test = await db.tests.save({
  version: 1,
  name: "homepage",
});

// retrieve active tests 21-30
const tests = await db.tests.find(
  { is_active: true },
  {
    offset: 20,
    limit: 10,
  }
);
```

[Working with documents](https://massivejs.org/docs/working-with-documents) looks much the same as the relational stuff above, but it’s stored as JSON:

```js
const report = await db.saveDoc("reports", {
  title: "Week 12 Throughput",
  lines: [
    {
      name: "1 East",
      numbers: [5, 4, 6, 6, 4],
    },
    {
      name: "2 East",
      numbers: [4, 4, 4, 3, 7],
    },
  ],
});
```

Finally, the thing I love most about the project is what Dian has done with the documentation (linked above). She goes into detail about every aspect of the tool - even [how to use it with popular web frameworks](https://massivejs.org/docs/framework-examples).

### Sequelize

One of the more popular data access tools - let’s call it a full on ORM - is [Sequelize](https://sequelize.org/v5/). This tool is a traditional ORM in every sense in that it allows you create classes and save them to multiple different [storage engines](https://sequelize.org/v5/manual/dialects.html), including Postgres, MySQL/MariaDB SQLite and SQL Server. It's _kind of not an ORM_ though because there is no mapping (the "M") that you can do aside from a direct 1:1, ActiveRecord style. For that, you can project what you need using `map` and I'll just leave that discussion right there.

If you’ve used ActiveRecord (Rails or the pattern itself) before then you’ll probably feel really comfortable with Sequelize. I used it once on a project and found its use straightforward and simple to understand. [Getting started](https://sequelize.org/v5/manual/getting-started.html) was also straightforward, as with any ORM, and the only question is how well an ActiveRecord pattern fits your project's needs **both now and into the future**. That's for you to decide and this is where I hit the architectural eject button again (even though I did once before which didn't seem to work).

Let’s have a look at some of the examples that come from the documentation.

Connecting is straightforward:

```js
const Sequelize = require("sequelize");
const sequelize = new Sequelize("postgres://user:pass@example.com:5432/dbname");
```

Declaring a “model” in Sequelize is matter of creating a class and extending from `Sequelize.Model` or using a built-in definition method. I prefer the latter:

```js
const User = sequelize.define(
  "user",
  {
    // attributes
    firstName: {
      type: Sequelize.STRING,
      allowNull: false,
    },
    lastName: {
      type: Sequelize.STRING,
      // allowNull defaults to true
    },
  },
  {
    // options
  }
);
```

Sequelize is capable of using this model definition and generating, or "sychronizing" your database just like Django's ORM does. That's really helpful in the early days of your project or if you just hate migrations as much as I do.

Sequelize is an outstanding data tool that allows you to work with your database in a seamless way. It has powerful queries and can handle some pretty intense filtering:

```js
Project.findOne({
  where: {
    name: "a project",
    [Op.not]: [{ id: [1, 2, 3] }, { array: { [Op.contains]: [3, 4, 5] } }],
  },
});
```

If you’ve worked with Rails and ActiveRecord Sequelize should feel familiar when it comes to associations, hooks and scopes:

```js
class User extends Model {}
User.init(
  {
    name: Sequelize.STRING,
    email: Sequelize.STRING,
  },
  {
    hooks: {
      beforeValidate: (user, options) => {
        user.mood = "happy";
      },
      afterValidate: (user, options) => {
        user.username = "Toni";
      },
    },
    sequelize,
    modelName: "user",
  }
);
class Project extends Model {}
Project.init(
  { name: Sequelize.STRING },
  {
    scopes: {
      deleted: {
        where: {
          deleted: true,
        },
      },
      sequelize,
      modelName: "project",
    },
  }
);
User.hasOne(Project);
```

And there you have it. The documentation for Sequelize is very complete as well, with examples and SQL translations so you know what query will be produced for every call.

### But what about…?

There are so many tools out there that can help you with Node and data access and I’m sure I’ve left a few off, so feel free to add your favorite in the comments. Please be sure it works with Postgres AND please be sure to indicate why you like it!

## Postgres is neat and all but how do I deploy my database?

Great question! That will have to be a topic for Part 3, unfortunately as this post is quite long and I have a lot of ideas. We’ll go simple and low fidelity with a simple docker container push, and then look at some of the hosted, industrial strength solutions out there - including [Azure’s Managed Postgres offering!](https://azure.microsoft.com/en-us/services/postgresql/)]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2020/02/burke_2.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2020/02/burke_2.jpg" />
  </entry>
  <entry>
    <title>PostgreSQL For Those Who Can’t Even, Part 1</title>
    <link href="https://bigmachine.io/posts/postgresql-for-those-who-cant-even-part-1" rel="alternate" type="text/html"/>
    <updated>2020-01-24T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/postgresql-for-those-who-cant-even-part-1</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2020/01/burke.jpg" alt="PostgreSQL For Those Who Can’t Even, Part 1" /></p>
Just yesterday I was talking to a friend about Postgres (not uncommon) and he said something that I found _shocking_:

> I can't even with Postgres, I know JACK SQUAT

This person calls themself my _friend_ too! I just don’t even know what’s real anymore.

So, **Friendo** is a Node person who enjoys using a document database. Can’t blame him - it’s easy to setup, easy to run and you don’t need to stress out about SQL and relational theory. That said, there are benefits to wrapping structure and rules around your data - it _is_ the lifeblood of your business after all.

If you’re like Friendo and you want to start from the very beginning with Postgres, read on! I’ll use his questions to me for the rest of this post. He has _a lot_ of questions, so I'm going to break this up into parts:

- Part 1 (this post) is for people who've never thought about a database before, let alone set one up and run a query
- Part 2 (next post) will be for Node people wondering what/why/how they could work with Postgres

I encourage you to play along if you're curious. If you're having fun and want to do more, [I wrote a really fun book](https://bigmachine.io/products/a-curious-moon/) about Postgres and the data from the Cassini mission (which you'll see below) that you're welcome to check out too!

## Where is Postgres? How do I get it and run it?

The easiest possible thing you can do is to run a docker image, which you can do by executing:

```
docker run -p 5432:5432 postgres:12.1
```

That will download and run a Postgres image, exposing the default Postgres port of 5432.

If you’re not a Docker person and are on a Mac, you can also [head over to postgresapp.com](https://postgresapp.com) where you can download a free executable app.

## How do I manage it with a tool?

Tooling for Postgres is both abundant and wanting. There is no clear cut answer to this question other than to offer the following options for a given context.

**Just playing around: Mac** If you’re on a Mac go get yourself a free copy of [Postico](https://eggerapps.at/postico/). It’s easy and you can quickly connect and start playing.

![](https://bigmachine.io/img/s_1FC55FF691E3C173A43C1C315DD0B563BE10884F81292ABAC9C59C8E67BDDA03_1579124251449_table-content-view.png)

**Just playing around: Windows (and Mac)**

There’s the free [Azure Data Studio](https://docs.microsoft.com/en-us/sql/azure-data-studio/download-azure-data-studio?view=sql-server-ver15) which uses the same interface as VS Code. There are extensions and all kinds of goodies you can download if you want as well.

To hook up to Postgres, make sure you grab the [Postgres extension](https://docs.microsoft.com/en-us/sql/azure-data-studio/postgres-extension?view=sql-server-ver15). You can install it right from the IDE by clicking on the square thingies in the bottom left of the left-most pane.

![](https://blog.bigmachine.io/img/s_1FC55FF691E3C173A43C1C315DD0B563BE10884F81292ABAC9C59C8E67BDDA03_1579124323609_image.png)

**Something substantial and you’re willing to pay for it (Windows and Mac)** My go-to tool for working with Postgres is [Navicat](https://www.navicat.com/en/products/navicat-for-postgresql). It’s a bit on the spendy side but you can do all kinds of cool things, including reports, charting, import/export, data modeling and more. I love this thing.

![](https://blog.bigmachine.io/img/s_1FC55FF691E3C173A43C1C315DD0B563BE10884F81292ABAC9C59C8E67BDDA03_1579124634184_image.png)

Don’t know what to choose? Just download **Azure Data Studio** and let’s get to work!

**Our first login** Let’s connect to our new shiny Postgres server. Open up Azure Data Studio and make sure you have the Postgres extension installed. You’ll know if you do because you’ll see the option to connect to PostgreSQL in the connection dialog:

![](https://blog.bigmachine.io/img/s_1FC55FF691E3C173A43C1C315DD0B563BE10884F81292ABAC9C59C8E67BDDA03_1579209401730_shot_05.jpg)

The server name is “localhost” and the Docker image comes with the login preset - “postgres” as the user name and “postgres” as the password.

We’ll go with the default database and, finally, name our connection “Local Docker”. Click “Connect” and you’re good to go.

**Our first database** Most GUI tools have some way of creating a database right through the UI. Azure Data Studio doesn’t (for Postgres at least) but that’s OK, we’ll create one for ourselves.

If you’ve connected already, you might be wondering “what, exactly, am I connected to”? Good question Friendo! You’re connected to the default database, “postgres”:

![](https://blog.bigmachine.io/img/s_1FC55FF691E3C173A43C1C315DD0B563BE10884F81292ABAC9C59C8E67BDDA03_1579209663733_shot_06.jpg)

This is the admin playground, where you can do DBA stuff and feel rad. We’re going to use our connection to this database to create another one, where we’re going to drop some data. To do that, we need to write a new query. Click that button that says “New Query”:

![](https://blog.bigmachine.io/img/s_1FC55FF691E3C173A43C1C315DD0B563BE10884F81292ABAC9C59C8E67BDDA03_1579209798287_shot_07.jpg)

In the new query window add the following:

```
create database cassini;
```

Now hit “F5” to run the query. You should see a success message like so:

![](https://blog.bigmachine.io/img/s_1FC55FF691E3C173A43C1C315DD0B563BE10884F81292ABAC9C59C8E67BDDA03_1579209996613_shot_08.jpg)

If you see a syntax error, check your SQL code and make sure there are no errors. You’ll also notice that nothing changed in the left information pane - there’s no “cassini” database! What gives!

Ease up Friendo! Just right click on the “Databases” folder and refresh - you should see your new database. Once you see, double-click it and in we go!

**Our first table** Our database is going to hold some fun information from the Cassini Mission, the probe that we sent to Saturn back in 1997. All of the data generated by the project is public domain, and it’s pretty fun to use _that data_ rather then some silly blog posts don’t ya think?

There’s [a whole lot of data](https://pds-atmospheres.nmsu.edu/data_and_services/atmospheres_data/Cassini/Cassini.html) you can download, but let’s keep things reasonable and go with the “Master Plan” - the dates, times and descriptions of everything Cassini did during it’s 20 year mission to Saturn. I trimmed it just a bit to bring the file size down, so if you want to play along you can [download the CSV from here](https://www.dropbox.com/s/fno2olahpdoh3r7/master_plan.csv?dl=0).

![](https://blog.bigmachine.io/img/s_1FC55FF691E3C173A43C1C315DD0B563BE10884F81292ABAC9C59C8E67BDDA03_1579210762545_shot_09.jpg)

We’ll load this gorgeous data in just one second. We have to create a table for it first! Let’s do that now by opening a new query window in Azure Data Explorer (which I hope you remember how to do). Make sure you’re connected to the “cassini” database, and then enter the following SQL:

```sql
create table master_plan(
  date text,
  team text,
  target text,
  title text,
  description text
);
```

This command will, as you might be able to guess, create a table called “master_plan”. A few things to note:

- Postgres likes things in lower case and will do it for you unless you force it to do otherwise, which we won’t.
- We don’t have a primary key defined, this is intentional and you’ll see why in a second.
- There are a number of ways to store strings in Postgres, but the simplest is `text`, without a length description. This is counterintuitive for people coming from other databases who think this will take up space. It won’t, Postgres is much smarter than that.
- Why are we storing a field called “date” as `text`? For a very good reason which I’ll go over in just a minute.

OK, run this and we should have a table. Let’s load some data!

## How do I load data into it?

We’re going to load data directly from a CSV, which Postgres can do using the `COPY` command. For this to work properly, however, we need to be sure of a few things:

- We need to have the absolute path to the CSV file.
- The structure of the file needs to match the structure of our table.
- The data types need to match, in terms of format, the data types of our table.

That last bit is the toughest part. CSV (and spreadsheets in general) tend to be a minefield of poorly chewed data-droppings, mostly because spreadsheet programs suck at enforcing data rules.

We have two ways to get around this: suffer the pain and correct the data when we import it or **make sure all the import columns in our database table are** `**text**`. The latter is the easiest because correcting the data using database queries tends to be easier than editing a CSV file, so that’s what we’ll do. Also: i*t’s a good idea not to edit the source of an import.*

Right - let’s get to it! If you’re running Docker you’ll need to copy the `master_plan` CSV file into your running container. I put my file in my home directory on my host. If you’ve done the same, you can use this command to copy the file into your container:

```
docker cp ~/master_plan.csv [CONTAINER ID]:master_plan.csv
```

Once it’s there, you can execute the `COPY` command to push data into the `master_plan` table:

```sql
COPY master_plan
FROM '/master_plan.csv'
WITH DELIMITER ',' HEADER CSV;
```

This command will grab the CSV file from our container’s root directory (as that’s where we copied it) and pop the data in positionally into our table. We just have to be sure that the columns align, which they do!

The last line specifies our delimiter (which is a comma) and that there are column headers. The final bit tells Postgres this is a CSV file.

Let’s make sure the data is there and looks right. Right-click on the table and select “Select top 1000 rows” and you should see something like this:

![](https://blog.bigmachine.io/img/s_1FC55FF691E3C173A43C1C315DD0B563BE10884F81292ABAC9C59C8E67BDDA03_1579725726323_shot_24.jpg)

Yay data! Before we do anything else, let’s add a primary key so I don’t freak out:

```sql
alter table master_plan
add id serial primary key;
```

Great! Now we’re ready to connect from Node.

## How do I connect to it from Node?

Let’s keep this as simple as possible, for now. Start by creating a directory for the code we’re about to write and then initializing a Node project. Feel free to use Yarn or NPM or whatever!

Open up a terminal and:

```
mkdir pg_demo
cd pg_demo
npm init -y
npm install pg-promise
touch index.js
```

These commands should work in Powershell on Windows just fine.

We’ll be using the [promise-based Postgres driver](https://github.com/vitaly-t/pg-promise) from Vitaly Tomalev called `pg-promise`, one of my favorites. The default Node driver for Postgres works with standard callbacks, and we want promises! There are also a few enhancements that Vitaly thew in which are quite nice, but I’ll leave that for you to explore.

The first step is to require the library and connect:

```js
const pgp = require("pg-promise")({});
const db = pgp("postgres://postgres:postgres@localhost/cassini");
```

I’m connecting to Postgres using a URL-based connection string that has the format:

```js
postgres://user:password@server/db_name
```

Since we’re using Docker, our default username and password is “postgres”. You can, of course, change that as needed.

Once we’ve set up the connection, let’s execute a query using some very simple SQL:

```js
const query = async () => {
  const res = await db.any("select * from master_plan limit 10");
  return res;
};
```

Because pg-promise is promise-based, I can use the `async` and `await` keywords to run a simple query. `db.any` will return a list of results and all I need to do is to pass in a SQL string, as you see i did. I made sure to `limit` the results to 10 because I don’t want all 60,000 records bounding back at me.

To execute the query, I call the method and handle the returned promise. I’ll pop the result out to the console:

```js
query()
  .then((res) => {
    console.log(res);
  })
  .catch((err) => {
    console.error(err);
  })
  .finally(() => {
    db.$pool.end();
  });
```

The last line in the `finally` block closes off the default connection pool, which isn’t required but the Node process won’t terminate unless you do (you’ll have to ctrl-c to stop it otherwise).

You can run the file using `node index.js` from the terminal, and you should see something like this:

![](https://blog.bigmachine.io/img/s_1FC55FF691E3C173A43C1C315DD0B563BE10884F81292ABAC9C59C8E67BDDA03_1579728450821_shot_25.jpg)

Glorious data! Notice it all comes back in lovely, formatted JSON, just as we like.

There’s a lot more we can do, but this post is already quite long and I think Friendo might have a few more questions for me. I’ll see if he does and I’ll follow up next time!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2020/01/burke.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2020/01/burke.jpg" />
  </entry>
  <entry>
    <title>Fine Tuning Full Text Search with PostgreSQL 12</title>
    <link href="https://bigmachine.io/posts/fine-tuning-full-text-search-with-postgresql-12" rel="alternate" type="text/html"/>
    <updated>2019-10-29T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/fine-tuning-full-text-search-with-postgresql-12</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2019/10/screenshot_205.jpg" alt="Fine Tuning Full Text Search with PostgreSQL 12" /></p>
I've [written about Full Text Indexing in PostgreSQL](/2018/07/23/setting-up-a-fast-comprehensive-search-routine-with-postgresql/) before but I was a bit more focused on speed and general use. Today I want to focus on something a lot more useful: _**relevance**_.

If you want to play along and have some fun, the SQL for what I'm about to do can be [downloaded from here](/img/2019/10/ndc_syndney.zip) (11K zipped SQL file).

## Make Those Results Meaningful!

The data I'm working with is from NDC Sydney and is a list of speakers, their talks, keywords and the time/day of the talk. Simple stuff, but it does present an interesting question:

_**How would you implement a full text index for this body of data?**_

Turning the speaker's name, the title and the keywords into a blob of text and then indexing it _will_ work, but it's **simply not enough** if we expect the results to actually mean something to our users. This is where things get complicated - which also means they get FUN so **strap yourself in, let's get TEXTUAL.**

## What Are You Looking For Anyway?

There's no way we can do this without fully understanding what our users want out of our search functionality, so let's come up with some scenarios:

- Jane has been to 5 conferences already this year and just wants to know what's new with DevOps and Azure. She takes out her phone and, while walking, enters the words as they come to her mind: "**devops azure**"
- This is Kunjan's first conference and he doesn't know where to start - all he knows is that [Heather](https://quorralayne.com) [Downing](https://quorralyne.com) is speaking and he really wants to be sure he can see her talks so he searches exactly on that: "**Heather Downing".**
- Nenne is excited about Blazor and knows the dev team is here, showing it off. She can't remember their names - just the project name - so she searches on that: "**blazor**".

## The Problems

We have three difference kinds of searches here:

- The first is _contextual_, which means that Jane knows the _topics_ she's interested in and wants to throw a list of words at our search, hoping for a ranked match.
- The second is _specific_, Kunjan wants to see a specific speaker's talk - that means we need to be sure that we can return a hit on exact part of a first or last name.
- Finally, Nenne's query is _relative_, which means she knows a term (the project name) and wants to see results relative to it.

If we're to show these people something meaningful we'll need to come up with a strategy for building our full text index. Thankfully, Postgres has the tools we need.

Let's take a quick second to (quickly) understand what goes on behind the scenes as our full text index is being created - it's really helpful when trying to debug things. Then we'll move on and create solutions for each of these problems.

## Behind the Scenes

A full text index is actually a data type in Postgres called `tsvector`. It's a weird name, but what it does is pretty simple:

```sql
select to_tsvector('english', 'nothing too tricky here');
     to_tsvector
---------------------
 'noth':1 'tricki':3
(1 row)
```

I'm using Postgres's built-in `to_tsvector` function to _tokenize_ my string of words into _lexemes_. What's a "lexeme" you ask? Hey, [good question](https://en.wikipedia.org/wiki/Lexeme)!

> A lexeme is a unit of lexical meaning that underlies a set of words that are related through inflection. It is a basic abstract unit of meaning, a unit of morphological analysis in linguistics that roughly corresponds to a set of forms taken by a single root word.
>
> Wikipedia

You can apply various _stems_ to a lexeme to create a set of different words. So `noth` in this case could be stemmed to "nothing" or "nothingness". The integers that you see in the results above are the position within the text body. The first word is "nothing" so we have a 1 and tricky is the third word. This comes in handy later on when we want to know positional information (which we will!).

Finally, you'll notice that `too` and `here` have been stripped. These are "stop words" (or noise words) and aren't indexed.

But how does all of this tokenization happen?

Postgres ships with a number of dictionaries that parse a given blob of text. If you want to raise the hood on this, you can run the `ts_parse` function to see what happens:

```sql
select * from ts_parse('default', 'nothing too tricky here');

 tokid |  token
-------+---------
     1 | nothing
    12 |
     1 | too
    12 |
     1 | tricky
    12 |
     1 | here
(7 rows)
```

The first argument to this function is the search configuration, which I'm setting to `default` as I don't want to break anything. What I get back is a list of tokens and their id. 1, for instance, is an ascii word and 12 is blank space.

You can see a lot more information if you use the `ts_debug` function, which is designed to help you if you're fiddling with the search config stuff:

```sql
select * from ts_debug('nothing too tricky here');
   alias   |   description   |  token  |  dictionaries  |  dictionary  | lexemes
-----------+-----------------+---------+----------------+--------------+----------
 asciiword | Word, all ASCII | nothing | {english_stem} | english_stem | {noth}
 blank     | Space symbols   |         | {}             |              |
 asciiword | Word, all ASCII | too     | {english_stem} | english_stem | {}
 blank     | Space symbols   |         | {}             |              |
 asciiword | Word, all ASCII | tricky  | {english_stem} | english_stem | {tricki}
 blank     | Space symbols   |         | {}             |              |
 asciiword | Word, all ASCII | here    | {english_stem} | english_stem | {}
(7 rows)
```

I think this is interesting, but **it's also academic** for our needs. Let's get back on track and setup our search index.

## Task 1: No Stems for Names!

Before we index anything, we need to consider _what_ the thing is and also _what it is not_. A little vague, but let's start with names.

Names are specific. While one could make the argument that some names might be more common in a given language, I think we can agree that's problematic. In that sense, tokenizing a name as if its English words doesn't make sense.

Heather's last name is "Downing", which could refer to what she might do to a glass of cold water after a long run or what she did to enemy planes during the war. Neither of those is the case, yet that's exactly how the tokenizer will treat her name.

That's how full text queries work in Postgres: _matching lexemes_. The `to_tsquery` function you see here simply tokenized the term given to it, applying the rules of the dictionary you specify, which is `english` in my case:

```sql
select to_tsquery('english', 'downing');
 to_tsquery
------------
 'down'
(1 row)
```

We can fix this problem by _**using a different dictionary**_. This makes perfect sense since we're don't consider names part of a language! For this, Postgres gives us the _simple dictionary_:

```sql
select to_tsquery('simple', 'downing');
 to_tsquery
------------
 'downing'
(1 row)
```

The simple dictionary doesn't create a lexeme from the token given to it - it just returns the raw word (unless it's a noise word) lower-cased. This will work perfect for indexing our names:

```sql
select to_tsvector('simple', body ->> 'name') from ndc limit 5;
       to_tsvector
-------------------------
 'alex':1 'mackey':2
 'adam':1 'furmanek':2
 'kristy':1 'sachse':2
 'downing':2 'heather':1
 'passos':2 'thiago':1
(5 rows)
```

Perfect. We'll use this when building our overall index in just a minute.

## Applying Weights to Keywords

Proper tagging is difficult to do. I'm not going to spend time on how to do that - let's just assume that you and your app have a cool set of tags you're happy with. Now comes the big question: **are those tags words**?

On one hand, it seems like the answer should be **yes**. Tags are contextual and tend to be things like "database", "career", "azure" etc. But what about the tags "virtual-machines" or "virtual-network"?

```sql
select to_tsvector('virtual-network');
                 to_tsvector
---------------------------------------------
 'network':3 'virtual':2 'virtual-network':1
(1 row)

select to_tsvector('virtual-machines');
                to_tsvector
-------------------------------------------
 'machin':3 'virtual':2 'virtual-machin':1
(1 row)
```

Both of these tags will match on the term "virtual", _no matter what it's followed by_. That means we'll get a hit on "virtual-conference", "virtual-meeting", and "virtually everything" since the word "virtually" will turn into the lexeme "virtual". That might be OK, it really depends on your tagging strategy. For me, I'll be using the simple dictionary once again because tags are specific, simple terms for this conference.

OK - now let's address the weighting. We can apply weights to our tags by using the `setweight` function in Postgres:

```sql
select setweight(to_tsvector('simple', (body ->> 'tags')),'A')
from ndc limit 5;

 'cloud':1A 'fun':2A
 'microsoft':2A 'net':1A
 'agile':1A 'design':2A 'devops':8A 'methodology':9A 'people':3A 'skills':6A 'soft':5A 'soft-skills':4A 'ux':7A
 'agile':1A 'ethics':6A 'people':2A 'skills':5A 'soft':4A 'soft-skills':3A
 'cloud':2A 'database':3A 'microsoft':4A 'net':1A
(5 rows)
```

Weighting is simply a matter of applying a letter suffix to the positional integer. As you can see, `cloud:1A` has replaced `cloud:1`. That will be used when we run our query later on.

Oh yeah - something neat to note here is that **Postgres is smart enough to take a JSONB array value and turn it into a text array** for us, on the fly, and then apply indexing :).

## Weighting Considerations

At this point we need to figure out relative weighting for the information we'll be searching. If you have only text blob your indexing, then it doesn't make sense to apply weighting - but that's rarely the case in an online app.

The thing you need to consider when weighting is what "hits" are valued more than others? Weighting doesn't affect which records will be recognized, it simply lifts those records to the top depending on how you weighted them (A through G).

I'm going to make the choice that if someone enters a tag, that should be raised to the top. Next would be someone's name (though you could argue it should be the other way around) and finally whatever was found in the title:

Given this, we can build our entire search index with something like:

```sql
select
  setweight(to_tsvector('english', (body ->> 'tags')), 'A') || ' ' ||
  setweight(to_tsvector('simple', (body ->> 'name')), 'B') || ' ' ||
  setweight(to_tsvector('english', (body ->> 'title')), 'C')::tsvector
as search_index
from ndc limit 5;
```

_**Note**: you'll notice that I'm using the `||` operator to concatenate the values together, including a space between them. If you don't do this you'll get words jammed together and crappy results._

We've applied the top weight, A, to`tags` and B to `name` with `title` coming in last with C. This is just relative ranking, which means that terms found in the keywords are ranked higher than the title, for instance. That will help Jane find her DevOps at Azure talks.

Kunjan will find Heather's talk as we're not stemming - so he won't get confused with bad results. And finally Nenne will easily find her "Blazor" talk as the name appears in the title.

The only tricky part to this is if a speaker's name appears in the title of a talk - so "Juana Blazor" might throw off the result - but there's simply no way we can know which our user might want. We _can_, however, make the decision that hits in the names should be counted higher! Which is what we did.

Let's add a generated column to our `ndc` table and test it out!

```sql
alter table ndc
add search tsvector
generated always as (
   (
   setweight(to_tsvector('english', (body ->> 'tags')), 'A') || ' ' ||
   setweight(to_tsvector('simple', (body ->> 'name')), 'B') || ' ' ||
   setweight(to_tsvector('english', (body ->> 'title')), 'C'))::tsvector
) stored;
```

This is a new feature in Postgres 12 - generated columns. They're virtual columns that are (for now) stored on disk and completely managed by Postgres. Whenever our record is updated our search index will be too!

We're now ready to start querying.

## Constructing a Proper Query

Let's start with the 3rd example first: "blazor", which in Nenne's query. This isn't a keyword match because it's not part of our tags, but it _is_ a project title which will, hopefully, appear in a title somewhere. In that case, we can run the following query just fine:

```sql
select
body ->> 'title' as title,
body ->> 'name' as name
from ndc
where search @@
to_tsquery('english', 'blazor');

-[ RECORD 1 ]--------------------------------
title | Blazor, a new framework for browser-based .NET apps
name  | Steve Sanderson
-[ RECORD 2 ]--------------------------------
title | Blazor in more depth
name  | Steve Sanderson Ryan Nowak
```

Groovy! We're using our `tsvector` field, `search`, and running a comparison with `@@` to the `to_tsquery` function. We get back some results and we can see that we have "Blazor" in the title. Great!

At that point Nenne remembers that Steve Sanderson is one of her favorite speakers, so she decides to search both "blazor" and "Sanderson":

```
ERROR:  syntax error in tsquery: "blazor sanderson"
```

**Oh no! What happened**? The short answer is that `to_tsquery` expects a single word as an argument, which seems really weird at first! I mean... **this is a full text search dude! WTF?**

The problem is that **Postgres doesn't know what you want to do with more than one word**. Is it just a collection of words? Or is it a _phrase_ which has some structure to it. The query "blazor Sanderson" doesn't mean anything to you or me, but Jane's query "Azure DevOps" could be considered a phrase, where the term "Azure" needs to come before "DevOps".

For that, we can modify our query using `plainto_tsquery`:

```sql
select
body ->> 'title' as title,
body ->> 'name' as name
from ndc
where search @@
plainto_tsquery('english', 'blazor sanderson');

-[ RECORD 1 ]------------------------------------
title | Blazor, a new framework for browser-based .NET apps
name  | Steve Sanderson
-[ RECORD 2 ]------------------------------------
title | Blazor in more depth
name  | Steve Sanderson Ryan Nowak
```

Yes! boom! That works really well. The function `plainto_tsquery` takes a plain text blob and treats it just like a bunch of words. In fact you can see exactly what it does by asking Postgres:

```sql
 select plainto_tsquery('blazor sanderson');

    plainto_tsquery
------------------------
 'blazor' & 'sanderson'
(1 row)
```

The text gets parsed into individual words, tokenized and turned into lexemes _and then_ placed into a logical AND condition. In other words: both "blazor" and "sanderson" must be in the search index.

But what about Jane's query? She wants to know what's knew with Azure DevOps:

```sql
select
body ->> 'title' as title,
body ->> 'name' as name
from ndc
where search @@
plainto_tsquery('english', 'azure devops');

-[ RECORD 1 ]-----------------------
title | Static Sites, Dynamic microservices, & Azure: How we built Microsoft Docs and Learn
name  | Dan Fernandez
-[ RECORD 2 ]-----------------------
title | DataDevOps for the Modern Data Warehouse on Microsoft Azure
name  | Lace Lofranco
```

Hmmm. Well that sort of worked in that we have two talks about Azure that also have the term "devops" in the title... however there's nothing there about the Azure DevOps product. One way that we can fix this is to send in a _phrase_ rather than a blob of words using `phraseto_tsquery`:

```
select
body ->> 'title' as title,
body ->> 'name' as name
from ndc
where search @@ phraseto_tsquery('english', 'azure devops');

(0 rows)
```

This is a bit more accurate: _there aren't any talks specifically about Azure DevOps_. The `phraseto_tsquery` function leverages the positional argument that's stored with `tsvector`, making sure that one word will appear before another. You can see this if you ask Postgres what's going on:

```
select phraseto_tsquery('azure devops');

  phraseto_tsquery
--------------------
 'azur' <-> 'devop'
```

The words are tokenized into lexemes once again, but this time there's the positional `<->` operator, indicating that "azure" must appear before "devops" in the string (the inclusive AND is implied).

OK, let's make sure that Kunjan can find Heather's talk and then we'll be done! I'll use the regular `plainto_tsquery` here since I want to be sure we match properly on name:

```
select
body ->> 'title' as title,
body ->> 'name' as name
from ndc
where search @@
plainto_tsquery('Downing');

(0 rows)
```

**Good grief - no results**!?!?! What the heck?

## Using the Right Dictionary

The problem we're having is matching dictionaries. When we use `to_tsquery` or, in this case, `plainto_tsquery,` the words we pass in will be tokenized according to some kind of dictionary. The default has to do with the location of the server and the default configuration - but it's typically set to the language of the region of the server.

In the case of our `name` tokens, however, we used the simple dictionary which means that lexemes didn't get generated and therefore will cause a match problem.

To see what I mean, take a look at our `plainto_tsquery` for "Downing" using the default dictionary (which is "english" in my case):

```
select plainto_tsquery('Downing');

 plainto_tsquery
-----------------
 'down'
(1 row)
```

**We're trying to match a literal term to a lexeme**, so of course we're going to have problems. We can get over this by using the simple dictionary with `plainto_tsquery`:

```
select
body ->> 'title' as title,
body ->> 'name' as name                                                                                                                                        from ndc                                                                                                                                                                                                 where search @@
plainto_tsquery('simple','Downing');

-[ RECORD 1 ]------------------------------
title | Keynote: The Care and Feeding of Software Engineers
name  | Heather Downing
```

Much better! But this raises another question...

## How Do You Query With Two Dictionaries?

I want to be able to query with both the English and simple dictionaries - but how can I do that and still get reasonable results?

The simplest way to do this with an `OR` query:

```
select
body ->> 'name' as name,
body ->> 'title' as title,
body ->> 'tags' as tags
from ndc where
search @@ plainto_tsquery('english', 'heather keynote') OR
search @@ plainto_tsquery('simple', 'heather keynote');

-[ RECORD 1 ]-------------------------------
name  | Heather Downing
title | Keynote: The Care and Feeding of Software Engineers
tags  | ["agile", "people", "soft-skills", "ethics"]
```

It's a bit on the verbose side, but as you can see we were able to find Heather's keynote just fine. Note also that I'm using `plainto_tsquery` here because I'm expecting a word salad, I can change that, however, in the case of names.

We're almost done! Now let's sort our results in a meaningful way.

## Ranking The Result Using Our Weighting

Weighting doesn't do much good unless we can apply it, so for that we'll need to make sure there's some form of "score" we can use when querying. For that, we have Yet Another Postgres Function: `ts_rank`.

There are actually _two_ of these functions. The first is `ts_rank` which is a score based on word frequency and the second is `ts_rank_cd`, which is based on frequency but also _coverage distance_ - which is basically how far words are apart in a query. For us, `ts_rank` will do fine.

To use these functions you have to pass in the `tsvector` value as well as the `tsquery`:

```sql
select
  ts_rank(search,plainto_tsquery('english', 'devops')) +
  ts_rank(search,plainto_tsquery('simple', 'devops')) as rank,
  body ->> 'name' as name,
  body ->> 'title' as title,
  body ->> 'tags' as tags
from ndc
where
  search @@ plainto_tsquery('english', 'devops') OR
  search @@ plainto_tsquery('simple', 'devops')
order by rank desc
limit 5;

-[ RECORD 1 ]-------------------------------------------------------------------------------------
rank  | 0.9074664
name  | Ashley Noble
title | Trials and Tribulations of a DevOps transformation in a large Company
tags  | ["devops"]
-[ RECORD 2 ]-------------------------------------------------------------------------------------
rank  | 0.6383234
name  | Damian Brady
title | Pragmatic DevOps - How and Why
tags  | ["devops"]
-[ RECORD 3 ]-------------------------------------------------------------------------------------
rank  | 0.6079271
name  | Enrico Campidoglio
title | Understanding Git — Behind the Command Line
tags  | ["t", "devops"]
-[ RECORD 4 ]-------------------------------------------------------------------------------------
rank  | 0.6079271
name  | Pooja BhaumikNick Randolph
title | Using Flutter to develop cloud enabled mobile applications
tags  | ["cross-pl", "mobile", "devops"]
-[ RECORD 5 ]-------------------------------------------------------------------------------------
rank  | 0.6079271
name  | Klee Thomas
title | Picking up the pieces - A look at how to run post incident reviews
tags  | ["agile", "devops"]
```

_**Update**: the original post had the \`query bits aliased but, as mentioned by Oleg in the comments, this isn't a very efficient query as it would require nested loops and joins. The query you see here is a bit more verbose, but a lot more efficient._

A few things to note about this code:

- I'm adding the `ts_rank` results together because each `tsquery` is going to have its own score. I'll get into this in a bit.
- I limited the results, because there are a lot.

The `OR` query works great and we're able to query by names, tags and titles and we're almost done - but as you can see the scoring is ... weird.

Postgres does some voodoo math behind the scenes and honestly it doesn't really matter what those scores are all about - what does matter is that some are scored higher than others and we need to make sure our scoring scheme works as we want.

Looking at the top 2 it's easy to see it does: they have the term "devops" as tags as well as the title. This is a classic SEO rule for the web, and we should feel good about our search strategy, don't you think? I guess it can be abused, however, if we pretend it's 1998 and load our title and speaker's name with keywords:

```sql
select
  ts_rank(search,plainto_tsquery('english', 'devops')) +
  ts_rank(search,plainto_tsquery('simple', 'devops')) as rank,
  body ->> 'name' as name,
  body ->> 'title' as title,
  body ->> 'tags' as tags
from ndc
where
  search @@ plainto_tsquery('english', 'devops') OR
  search @@ plainto_tsquery('simple', 'devops')
order by rank desc
limit 5;

-[ RECORD 1 ]-------------------------------------------------------------------------------------
rank  | 0.9074664
name  | Ashley DevOps Noble
title | DevOps Trials and DevOps Tribulations of a DevOps transformation in a large DevOps Company
tags  | ["devops"]
-[ RECORD 2 ]-------------------------------------------------------------------------------------
rank  | 0.6383234
name  | Damian Brady
title | Pragmatic DevOps - How and Why
tags  | ["devops"]
-[ RECORD 3 ]-------------------------------------------------------------------------------------
rank  | 0.6079271
name  | Enrico Campidoglio
title | Understanding Git — Behind the Command Line
tags  | ["t", "devops"]
-[ RECORD 4 ]-------------------------------------------------------------------------------------
rank  | 0.6079271
name  | Pooja BhaumikNick Randolph
title | Using Flutter to develop cloud enabled mobile applications
tags  | ["cross-pl", "mobile", "devops"]
-[ RECORD 5 ]-------------------------------------------------------------------------------------
rank  | 0.6079271
name  | Klee Thomas
title | Picking up the pieces - A look at how to run post incident reviews
tags  | ["agile", "devops"]
```

OK it's not perfect, but it's much better than indexing a blob of text because:

- We can recognize speaker names
- We're weighting tag recognition over title
- We're weighting tags and names over the loose text of a title

I think for most web applications this will work really well!

## Flexing Postgres 12

Trying to decide between `to_tsquery`, `plainto_tsquery` and `phraseto_tsquery` can be difficult. It was kind of straightforward in our case - we're not searching on any phrases really.

The Postgres team decided to be helpful in this regard, especially when it comes to web applications, so they created `websearch_to_tsquery`. It basically treats the input as if it were entered into a Google search. To be dead honest I have no idea what's happening under the covers here, but it's supposed to be a bit more intelligent than `plainto_tsquery` and a little less strict than `phraseto_tsquery`.

I've played with it a few times and haven't noticed much of a difference - it is worth noting however!

Phew! Long post - hope it was helpful!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2019/10/screenshot_205.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2019/10/screenshot_205.jpg" />
  </entry>
  <entry>
    <title>Virtual, Computed Columns in PostgreSQL 12</title>
    <link href="https://bigmachine.io/posts/virtual-computed-columns-in-postgresql-12" rel="alternate" type="text/html"/>
    <updated>2019-10-24T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/virtual-computed-columns-in-postgresql-12</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2019/10/screenshot_202.jpg" alt="Virtual, Computed Columns in PostgreSQL 12" /></p>
The PostgreSQL team has been jamming out updates on a regular basis, adding some amazing features that I hope to go into over time but one of these features made me extremely excited! [Generated columns](https://www.postgresql.org/docs/current/ddl-generated-columns.html):

> A generated column is a special column that is always computed from other columns. Thus, it is for columns what a view is for tables.
>
> Yay!

What this means is that you can have a managed "meta" column that will be created and updated whenever data changes in the other columns.

Too bad [Dee didn't know about](https://bigmachine.io/products/a-curious-moon/) this when she was working with the Cassini data! Setting up those search columns would have been much easier!

## An Example: A Fuzzy Search for a Document Table

Let's say you have a table where you store JSONB documents. For this example, I'll store conference talks in a table I'll call "NDC", since I was just there and did just this:

```
create table ndc(
  id serial primary key,
  body jsonb not null,
  created_at timestamptz not null default now(),
  updated_at timestamptz not null default now()
);
```

Here's an example of a talk - a real one I scraped from the NDC site, which happens to be [Heather Downing's](https://www.quorralyne.com/) amazing keynote:

```
{
  "title": "Keynote: The Care and Feeding of Software Engineers",
  "name": "Heather Downing",
  "location": "Room 1",
  "link": "https://ndcsydney.com/talk/keynote-the-care-and-feeding-of-software-engineers/",
  "tags": ["agile", "people", "soft-skills", "ethics"],
  "startTime": {
    "hour": 9,
    "minutes": 0
  },
  "endTime": {
    "hour": 10,
    "minutes": 0
  },
  "day": "wednesday"
}
```

This wad of JSON will get stored happily in our new table's `body` field but querying it might be a pain. For instance - I might remember that Heather's talk is the Keynote, but it's a long title so remembering the whole thing is a bummer. I _could_ query like this:

```
select * from ndc where body ->> 'title' ilike 'Key%';
```

Aside from being a bit of an eyesore (the `body ->> 'title'` stuff is a bit ugly), the `ilike 'Key%'` has to run a full table scan, loading up the entire JSON blob just to make the comparison. Not a huge deal for smaller tables, but as a table grows this query will start sucking resources.

We can fix this easily using the new `GENERATED` syntax when creating our table:

```
alter table ndc
add column title text
generated always as (body ->> 'text');
```

Run this and the generated column is created and then populated as well! Check it:

![](https://bigmachine.io/img/screenshot_201.jpg)

title is now a relational column

_**But wait, there's more**_. If we tried to run our search query with the fuzzy match on title we'd still have to do a full table scan. Generated columns _actually store the data_ as opposed to computing it at query time, which means we can...

```
create index idx_title on ndc(title);
```

BAM! What used to require a few triggers and an occassionally pissed off DBA is now handled by PostgreSQL.

Also - just to be sure this is clear - we could also have declared this in the orginal definition if we wanted:

```
create table ndc(
  id serial primary key,
  body jsonb not null,
  title text generated always as (body ->> 'title') stored,
  created_at timestamptz not null default now(),
  updated_at timestamptz not null default now()
);
create index idx_title on ndc(title);
```

## Into the Weeds: The Search Field

Adding a full text search index would seem to be the obvious use of `GENERATED` don't you think? I decided to wait on that because, for now, it's not exactly straightforward.

If all I wanted to do was to search on the title of a talk then we're in business... _sort of_:

```
alter table ndc
add search tsvector
generated always as (to_tsvector('english', body ->> 'title')) stored;
```

This works really well, as you can see:

![](https://blog.bigmachine.io/img/screenshot_202.jpg)

But it took me about 2 hours (seriously) to figure this out as I kept getting a really annoying error, which I'll go into in a minute:

```
ERROR:  generation expression is not immutable
```

Long story short, if you don't add the _english_ language definition to the `ts_vector` function than things will fail. The expressions that you use to define a generated column must be immutable, as the error says, but understanding which functions are and are not can be a bit of a slog.

## Deeper Into the Weeds: Using Concat

Let's keep going and break things shall we? We've got a lot of lovely textual information in our JSON dump, including `tags` and `name`. This is where we earn our keep as solid PostgreSQL brats because _we know, ohhh do we know_ that a blanket full text indexing that tokenizes everything evenly is pure crap :).

We'll want to be sure to weight the `tags` and maybe suppress the tokenization of names - I'll get to that in a later post - right now I just want to take the next step, which is to add other fields to our search column. All we have at the moment is the `title` - let's add name:

```
alter table ndc drop search;
alter table ndc
add search tsvector
generated always as (
  to_tsvector('english',
    concat((body ->> 'name'), ' ', (body ->> 'title'))
  )
) stored;
```

I formatted this so it reads better - hopefully it's clear what I'm trying to do? I'm using the `concat` function to, well, concatenate the name with a blank space and then a title. I need that blank space in there otherwise the name and title will be rammed together making it useless.

```
ERROR:  generation expression is not immutable
```

Crap! What? This is a concatenation!?!?! How is this not immutable? Turns out it's the `concat` function that's causing the problem, and I'm not sure why (if you know please leave me a comment). This, however, does work:

```
alter table ndc drop search;
alter table ndc
add search tsvector
generated always as (
  to_tsvector('english',
    (body ->> 'name') || ' ' || (body ->> 'title')
  )
) stored;
```

That, my friends, is super hideous - but it gets the job done. I'll get more into full text indexes in a later post as I've had some really good fun with them recently.

## Summary

I've had a lot of fun goofing around with the generated bit. If you're wondering, the actual update goes off right after the `before` trigger would normally go off - so if you do have a `before` trigger on your table, you can use whatever values are generated there.

You also might be wondering about the `stored` keyword you see here? Right now it's the only option: the generated bits are stored on disk next to your data. In future releases you'll be able to specify `virtual` for just in time computed bits... but not now.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2019/10/screenshot_202.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2019/10/screenshot_202.jpg" />
  </entry>
  <entry>
    <title>Audiobook Review: Fall; Or, Dodge in Hell by Neal Stephenson</title>
    <link href="https://bigmachine.io/posts/fall-or-dodge" rel="alternate" type="text/html"/>
    <updated>2019-08-27T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/fall-or-dodge</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2019/08/Stephenson_Fall_cover.png" alt="Audiobook Review: Fall; Or, Dodge in Hell by Neal Stephenson" /></p>
The problem with any book by Neal Stephenson is that the person foolish enough to try and "review" it has to start _somewhere_. A foothold on the story, its arc, the social relevance and a bunch of other _blah blah blah_. To use a common Stephenson affectation: _with this story, like the rest of his stories, there isn't such._

Because to get to that _somewhere_ involves introspecting the story and coming up with a Gestaldtish summation of WTF just happened. Stephenson's stories are not short and are brutally thick with cause and effect to the point where you just... kind of lose your train of thought, as I'm doing.

This story _is f\*\*\*ing overwhelming_, and _life changing_. It's also a bunch of other hyperbolic superlatives that I'll just wrap up with the typical feeling I have after finishing any of Mr. Stephenson's books (whether first or fourth reading): _**I'll never think the same way again**_.

## The Commitment

Every Stephenson book I've read requires commitment on the reader's part. Some people dig the challenge, others just aren't into the mental toil. I'm both, if I'm honest. It took me 3 efforts to get through _Anathem_, 5 to get through _Cryptonomicon_ and I'm still trying to get through _Seveneves_. It's a rite of passage.

My brother threw _Fall_ across the room, and he's not a light reader (Pynchon is his favorite writer). Of the 8 friends I have who are trying to read this book, 3 have given up, 2 reported it to be slog and 3 utterly loved it. **I was each of these people.**

I don't think it's possible to find two people who agree on this book, which, to me, means it's worth trying and that is my first point: **take this book on. Accept the challenge. It's worth it.**

## A Digital Afterlife

This book is about the digital afterlife. If you're a Black Mirror fan and are thinking about _[San Junipero](https://www.netflix.com/title/70264888)_ don't - it's not that at all. The premise, however, is sort of the same.

![](https://i1.wp.com/bigmachine.io/img/2019/08/screenshot_171.jpg?fit=640%2C349&ssl=1)

The book picks up a few years after _REAMDE_ and features many of the same characters (C+, Zula, Richard "Dodge" Forthrast, etc) and it also weaves in characters from other Stephenson stories - most notably _Cryptonomicon_ and the _Baroque Cycle_. I thought that was a nice touch.

The story starts off with energy and pulls you in quickly, starting with the death of Dodge and, to me, a masterful social media hoax involving a nuclear bomb going off over Moab, Utah.

This is Stephenson at his best. Breathtaking depth and vision with absurd assertions backed up by relentless science. The nuclear bomb turns out to be a hoax, but the entire world buys into it because video footage (using actors from a fake movie) dropped on social media played into people's fears. Meanwhile, a DDoS attack in Moab itself cut it off from the internet - so for a few days people believed that was simply gone.

That and the following chapters were the most fun to read and, consequently, the ones I forgot about first. They deal with the destruction of the internet as we know it, replaced by a block chain analogue that embraces accountability. I had to relisten to a few chapters (out of choice) because Stephenson goes into such rich detail and I absolutely loved the idea of an "internet apocalypse", something I don't think I've read about before.

From there things get weird, fast.

## Shaping a Digital Afterlife

I don't think it's possible to spoil a Stephenson book because **the fun is in the journey, not the story**. If you remember anything from this review it's that: _it's a long, slow, dense journey_. If you want to be totally surprised by it, stop reading here. I don't plan to give away major plot points... but just in case.

Stephenson doesn't just spring the idea of a digital afterlife on you, he shapes a world - the only world - in which it's possible to have such a thing. He addresses the computational needs (using quantum machines of course) and the energy and cooling required. The center of the this phase of the book - the "buildup" if you will - is a legal back and forth regarding Dodge's will. In it he specified exact instructions on what to do with his body when he dies because he wants it preserved for the time and place when it's possible to load his "connectome" (his digital self) into a computer.

> One of the funny things about it, in retrospect, was its slowness, the lack of any dramatic Moment When It Had Happened. It was a little bit like the world’s adoption of the Internet, which had started with a few nerds and within decades become so ubiquitous that no person under thirty could really grasp what life had been like before you could Google everything.
>
> **Neal Stephenson,** Fall; Or, Dodge in Hell

This is what I love about Stephenson: you get completely lost in his overly active brain. He comes at you with so many ideas that you have to pause the audio (or put the book down) to let the myriad whacko nuttiness settle and form some type of recognizable concept.

The idea of "mapping" a human brain by creating what is, essentially, a 3-dimensional graph of neural connections (the "connectome") is brilliant and bullshit at the same time. I think. Maybe not. _I really have no idea_! Stephenson's tech-literary Judo comes at you so fast you find yourself on the ground before you know what's happening, disbelieving all of it until you realize that... hey wow you're on the ground and he's standing over you laughing trying to help you back up.

_What just happened?_

When Sophia, Dodge's niece, goes to work for Corporation 9592 she is tasked with understanding the _DB_, or "Dodge's Brain". All they have is his connectome in the form of binary files on disk, which she loads into a distributed quantum network and... _just turns it on_.

I loved this cowboy coder technique of booting up the first digital brain because it's _exactly what would have happened_. It takes everyone by surprise in the book too, which I thought was wonderful and I found myself giggling as Sophia just kind of shrugged _as if_ while the scientists around her pitched fits. Nice touch :).

## Behold: Egdod

Skipping ahead: they managed to boot Dodge's brain and his consciousness came online. I thought more work could have been done here but Stephenson chose to go down a path that I thought was simultaneously interesting, odd, and also obvious. _He chose the to form the digital afterlife into some kind of fantasy realm_.

Egdod is Dodge's avatar from T'rain, the MMO at the center of REAMDE (a great book btw). When Dodge "awakens" his brain is forced to make sense of chaos, and eventually does like something straight out of the book of Genesis. Egdod ("Dodge" spelled backwards) is definitely god-like, sprouting wings and forming the land with not much recollection of who he is or what's happened to him.

It was hard for me to make this transition. I _loved_ the Black Mirror world that Stephenson was building and the stories happening therein with characters that I was attached to from his previous books. Transitioning to a fantasy story seemed... clumsy to me.

And Stephenson being Stephenson, he clubs you over the head with it until you submit. Egdod is joined by other "souls" over time and they start to branch out into other forms, including a "hive" of souls that mass together and threaten Egdod's superiority. At one point, a soul named "Spring" figures out how to create life from the chaos in the form of a bee.

Everything about this part of the book: the tone, speech patterns, and the prose, are all reminiscent of a fairly standard fantasy novel full of gods, dense forests and magic. Eventually you get used to it, and then...

## Back to The Real World

Stephenson transitions between these world abruptly, as if trying to reinforce the idea that you are, in fact, reading two books. This made sense to me because a digital afterlife really is a different plane of existence (as it's described later in the book). I mean: if you ended up in _[San Junipero](https://www.netflix.com/title/70264888)_ would you want to spend time thinking about your previous existence?

I don't want to make light of this point (and Stephenson certainly didn't). The characters in both realms (real and digital) don't interact at all. The only way people in the real world can understand what's happening in "Bitworld" (as they call it) is through the use of a viewer which tracks signal associations between processes. It's overwhelming to think about how that might work, and the characters in the book "get used to it" in the same way Cipher got used to spotting people in the _Matrix_ by looking at the code from the image translators.

Over decades, the viewer becomes so good that people begin watching it as a source of entertainment, leading to one of my favorite lines from the book:

> The living stayed home, haunting the world of the dead like ghosts.
>
> Neal Stephenson, Fall or Dodge in Hell

The dead in Bitworld, however, have some idea about where they came from but its assigned a mythical quality. The only one to have a full understanding of his past is Dodge, who is defeated in battle by his nemesis in the book and, at the same time, gains full understanding of both worlds. Interestingly, nothing more comes of this, which I thought was weird.

## The Fading of the Real World

Decades go by in the real world while in Bitworld, time flows according to processing power. Another fine touch by Stephenson: addressing the _time slip_ between Bitworld and the real world. Time goes by as normal in Bitworld, but to the people watching it at home it will slow to a crawl or speed up to the point where days go by in minutes.

To handle the processing power, servers are put in orbit and capture solar radiation for energy and thermal radiation is mirrored out into space to avoid overheating the planet. I like the way Stephenson handles this. There's a lot of money to be made off of Bitworld so we will invent accordingly.

Interestingly, Stephenson doesn't spend much time talking about too many other real world details. That's been done to death in other stories - he's far more interested in how Bitworld matures over time. It's in this part of the book that I think most people get lost or flat out give up.

It's easy to see why: the entire narrative is thrown completely on its head. Bitworld matures into a full-blown fantasy realm straight out of Lord of the Rings. People can do different forms of magic, fantastical beings (lightning bears being one of my favorite) inhabit treacherous landscapes and everything reads like a Tolkein novel.

It's all very confusing. Until you consider one thing: _how else could it truly be?_

It seems that when we humans have a chance to invent a different world we reach for the fantastic. One of my favorite games is Witcher 3 and it could easily take place in Bitworld.

Wait... _it does take place in Bitworld_, except Geralt isn't formed from the soul of a dead person. At least... that we know of.

This was the slow realization as I finished _Fall_: Stephenson captured the _only way this could possibly work out_. Left to their own design, dead souls will build and create what they find interesting or, put another way, what inspiration wells from their deep memory. This, to me, was a stroke of genius and, once again, Stephenson has spun my brain.

## The Cloud Within the Silver Lining

With a book this long it's impossible to not become irritated by an author's affectations. This is especially true with audiobooks. I began to sense when Stephenson was tired of rolling out a certain plot point - fatiguing him to the point where the word "various" would crop up more than normal and he would flip into passive voice:

> Various nobility were arrayed around the table while goblets of wine were filled bawdy jokes told.
>
> Example of Tired Stephenson

You can't blame Stephenson for succumbing to fatigue with this book. It's HUGE and it's DENSE, but sentences like this should be tackled (in my opinion) by an editor and reworked into something less throwaway.

### It's Almost Too Huge

The subjects in Fall are legion. Stephenson talks about social change, the death of fact and the internet as well as its utopian blockchain-powered replacement - this could have been a satisfying book on its own! There's the question of what a soul is and the religious probings of an afterlife, each of which get their turn albeit a shallow one.

It's unlike Stephenson to arm wave any detail, but I felt in this book he had to in order to finish it. I don't mind that he didn't drill into _everything_, but some pruning could have been helpful. For instance: server farms in space sound fun, but how exactly do you network those to avoid the obvious latency issues? Cosmic rays and radiation, meteorites and natural disasters taking souls out of existence...

This might sound demanding, but Stephenson is known for rounding out these kinds of plot elements. I mean... _servers in space?_ Yeah! I can barely manage to get an app deployed to AWS ... but deploying a virtual soul? Yikes! And what about viruses...

Finally: you want to spend time with beloved characters from his old books that Stephenson reintroduces in this story. The Shaftoes make a tangential entrance and I immediately started thinking about Bobby, one of my favorite characters from _Cryptonomicon_. I kept thinking that they would play a larger role but ... nope.

Some characters do (I won't spoil that who it is) and it's... weird. I never fully understood why this character was there but... that's OK I loved them before and I still love them now :).

### A Missed Opportunity... Maybe

I brought up _[Anathem](https://www.amazon.com/Anathem-Neal-Stephenson/dp/006147410X)_ before, primarily because it's one of my favorite books from Stephenson. Such a bizarre story in an alternate reality that dealt with the very idea of what reality and consciousness is.

It would fit perfectly within _Fall_. In fact I was convinced that the ending would feature Bitworld giving itself the name of "Arbre" with the main characters founding the Concents to avoid some kind of collapse. I think that would have been fun... but maybe a little over the top (as if that's a problem here).

## The Audio Performance

The book is read by the same narrator as REAMDE: [Malcom Hillgartner](https://www.malcolmhillgartner.com/). He is _extraordinary_ in this book, his attempts at an Australian accent aside. I do voice over stuff for videos and I can tell you that keeping the energy and pace as he does is a miracle. I couldn't help myself in trying to figure out where the daily breaks were - a narrator's voice will typically sound crisper from one chapter to the next - but I couldn't do it with Mr. Hillgartner.

Well that's it! I really enjoyed this book but it takes dedication. I had to make sure that I didn't listen in small bits (10 mins or less) as I'd lose the plot quickly. Instead, I made time in the evening to sit for an hour or so, and also during lunch breaks, which made all the difference.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2019/08/Stephenson_Fall_cover.png" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2019/08/Stephenson_Fall_cover.png" />
  </entry>
  <entry>
    <title>WTF is Big O Notation?</title>
    <link href="https://bigmachine.io/posts/wtf-is-big-o-notation" rel="alternate" type="text/html"/>
    <updated>2019-03-25T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/wtf-is-big-o-notation</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2019/03/screenshot_65-825x510.jpg" alt="WTF is Big O Notation?" /></p>
When I started writing _[The Imposter's Handbook](https://bigmachine.io/products/the-imposters-handbook/)_, this was the question that was in my head from the start: _what the f\*\*\* is Big O and why should I care?_ I remember giving myself a few weeks to jump in and figure it out but, fortunately, I found that it was pretty straightforward after putting a few smaller concepts together.

**UPDATE 3-26-2019:** This post hit Hacker News and a few other opinion boards... and it caused quite a few arguments. I responded, but I figured I'd clarify a few things quickly right here.

**First**: _Big O is conceptual_. Many people want to qualify the efficiency of an algorithm based on the number of inputs. One commenter said "if I have a list with 1 item it can't be O(_n_) because there's only 1 item so it's O(1)". This is an understandable approach, but Big O is a _technical adjective_, it's not a benchmarking system. It's simply using math to describe the efficiency of what you've created.

**Second**: _Big O is worst-case_, always. That means that even if you thing you're looking for is the very first thing in the set, Big O doesn't care, a loop-bases find is still considered O(_n_). That's because Big O is just a descriptive way of thinking about the code you've written, not the inputs expected.

If you disagree with me, feel free to drop me an email using rob at this domain.

\---

I was recently at NDC London and gave a talk which had Big O in it. I asked a few of the attendees and other speakers about the subject - wondering if it would be a useful thing to talk about or if it was too academic and theoretical. The replies I got were a bit mixed, but there was, overwhelmingly, a common refrain.

## Big O? I HATE Interview Questions! This One Time...

This was the primary response from asking roughy 15 people what they thought. The common sentiment was along these lines:

> I get paid to write code, not white papers. Big O has nothing to do with my day job.

I promise you: **_I am not overstating this for effect_**. People don't like to be put on the spot in interviews. They don't want to be made to feel stupid. All of this is understandable but the unfortunate side effect is that a very useful concept (Big O) gets kicked to the curb.

I'm glad I took the time to learn Big O because **I find myself thinking about it fairly often**. If you've always wondered about Big O but found the descriptions a bit too academic, I've put together a bit of a _Common Person's Big O Cheat Sheet,_ along with how you might use Big O in your every day work.

Rather than base this on arrays and simplified nonsense, I'll share with you a situation that I was in just a month ago: _choosing the right data structure in Redis._ If you've never used Redis before, it's a very basic key-value store that works in-memory and can optionally persist your data to disk.

When you work in a relational database like PostgreSQL, MySQL, or SQL Server you get a single data structure: _the table_. Yes, there are data _types_, sure, but your data is stored in a row separated by columns, which is a data _structure_.

Redis gives you a bit more flexibility. You get to choose the data structure that fits your programming need the best. There are a bunch of them, but the ones I find myself using most often are:

- **String.** With this structure you store a string value (which could be JSON) with a single key.
- **Set.** A Set in Redis is a bunch of unordered, unique string values.
- **Sorted Set**. Just like a Set, but sorted.
- **List**. Non-unique string values sorted _by order of insertion_. These things operate like both stacks and queues.
- **Hash**. A set of string values identified by "sub keys". You can think of this as a JSON object with values being only strings.

**Why are we talking about Redis when this post is about Big O**? Because Redis and Big O go hand in hand. To choose the right data structure for your needs, you need to dig you some Big O (whether you know it's Big O or not).

## Finding Something in a Shopping Cart

Let's say you're tasked with storing Shopping Cart data in Redis. Your team has decided that an in-memory system would work well because it's fast and it doesn't matter if cart data is lost if the server blows up.

The question is: _how do you store this information?_ Here's what's required:

- Finding the cart quickly by key
- CRUD operations on each item within the cart
- Finding an item in the cart quickly
- Iterating over each item in the cart

Believe it or not, **you're thinking in Big O right now** and you might not even know it. I used the words "quickly" and "iterate" above, which may or may not mean something to you in a technical sense. The thing I was trying to convey by using the word "quickly" is that I want to get to the cart (or an item within it) _directly_, without having to jump through a lot of hoops.

Even that description is really arm-wavy, isn't it? We can dispose of the arm-waving by **thinking about things in terms of** _**operations per input**._ How many operations does my code need to perform to get to a single cart from the set of all carts in Redis?

## Only One Operation: _O(1)_

The cool thing about Redis is that it's a key-value store. To find something, you just need to know its key. You don't have to run a loop or do some complex find routine – it's just right there for you.

When something requires only one operation we can say that directly: _my code for finding a shopping cart is on the order of 1 operation_. If we want to be Big O about it, **we can say it's order 1, or "O(1)"**. It doesn't matter how many carts are in our Redis database either! We have a key and we can go right to it.

A more precise way to think about this is to use the term "constant time". It doesn't matter how many rows of data are in our database (or, correspondingly, how many inputs to our algorithm) - the algorithm will run in _constant time_ which doesn't change.

What about the items in the cart itself?

## Looping Over a Set: O(_n_)

We know that our cart will need to store 0 to _n_ items. I'm using _n_ here because I don't know how many items that will be - it varies per customer.

I can use _any_ of Redis's data structures for this:

- I can store a JSON blob in a String, identified by a unique cart key
- I can store items in a Set or Sorted Set, with each item being a bit of JSON that represents a `CartItem`
- I can store things in a List in the same way as a set
- I can store things in a Hash, with each item having a unique sub key

When it comes to items in the cart, we need to be able to do CRUD stuff but we also need to be able to find an item in the cart "as quickly as possible". If we use a String (serializing it into JSON first), a Set or a List we'll need to loop over every item in a cart in order to find the one we're looking for.

Rather than saying "need to loop over every item", we can think about things in terms of operations again: *if I use a Set or a List or a String I'll need to have one operation for every n items in my cart*. We can also say that this is "order _n_", or just O(_n_).

You can spot O(_n_) operations easily by simply looking for loops in your code. This is my rule of thumb: "if there's a loop, it's O(_n_)".

## Looping Within a Loop: _O(n^2)_

Let's say we decided to keep things simple and deal with problems as they arise so we chose a Set, allowing us to dump unique blobs of JSON data that we can loop over if we need to. Unfortunately for us, this caused some issues:

![](https://i0.wp.com/bigmachine.io/img/2019/03/screenshot_57.jpg?fit=1024%2C450&ssl=1)

Duplication in our Set

By changing the `quantity` in our `CartItem` we have made our JSON string unique, causing duplication. We need to remove these duplications now, otherwise our customers won't be happy.

Simple enough to do: we just loop over the items within a cart, and then loop over the items one more time (skipping the current loop index) to see if there's a match on `sku`. This is a classic _brute force_ algorithm for deduping an array. That's a lot of words to describe this nested loop algorithm and we can do better if we use Big O.

Thinking in terms of operations, we have _n_ operations per _n_ items in our cart. That's `n * n` operations, which we can shorthand to "order _n_ squared" or O(_n_^2). Put another way: _deduping an array is an O(n^2) operation, which isn't terribly efficient_.

As I said before, I like to think of these things in terms of loops. My rule of thumb here is that if **I have to use a loop within a loop, that's O(**_**n**_**^2).** Another rule of thumb is that the term "brute force" almost always denotes an O(_n^2_) algorithm that uses some kind of nested loop.

## Indexing a Database Table and O(_log n_).

If you've ever worked on a larger project with a DBA, you've probably been barked at for querying a table without utilizing an index (a "fuzzy" search, for instance). Have you ever wondered what the deal is? I have. I _was_ that DBA doing the barking!

Here's the thing: tables tend to grow over time. Let's say that our commerce site is selling independent digital films and our catalog is constantly growing. We might have a table called `film` filled with ridiculous test data that we want to query based on `title`. Unfortunately, we don't have an index just yet and our query is beginning to slow down. We decide to ask PostgreSQL what's going on using `EXPLAIN` and `ANALYZE`:

![](https://i2.wp.com/rob.conery.io/img/2019/03/screenshot_58.jpg?fit=1024%2C426&ssl=1)

Our database is doing what's called a "Sequential Scan". In SQL Server land this is called a "Full Table Scan" and it basically means that Postgres has to loop over every row, comparing the `title` to our query argument.

In other words: a Sequential Scan is a loop over every item which means it's O(_n_), where _n_ represents the number of rows in our table. As our table grows, the efficiency of this algorithm goes down linearly.

It's easy to improve the performance here by adding an index:

![](https://i2.wp.com/rob.conery.io/img/2019/03/screenshot_59.jpg?fit=1024%2C470&ssl=1)

Now we're using an _Index Scan_, which is, I suppose, much faster. But how much? And how does it work?

Under the covers, most databases use a version of an algorithm called _binary search_ - [I made a video](https://bigmachine.io/products/the-imposters-handbook/) about this and other algorithms which you can [watch right here](https://bigmachine.io/products/the-imposters-handbook/) if you want. For binary search to work properly, you have to sort the list of things you're working with. That's exactly what Postgres does when you first create the index:

![](https://i1.wp.com/rob.conery.io/img/2019/03/screenshot_61.jpg?fit=1024%2C676&ssl=1)

Now that the index is sorted, Postgres can find the `title` we're looking for by systematically splitting this list in half until there's only one row left, which will be the one we want.

This is much better than looping over every row (which we know is O(_n_)), but how many operations do we have here? For this we can use _logarithms:_

![](https://i0.wp.com/rob.conery.io/img/2019/03/screenshot_62.jpg?fit=1024%2C677&ssl=1)

We're continually splitting things in half in a sorted set until we arrive at the thing we want. We can describe this with an inverted binary tree, as you see above. We start with 8 values, split, and are left with 4, which we split again to get 2, then finally 1.

This is an inverse squaring operation as we're going from 2^3 (8) down to 2^2 (4) down to 2^1 (2) and finally 2^0 (1). Inverse squaring operations are called _logarithms_. That means that we can now describe the operations of our database index as "being logarithmic". We should also specify "logarithmic of _what_" to which we can answer **"we don't know, so we'll say it's** _**n**_**", also known as O(**_**log n**_**).**

This kind of algorithm is called "divide and conquer" and when you see those words, you know immediately that you're talking about a _log n_ algorithm.

## ... And So What?

Here's why you care about turning something that's O(_n_) into O(_log n_) and the best part is that it's not really arguable **because it's math** (I was told that means you're always right :trollface:).

Let's say we have 1000 records in our `film` table. To find "Academy Dinosaur" our database will need to do 1000 operations (comparing the `title` in each row). But how many will it do if we use an index? Let's use a calculator and find out, shall we? I need to find the log base 2 (because of the binary split) of 1000:

![](https://i2.wp.com/rob.conery.io/img/2019/03/screenshot_63.jpg?fit=1024%2C590&ssl=1)

10 operations with our index

Only Ten! Ten splits of 1000 records to find what we want in our database. That's a performance gain of a few orders of magnitude, and it's a lot more convincing to tell someone _that_ as opposed to "it's a lot faster".

The best part here is that we can keep using this calculator to find out how many operations will be needed if we have a _million_ records (it's 20) or a _billion_ (it's 30). That kind of scaling as our inputs goes up is the stuff of DBA dreams.

## Bonus Question: What's The Big O of a Primary Key Lookup?

It's tempting to think that if I have a primary key and I know the value of that key that I _should_ be able to simply go right to it. Is that what happens? Think about it for a second and while you're thinking let's talk about Redis a bit more.

A major selling point of Redis (or any key-value system really) is that you can do a _lot_ of stuff with O(1) _time complexity_. That's what we're measuring when we talk about Big O – _time complexity_, or long something takes given the inputs to an algorithm you're working with. There's also _space complexity_ which has to do with the resources your algorithm needs, but I'll save that for another post.

Redis is a key-value store, which means that if you have a key, you have an O(1) operation. For our Shopping Cart above, if I use a Hash I'll have a key for the cart _as well as_ a key for each item in the cart, which is huge in terms of performance – or I should say "optimal time complexity". We can access any item in a cart without a single loop, which makes things fast. Super fast.

OK, back to the question regarding primary key queries: _are they O(1)?_ **Nope**:

![](https://i0.wp.com/rob.conery.io/img/2019/03/screenshot_64.jpg?fit=1024%2C607&ssl=1)

This surprised me!

Indexes in most database systems tend to use a variation of binary search, and primary key indexes are _no different_. That said, there are plenty of optimizations that databases use under the covers to make these queries extremely fast.

I should also note that some databases, like Postgres, offer you different types of indexes. For instance you can use a Hash Index with Postgres that will give you O(1) access to a given record. There is a lot going on behind the scenes, however, to the point where [there's a pretty good debate](https://dba.stackexchange.com/questions/212685/how-is-it-possible-for-hash-index-not-to-be-faster-than-btree-for-equality-looku) about whether they're actually faster. I'll side step this discussion and you can go read more for yourself.

## There You Have It

I find myself thinking about things in terms of Big O a lot. The cart example, above, happened to me just over a month ago and I needed to make sure that I was flexing the power of Redis as much as possible.

I don't want to turn this into a Redis commercial, but I will say that it (and systems like it) have a lot to offer when you start thinking about things in terms of _time complexity_, which you should! **It's not premature optimization to think about Big O upfront, it's** _**programming**_ and I don't mean to sound snotty about that! If you can clip an O(_n_) operation down to O(_log n_) then you should, don't you think?

So, one last time:

- Plucking an item from a list using an index or a key: O(1)
- Looping over a set of _n_ items: O(_n_)
- A nested loop over _n_ items: O(_n^2_)
- A divide and conquer algorithm: O(_log n_)

Hope this helps!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2019/03/screenshot_65-825x510.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2019/03/screenshot_65-825x510.jpg" />
  </entry>
  <entry>
    <title>PostgreSQL Tools for the Visually Inclined</title>
    <link href="https://bigmachine.io/posts/postgresql-tools-for-the-visually-inclined" rel="alternate" type="text/html"/>
    <updated>2019-03-04T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/postgresql-tools-for-the-visually-inclined</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2019/03/screenshot_1451-825x510.png" alt="PostgreSQL Tools for the Visually Inclined" /></p>
I started my career on the Microsoft stack building forms and websites using drag and drop tools. Over time that became a punchline, which is unfortunate because honestly, the productivity was insane.

In 2008 I made the jump to the Linux world and I was completely disoriented. _Everything was a damn text file_. Yes, you could use a Mac or Ubuntu or whatever Unix Desktop du Jour seemed fun but there simply was no getting around the need to know your commands, which I did.

Just like learning SQL, learning your text commands makes you more efficient. _I promise you that I'm not about to flip the l33t bit_. I'm not here to convince anyone of anything – what I do want to do is to share how I embraced the command line with respect to PostgreSQL and was damn happy for it.

## Friendly vs. Friendly

I've been meaning to write this post for years but it was [this post](https://www.softwareandbooz.com/postgresql-for-a-sql-server-dba-the-tooling-stinks/) from Ryan Booz that made me fire up the editor. Ryan is a SQL Server DBA that is writing a series on how [he's learning PostgreSQL](https://www.softwareandbooz.com/postgresql-for-a-sql-server-dba-a-series/) after a 15 year (!) career as a SQL Server DBA. I can't imagine that change is an easy one.

Basically, Ryan has concerns (which I understand):

> In the case of PostgreSQL, I’ve quickly come to the conclusion that bad tooling is one of the main reasons the uptake is so much more difficult and convoluted coming from the SQL Server community. Even the devs I’m currently working with that have no specific affinity for databases at all recognize that PostgreSQL just feels like more of a black box then the limited experience they had previously with SQL Server.

I can't say he's wrong on this, although I will say the term "bad" is a bit subjective.

Let me get right to it: jumping from SQL Server to PostgreSQL is _much more_ than changing a tool. **PostgreSQL was built on Unix**, with Unix in mind as the platform of choice, and typically runs best when it's sitting on some type of Unix box. **The Unix world has a pretty specific idiom** for how to go about things and it **certainly isn't visual**!

As someone who learned to code visually, I had to learn what each icon meant and the visual cues for what happens where. I came to understand property pains, the lines under the text of a button that described shortcuts, and the idiomatic layout of each form. Executing a command meant pressing a button.

In the Unix world you write out that command. The check boxes and dialogs are replaced by option flags and arguments. You install the tools you need and then look for the binaries that help you do a thing, then you interrogate them for help, typically using a `--help`command (or just plain `help`).

The same is true for PostgreSQL. This is the thing that I think was stumping Ryan. He's searching for visual tooling in a world that embraces a completely different idiom. It's like going to Paris and disliking it (and France) because the barbecue is horrible.

Let's walk through some common PostgreSQL DBA "stuff" to show what I mean.

## Your Best Friend: psql

When you encounter a new Unix tool for the first time (and yes, I'm labeling PostgreSQL that) you figure out the binaries for that tool. PostgreSQL has a number of them that you'll want to get to know, including `pg_dump` and `pg_restore`among others. The one we want right now is `psql`, the interactive terminal for PostgreSQL that gets installed along with the server. Let's open it and ask it what the hell is going on:

![](https://i1.wp.com/bigmachine.io/img/2019/03/screenshot_1430.png?fit=1024%2C731&ssl=1)

Hello psql

I'm using Mac's Terminal app but you can use any shell you like, including Powershell and the Windows command line. I would strongly urge you, however, to crack open a Linux VM or Docker to get the "flavor" of working with PostgreSQL. You can, indeed, find barbecue in Paris but it might help to explore the local cuisine.

Reading through this list of options and commands will take some patience the first time – but it's worth it! At the top of the list are the common options, like using `-c`for running a command and `-d`for the database to run the command in. There's a key statement, however, at the very end of this help screen:

![](https://i1.wp.com/rob.conery.io/img/2019/03/screenshot_1432.png?fit=1024%2C731&ssl=1)

psql: it's interactive!

The `psql` tool is interactive! This will help us - so let's log in to a database and have a look around. But which database? We'll create one by running this on the command line:

```
createdb redfour
```

The `createdb`binary has one job, in typically Unix fashion: _create a database on the local server_. It has a counterpart binary as well: `dropdb`. How do I know this? It's one of those things you get used to as you work with Unix systems - figure out the binaries and what they do.

How do you do that? We know about one binary so far, `psql`, so let's figure out where that lives and hopefully the other binaries live there too:

![](https://i1.wp.com/rob.conery.io/img/2019/03/screenshot_1433.png?fit=1024%2C561&ssl=1)

Using which and ls to tell us more

This is one of those things you learn over time: asking Unix `which` version of a tool/binary/runtime it's using and where it's located. The result of that command is telling me that `psql` lives in the `/Applications/.../bin` directory, which is pretty standard for binary tools. I copy/paste the result to the `ls` command (list contents) and we can see the binary tools at our disposal.

Yay. Let's log in and play around.

![](https://i1.wp.com/rob.conery.io/img/2019/03/screenshot_1436.png?fit=1024%2C360&ssl=1)

## What the Hell Is Happening?

Right now I'm at an interactive terminal within my database... and have no idea what to do next. This is the major upside of visual tooling: you have cues that you can follow which inform you as to what's happening. It's the difference between Halo on the Xbox and an old school MUD – it feels outdated and silly.

Let's keep going and see if that's true. When we ran the `--help` command before, it told us to use `\?` to figure out commands within psql, so let's try that first:

![](https://i2.wp.com/rob.conery.io/img/2019/03/screenshot_1437.png?fit=1024%2C543&ssl=1)

Hello sea of text crashing over me!

There is _so much to absorb here_. All of these cryptic little commands _do something_ but what they do, at first, will likely be opaque to you. This is Yet Another Patient Deep Breath point, because pretty soon we're about to light this shit on fire (in a good way). What you have, right here, is a lot of raw _power_ right at your fingertips. It just takes a few hours to understand the cadence of these commands as well as their modifiers. I'll show you exactly what I mean in just a few minutes, for now let's ground ourselves.

Scroll down (using down arrow or your mouse) to the Informational command section. This is your bread and butter - here you can see what's in your database at a quick glance. We can do that by using `\d` (press Q to get out of the text view of the help page):

![](https://i1.wp.com/rob.conery.io/img/2019/03/screenshot_1438.png?fit=1024%2C536&ssl=1)

Our database is empty. Let's fix that by creating a quick table for our users:

![](https://i1.wp.com/rob.conery.io/img/2019/03/screenshot_1439.png?fit=1024%2C539&ssl=1)

When you write a SQL command within psql you can wrap the lines. Notice that the prompt changes as well, telling you that you currently have an open paren. To finish the command add a semi-colon and we're done. _Note: I'm not going to get into SQL but [it's really worth your time](http://www.craigkerstiens.com/2019/02/12/sql-most-valuable-skill/) to learn._

Now let's list out our relations again:

![](https://i2.wp.com/rob.conery.io/img/2019/03/screenshot_1440.png?fit=1024%2C539&ssl=1)

Lovely. We have our table and the thing that handles the id generation for that table, called a _sequence_. Let's ask psql more about this table using `\d users`:

![](https://i0.wp.com/rob.conery.io/img/2019/03/screenshot_1441.png?fit=1024%2C422&ssl=1)

The structure of our table is laid out in glorious ASCII, heavy with information and completely bereft of anything resembling prettiness. For visual people, this is a turn off as it's completely different than what they're used to (which I understand). For people used to working in a text-based idiom, this is heavenly.

Why? _It's the speed of the thing_. Let's put a clock to the problem. One of your appdevs just did something ill-advised with their ORM and they think they might have broken the `users` table. You decide to investigate:

```
psql redfour
\dt users
```

When you're just starting out with PostgreSQL (and psql), you'll need to squeeze your memory a bit for the commands to inspect a table. After a while your fingers will be done typing before your next breath.

_This is the power you want as a DBA_.

At this point I could go off on all of the psql commands available to you, however I would encourage you to explore these for yourself and see what's possible, and how blazingly fast you can get it done. My coworker (ha ha so fun to say that now) Craig Kerstiens has written extensively on PostgreSQL, and [this post is extremely helpful](http://www.craigkerstiens.com/2013/02/13/How-I-Work-With-Postgres/) for people getting used to the command line aspect of it.

I want to get into why this kind of thing matters.

## Text is a Helluva Drug

If I asked you to move data from one server to another using your favorite visual tool, how would you do it? If you do it often then the process would be a simple one and likely involve some right-clicking, traversing a menu, and kicking off a process in your tool of choice.

In Unix land (and therefore Postgres land) it's a matter of remembering a few commands. But this is where it gets interesting because _everything in Unix is a text file._ Almost every task you can think of in Unix can be done using a text-based command. It would be like trying to find barbecue in Paris when every building is made of meat and the Seine is a river of hot coals.

To show you what I mean, here's how you might pull your production database down to your local server:

```
pg_dump postgres://user:password@server/db > redfour.sql
createdb redfour
psql redfour < redfour.sql
```

The `pg_dump` binary has the singular task of turning a database into a SQL file. You can, of course, tweak and configure how this works and to find out all of the options you would use... can you guess? `pg_dump --help` . So we're dumping the structure and data to a SQL file, creating our database and then pushing that SQL file as a command into our new database.

This entire process will execute in < 5 seconds on a smaller sized database (~20mb). This is why we like text and text-based interfaces - SPEED!

## There's Always a Way

As you might be able to tell, I've had this conversation more than a few times. Visuals are very important, to be sure! But they have their place when it comes to your daily workflow as a DBA. I would argue that double-clicking, right-clicking, and drag/drop are much slower than taking the time to memorize some common commands.

One place that psql sucks, however, are visuals. Executing a query on a large table can look horrible:

![](https://i0.wp.com/rob.conery.io/img/2019/03/screenshot_1443.png?fit=1024%2C667&ssl=1)

Yuckity Yuck

This is DVD Rental sample database, running a `select * from film;` query. It looks like crap! The good news is that we _should_ be able to fix this. Let's ask psql what's going on using `\?` :

![](https://i1.wp.com/rob.conery.io/img/2019/03/screenshot_1442.png?fit=1024%2C575&ssl=1)

There are two things to notice here. The first is `\x` which allows for expanded output, or vertical alignment of data. That looks like this:

![](https://i1.wp.com/rob.conery.io/img/2019/03/screenshot_1444.png?fit=1024%2C667&ssl=1)

Using expanded output

The other thing you can do is to set HTML as the output using `\H`. This will execute the query, returning the results in HTML:

![](https://i2.wp.com/rob.conery.io/img/2019/03/screenshot_1445.png?fit=1024%2C667&ssl=1)

This is interesting but I want this saved to a file. To do that, I can use `\o` (which you can see in the help menu) and specify which file:

![](https://i0.wp.com/rob.conery.io/img/2019/03/screenshot_1448-1.png?fit=1024%2C525&ssl=1)

The file produced isn't terribly exciting, but it's somewhat useful:

![](https://i0.wp.com/rob.conery.io/img/2019/03/screenshot_1449.png?fit=1024%2C648&ssl=1)

This is where we can embrace the texty nature of Unix and see what's possible if we start jamming binaries together with some core Unix commands, which are all based on text.

Let's use psql to execute a query, but this time we'll format things using Bootstrap:

```
echo "<link rel='stylesheet' href='https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css'>" > report.html && psql chinook -c "select * from film" -H >> report.html && open report.html
```

![](https://i1.wp.com/rob.conery.io/img/2019/03/screenshot_1450.png?fit=1024%2C608&ssl=1)

OK it's certainly not crazy amazing but for a quick report it's not so bad. You can alter the SQL statement to output only the columns you want, and you could formalize the call using a bash function to make it all pretty.

## Yeah But It's Not Management Studio!

Very true. You can't double-click a table and edit the rows, for instance, and there are no spiff icons. Altering data is done with INSERT and UPDATE commands, deleting is done with DELETE. This is something that you do have to get used to, for sure, and if this is a common task for you than you might want to focus on a tool that allows that (such as [Postico](https://eggerapps.at/postico/), which is free).

If there's one reason to use psql it's _speed_. I would also argue _power_ but for now I'll go with speed as the primary reason. You're done before you know what happened and, if you have repetitive tasks, you can save your commands as text files to run as you need, when you need.

Change isn't easy, but the people I know that have made the change use psql on a daily basis and absolutely love it. They also flip into a visual tool as needed. One thing they all agree on, however, is that they don't miss the visual stuff at all.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2019/03/screenshot_1451-825x510.png" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2019/03/screenshot_1451-825x510.png" />
  </entry>
  <entry>
    <title>Random Travel Hacks</title>
    <link href="https://bigmachine.io/posts/random-travel-hacks" rel="alternate" type="text/html"/>
    <updated>2019-02-02T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/random-travel-hacks</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2019/02/screenshot_63.jpg" alt="Random Travel Hacks" /></p>
I’m here in London at [NDC](https://ndclondon.com/) on a lovely Saturday morning and I have the day to myself. I just made myself some coffee and I was recalling a conversation I had last night with some friends that are here about traveling internationally, what to pack, and the little tricks we use.

[I traveled for a year](http://localhost:5555/2014/09/17/being-a-nomad-for-a-year/) with my family (my wife, myself, and my two daughters aged 10 and 13) and we focused on being as minimal as possible. I learned some very interesting ways to be self-sufficient, which I’ll share below in no particular order.

## **Dealing With Jet Lag**

I’m lucky enough to speak (from time to time) at international conferences, and without fail the first topic that comes up with fellow travelers is _sleep_, or the lack of it. Everyone has their way of dealing with – in fact I know a few people who don’t get jet lag at all! – but I haven’t found anything that works as well as the biohack I used last year: _melatonin_.

![](https://i2.wp.com/bigmachine.io/img/2019/02/IMG_0248.jpeg?fit=1024%2C768&ssl=1)

Melatonin, my savior
Melatonin is an over-the-counter, natural thing that you can find in most health stores or [online](https://www.amazon.com/Supplement-Melatonin-L-Theanine-Chamomile-Blackberry/dp/B0145QI7O0/ref=sr_1_2_s_it?s=hpc&ie=UTF8&qid=1549105494&sr=1-2-spons&keywords=melatonin&th=1). It’s all natural and your body produces it in order to regulate your sleep cycle.

When I came back from NDC London last year, I lived in Seattle and we hit a stretch of rain that lasted for a good 3 weeks. It’s really dark this time of year in Seattle so my body simply would not adjust to the time change - there was _no sun_. I tried everything, and finally my wife bought me some melatonin (which she had been suggesting I try _forever_):

> Melatonin is a hormone that regulates sleep-wake cycles. This hormone is primarily produced by the pineal gland. As a medication, it is used for the short-term treatment of trouble sleeping such as from jet lag or shift work.
> From [Wikipedia](https://en.wikipedia.org/wiki/Melatonin)

I wish I would have listened to her sooner, this stuff is _miraculous_. I took it and **that night slept like the big fat baby I am**.

I bought the spray kind that you see above at Whole Foods 3 days before I left, wondering if it would work with the 10 hour time difference I was about to face. We have since moved back to Honolulu and I’d grown used to sunlight and the longer days – I was kind of dreading the jet lag.
I took some on the plane and slept for a solid 6 hours, which I never do on planes! I have been taking it every night since and (I still can’t believe this) didn’t feel _any_ jet lag. Zero. None. CRAZY!

I’m sure this stuff effects people differently, but for me it was a wonder drug.

## **Fewer Clothes, More Laundry**

I haven’t checked luggage in years, reducing everything I need to smaller carryon-sized bags. Recently I managed to travel for 2 weeks with nothing but a simple backpack, which for some people is exceedingly easy, but for me it’s kind of hard. My problem is a simple one: **I really like shoes.**

I’ll talk about shoes later, but I thought I would share with you how I cut my luggage size _in half_ and used only a small backpack.
First thing: _get a good bag_. Spend some money on this and _don’t be cheap about it_.

![The Tom Bihn Synapse 25](https://paper-attachments.dropbox.com/s_4090E8044833D63E1F3FCD590A698B89E2924C07F711E3504C1D3C480CDFB3D1_1629303398533_screenshot_56.jpg)

This is a [Tom Bihn](https://www.tombihn.com/) bag and I’m a freak for these things. I hate to admit it, but I have _five bags made by this company_. Two are laptop bags, one backpack (this one above) and one carry on size (the [Aeronaut](https://www.tombihn.com/products/aeronaut-45?variant=34039227783)) and the middle of the road sized [Tri-Star](https://www.tombihn.com/collections/travel-backpacks/products/tri-star?variant=41947923719). A good bag makes _all the difference_ when you’re traveling for a longer time.

To make this work, you need to convince yourself that you need half as much as you think you do. This is the hardest part and I wish I had a trick for you but I don’t. I deal with this every time I go anywhere.
What has worked for me is the following:

- 4 pairs of underwear
- 3 pairs of socks
- 3 shirts
- 2 undershirts (Under Armour or the like)
- 2 - 3 pants (no jeans!)
- 1 pair of shoes
- Trial-sized or _no_ toiletries
- 1 small bottle of Dr. Bronner’s soap
- Laptop, charger, optional small iPad
- Drying line and camping water heater

All of this will fit easily in the Synapse, the trick is making it fit into your mind! I can’t really help with that part other than to say it’s _really nice_ to pack things up in 10 minutes and not carry around a giant bag!
The key here is the undershirts. I like Under Armour because they’re not cotton (have I said this already), feel great, and I wash _them_ instead of my shirts. I’m being purposely vague about what these shirts are because you pack what you need for your occasion. Dress shirts for work stuff, more casual if you need.

To make this all work properly, we need to break down this list a bit. Let’s start with the underwear.

## **Let’s Talk About Undies**

This tip works for whatever body you inhabit: _good underwear and socks will make the trip exponentially better_. I have a simple rule for traveling and do my best to stick to it: **cotton is your enemy**. I’m not suggesting you run out to your local mountain performance store and stock up on technical gear! I simply mean there are some alternatives you can think about.

I’m a huge fan of [Lululemon](https://shop.lululemon.com/) underwear. The men’s briefs and women’s undies are amazingly comfortable and super easy to clean. I bring two kinds with me: regular briefs and performance briefs (for running or playing basketball). The performance ones are wicking and are _perfect_ for long flights. They’re also great for walking miles and miles, which I love to do in cities like London.

The best part is that they dry fast. This is why you don’t want cotton! You can, literally, wash these things in the sink, roll them in a towel and sit on them for 5 minutes (rolled up) and they’ll be dry enough to put on a walk out the door. No kidding! More on laundry later.
The same thing goes for socks: avoid cotton, get some performance ones. Your feet will seriously thank you.
Finally: _pants_. You can find stretchy cotton pants everywhere these days and they’re great because they wrinkle less than full cotton. Blue jeans are the worst choice because they take forever to dry (which is horrible if you go to a rain climate), don’t insulate well at all, don’t wick worth a damn and tend to “dress you down” unless they’re the expensive kind. They also take up a ton of space and weigh a lot.

For this trip I brought two pairs of [Vans chinos](https://www.vans.com/shop/mens-clothes-pants/authentic-chino-stretch-pant-vans-black#hero=0) and they’ve been perfect:

![](https://paper-attachments.dropbox.com/s_4090E8044833D63E1F3FCD590A698B89E2924C07F711E3504C1D3C480CDFB3D1_1629303430353_screenshot_57.jpg)

**Tip: Recycling Your Clothes is OK**
When we went to Europe for the year I knew we would be facing all kinds of variation in temperature and weather so I came prepared to recycle my clothes. There are a lot of second-hand stores across the world and they’re easy to find - just Google for “Good will store near me” (or Oxfam, Salvation Army, etc).

I often find myself caught out with clothes that are either too cold or too warm. There are bargain stores everywhere or, if you like shopping like I do, just get the thing you need and recycle something you brought with you.

I try not to emotionally attach myself to my clothes. On our big trip I recycled 90% of what I brought with me.

## **Hotel Laundry**

Most people I know have done hotel laundry: dumping some bath gel or shampoo into the hotel sink (or bath tub) and doing it all by hand. This works fine, but there’s a better way! The laundry bag itself:

![](https://i0.wp.com/rob.conery.io/img/2019/02/screenshot_58.jpg?fit=1024%2C646&ssl=1)

Most hotels provide a laundry service and give you bags to put your clothes in that you want cleaned. This service is pretty expensive and you can easily spend $50 on a single load.

Some hotels offer coin-operated laundry as well, but that’s a pain and it also doesn’t apply to us because we’re not bringing enough clothes to justify that much energy/water consumption.

A super-simple easy way to do laundry is to fill the laundry bag to half-full, add a few squirts of your [Dr. Bronner’s](https://www.amazon.com/Dr-Bronners-Pure-Castile-Liquid-Soap/dp/B00120VWJ0/ref=sr_1_1_a_it?ie=UTF8&qid=1549102491&sr=8-1&keywords=bronner&th=1) soap and shake. This will foam everything up and then you dump your clothes in.
You can spin everything by hand or just roll the bag around the tub - but keep it in the tub! These bags can easily pop open!

Here’s another tip: turn the bag upside down while holding it semi-closed. Water will come out and create a siphon within the bag, squeezing out a lot of the water so you don’t have to.

Refill the bag 2 more times for the rinse and you’re done.
**Simple Drying With Hangers and a Line**
Once you’re done with your laundry you’ll be very happy that you _don’t have cotton_. Drying synthetic fabrics is so much faster and simpler.

For this I’ll use the hangers in the closet, which usually have those clips on them, to dry things. If I have more clothes with me (and therefore more laundry) I’ll bring a drying line, which you can find at any camping store. That’s a bit more stuff, however, and recently I’ve let the thing stay at home.

**Quick Dry With a Towel**
As I mention above, if you need some undies quickly you can lay a towel on the floor, put your undies on one side, fold the towel over them and then roll it all up as tightly as you can. Sit on the rolled towel for 5 minutes and you’ll be amazed at how dry everything is.

**Super Quick Dry With an Iron or Blow Dryer**
Be very very careful with this one - if you bought those nice undies you can easily melt them! Make sure you set the iron appropriately.

When I was in college I had a job as a waiter at a Mexican restaurant which made us wear these ridiculous tuxedo shirts. They had to be cleaned and pressed every day, and it was a huge pain. Until I learned how to “shower wash” them.

I was always late, but I had enough time to take a quick shower and bring the shirt in with me. It turns out that shampoo is a pretty good laundry detergent so I would wash the thing quickly, wrap it in a towel to dry it, and then put it under an iron.

Within 5 minutes the thing was almost perfectly dry. You can do the same with a blow dryer as well, which works great for socks.

## **Coffee!**

I like good coffee but when I’m traveling I have to let that idea go. Hotels (especially internationally) will usually offer you instant and if you want anything better you have to go downstairs and wait at breakfast. Even then, it tends to be crap.

This triple sucks when you’re jet lagged, awake with a caffeine headache at 3am and just want _something right now_. For this I turn to [Starbucks VIA Instant](https://athome.starbucks.com/coffees-by-format/starbucks-via-instant/) – my life saver.

There are Starbucks in most airports around the world, including my home airport in Honolulu. Before I go I’ll buy 2-3 (depending on how long I’m gone) boxes of the stuff and put the little packets in the side pocket of my bag:

![](https://i1.wp.com/rob.conery.io/img/2019/02/screenshot_59.jpg?fit=1024%2C601&ssl=1)

Most hotels will give you a carafe of some kind for tea or instant coffee if you’re in Europe, but in the US this often isn’t the case so I bring this thingy with me on every trip:

![](https://i0.wp.com/rob.conery.io/img/2019/02/screenshot_60.jpg?fit=1024%2C625&ssl=1)

These can be hard to find, but I found mine at REI here in the US. Most camping stores have them and you can Google “submersible water heater” to find one near you.

My wife laughed at me when I brought this thing on our big trip, telling me I was being a “techy camper” with goofy gadgets. When she was craving coffee at 4am in our hotel in Iceland that didn’t have a water carafe she gave me a massive hug! That one moment (which has repeated 4 times in my life) will make this gadget a necessity for you.

Most hotels will put coffee mugs in your room, but I don’t like trusting that so I bring my “special cup” with me, which I suppose is a bit of a convenience but this is also where my socks/underwear are packed inside my bag:

![](https://paper-attachments.dropbox.com/s_4090E8044833D63E1F3FCD590A698B89E2924C07F711E3504C1D3C480CDFB3D1_1629305159040_screenshot_61.jpg)

I have yet to find a cup as versatile and useful as this here Yeti. I can put two packets of coffee in here and it stays delightfully hot on these cold London mornings. Later I can put a nice cold beer or some water that I’ve lifted from the breakfast buffet.

## **Shoes**

This is my weakness. _I love a good pair of shoes_ and I tend to lose focus when packing. I convince myself that I’ll need a dress pair (for speaking or just going out) and a pair for walking/exercising. This is almost _never_ the case, but I still give in anyway (as I did on this very trip).

I gave in because I knew it was going to be cold and rainy and that I’d probably soak my shoes more than once. I was right, but I could have done this better!

If you get some comfortable, _non-cotton_ performance socks then a simple pair of trainers or running shoes will dry incredibly fast if your feet get wet. I was thinking about this last night because I went out and bought myself some trainers! I left mine at home fearing the cold and rain - turns out walking 5 miles in a pair of Vans slip ons _really really hurts_.

![](https://i0.wp.com/rob.conery.io/img/2019/02/screenshot_62.jpg?fit=1024%2C573&ssl=1)

Some stylish women’s trainers from [Allbirds](https://www.allbirds.co.uk/products/womens-wool-runners-hackney-cab-black-hackney-cab-black-sole)
You just have to look around a bit and you should find some super comfortable performance trainers that you can use for a run or a nice dinner out.

I found some New Balance on sale for $40 - an amazing deal - and have been wearing them ever since. I soaked them last night as I was walking around but I had my non-cotton (wicking) socks on so my feet were fine. I got home, put the blow dryer on my shoes for 5 minutes and they were passably dry.

## **Fewer/Smaller Toiletries**

Most grocery stores have a “trial section” where you can shrunken versions of most toiletries. Every time I travel I grab:

- A new toothbrush (which I leave behind)
- Toothpaste and deodorant
- A single razor
- Small thing of Advil (just in case)
- Visine (lubricating)

I also bring along nail clippers because it really sucks to find out, after a 5 mile walk, that your toe nails are a bit long (sorry I know that’s gross but hey, we’re adults here).

I leave most of this stuff behind, but if I have room and haven’t used most of an item I’ll bring it back with me.

## **Have Some Tips for Me?**

I’m not a super travel guru, but I have done my fair share and hacked together some fun little tricks. If you think I forgot something or have a question/comment/suggestion, feel free to leave in the comments below.
Cheers!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2019/02/screenshot_63.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2019/02/screenshot_63.jpg" />
  </entry>
  <entry>
    <title>Imposter&apos;&apos;s Handbook, Season 2 is Released</title>
    <link href="https://bigmachine.io/posts/imposters-handbook-season-2-is-released" rel="alternate" type="text/html"/>
    <updated>2019-01-03T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/imposters-handbook-season-2-is-released</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2019/01/slide.jpg" alt="Imposter&apos;&apos;s Handbook, Season 2 is Released" /></p>
Over the winter holiday break (on Christmas Eve, to be precise), [Scott Hanselman](https://hanselman.com) and I released [the next volume in the](http://bit.ly/2F2vS3Z) _[Imposter's Handbook](http://bit.ly/2F2vS3Z)_ [series.](http://bit.ly/2F2vS3Z) It took us just over 18 months to put this thing together and I couldn't be happier with it.

![](https://bigmachine.io/img/mutual-1024x768.png)

Shannon's Second Theorem, Illustrated via Deathstar

## From Logic to Boolean Algebra, Binary to Circuits

There was so much content I had to ax from the first volume of _The Imposter's Handbook_ because I ran out of time and space. For instance: I knew very, very little about binary operations and even less about encryption and I desperately wanted to change that. Unfortunately, it had to wait until I had the time.

That time came over the last 18 months. Scott and I dove into things like binary addition and subtraction, logic gates, and Boolean Algebra.

![](https://blog.bigmachine.io/img/xor-1024x792.png)

Something I could never remember: XOR

It was fun to dive into these subjects but I was not expecting what came next. Even writing this now – I'm struggling to come up with a way to accurately capture the singular importance of one person's work. He's been compared to Einstein, Edison, Newton – all rolled into one.

## Claude Shannon and Information Theory

Claude Shannon created Information Theory with a single paper written in 1948. In it, Shannon detailed a way to describe information _digitally_, that is with 1s and 0s. He detailed how this information might be transmitted in a virtually lossless way and, as if that wasn't enough, he also described _just how much information that digital signal could contain_.

We take this kind of thing for granted today, but keep in mind that in Shannon's day, the only way to communicate over great distances was with telegraph or telephone!

But wait, there's more!

Prior to inventing Information Theory, Shannon wrote a master's thesis that many people regard as the _most important master's thesis ever written_.

## The Creation of Digital Circuits

In the late 1930s, Claude Shannon was working on his master's degree at MIT. He was also working with Vannevar Bush on the Differential Analyzer, a room-sized mechanical computer that would calculate differential equations typically for military use.

![](https://i0.wp.com/bigmachine.io/img/2019/01/Cambridge_differential_analyser.jpg?fit=1024%2C748&ssl=1)

The Twin Cambridge University Differential Analyzer (Public Domain)

This machine had to be programmed by hand, which meant breaking it down and rebuilding it, using rods, wheels, and pullies as variables in a complex ballistic equation. Eventually, many of the mechanical bits were replaced with electrical switches that moved levers here and there, reducing the time it took to break the machine down and rebuild it.

These switches sparked something in Shannon, who recalled a class he took at the University of Michigan called _Boolean Algebra_. Turns out, this chap named George Boole figured out a way to apply mathematical principles to simple logical propositions. Shannon extended those ideas and figured out how the entire room-sized computer could be replaced with a series of _electrical circuits_.

**Claude Shannon invented the digital circuit**.

## Encryption, Hashing and Blockchain

The story kind of wrote itself from that point on. Digital circuits led to digital computers which led to more efficient ways of calculating things which led to the need to transmit that information which led to the need to keep it a secret which led to where we are today.

Scott and I dove into all of this.

![](https://blog.bigmachine.io/img/rsa_1.png)

Cracking a simple asymmetric cipher

We explore SSH keys and how RSA works. We dive into hashes, discussing the goods and bads of each – including how Rainbow Tables can be used to quickly and easily crack credit card information that hasn't been salted.

![](https://i2.wp.com/rob.conery.io/img/2019/01/sha-gringer.png?fit=1024%2C759&ssl=1)

Simplified doodle of SHA256

We eventually end up with a discussion of cryptocurrency and blockchain, detailing why some people love it and others absolutely hate it. Both sides have some pretty good points...

It was a ton of fun writing this book – made even moreso because I got to do it with a friend! [I really hope you enjoy it](http://bit.ly/2F2vS3Z)!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2019/01/slide.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2019/01/slide.jpg" />
  </entry>
  <entry>
    <title>Mod and Remainder are not the Same</title>
    <link href="https://bigmachine.io/posts/mod-and-remainder-are-not-the-same" rel="alternate" type="text/html"/>
    <updated>2018-08-21T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/mod-and-remainder-are-not-the-same</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2018/08/mod_v_remainder.jpg" alt="Mod and Remainder are not the Same" /></p>
Get ready, here comes some fringe pedantry that could very well be the difference in an interview, or the thing that saves you from hours of chasing a production bug!

I'm teeth-deep into the next season of [The](https://bigmachine.io/projects/imposters-handbook-presale) [Imposter's](https://bigmachine.io/products/the-imposters-handbook-season-2) [Handbook](https://bigmachine.io/projects/imposters-handbook-presale) and currently, I'm writing about RSA, the cipher that powers SSH and is, evidently, the most downloaded bit of code in history.

I want to know the story behind this thing. Who came up with it, how it works, why it works, and will it *keep working*. So far, I've dug up **one hell of a story.** I'm not a crypto wonk, but I can see how people get sucked right into this field. I'm not really cut out for it, however, because little rabbit holes exist everywhere and I'm kind of like a magpie-rabbit: _I chase shiny things down deep holes_. I'm also super amazing with metaphors.

Anyway: last week I found out something weird that I thought I would share: *mod and remainder are not the same thing*. The really fun thing about that statement is that there are a small fraction of people reading this, jumping out of their chairs saying "NO SHIT I'VE BEEN TRYING TO TELL YOU AND EVERYONE ELSE FOREVER".

Shout out to mods-not-remainder crew! This one's for you.

## What's a Mod?

I had to look this up to, just like the last time the subject came up. It's one of those things that *I know*, but don't retain. When you "mod" something, you divide one number by another and take the remainder. So: _5 mod 2_ would be 1 because 5 divided by 2 is 2 with 1 left over.

The term "mod" stands for the _modulo_ operation, with 2 being the *modulus*. Most programming languages use `%` to denote a modulo operation: `5 % 2 = 1`.

That's where we get into the weird gray area: *1 is the remainder, not necessarily the result of a modulo.*

### Clock Math

I remember learning this in school, and then forgetting it. There's a type of math called "Modular Mathematics" that deals with cyclic structures. The easiest way to think of this is a clock, which is cyclical in terms of the number 12. To a mathematician, a clock is `mod 12`. If you wanted to figure if 253 hours could be evenly divided into days, you could use `253 mod 24`, which [comes out to 13](https://www.google.com/search?q=253+mod+24) so the answer would be no! The only way it could be "yes" is if the result was 0.

Another question you could answer would be "if I start on a road trip at 6PM, what time would it be when I get to my destination 16 hours later?". That would be `6 + 16 mod 12` which is 10.

Cryptographers love `mod` because when you use it with really large numbers you can create what are known as *one-way functions*. These are special functions which allow you to easily calculate something in one direction, but not reverse it.

If I tell you that 9 is the result of my squaring operation, you can easily deduce that the input was 3 (or -3 as the case may be). You would have the entire process front to back. If I tell you that 9 is the result of my function `mod 29`, you would have a harder time trying to figure out what the input was.

Crypto folks like this idea because they can use a modulo with gigantic prime numbers in order to generate a cryptographic key. That's a whole other story and you can [buy the book](https://bigmachine.io/products/the-imposters-handbook-season-2) if you want to read about it.

I need to stay on track.

## Remainders and Clock Math

Now we get down to it: modulo and remainder act the same when the numbers are positive but much differently when the numbers are *negative*.

Consider this problem:

```
const x = 19 % 12;
console.log(x);
```

What's the value of `x`? Some quick division and we can say there's a single 12 with 7 left over, so 7 is our answer, which is correct. How about this one:

```
const y = 19 % -12;
console.log(y);
```

Using regular math, we can multiply -12 by -1, giving us 12, and we still have 7 left over, so our answer is 7 once again.

JavaScript agrees with this:

![](https://bigmachine.io/img/screenshot_956.png)

C# also agrees with this:

![](https://blog.bigmachine.io/img/screenshot_957.png)

Google agrees with the first statement, but **disagrees with the second**:

![](https://blog.bigmachine.io/img/screenshot_958.png)

Ruby agrees with Google:

![](https://blog.bigmachine.io/img/screenshot_959.png)

**What in Djikstra's name is HAPPENING HERE!**

## Spinning The Clock Backwards

The answer to this problem is understanding the difference between a *remainder* and a *modulo*. **Programmers conflate these** operations and they should not, as they only act the same when the divisor (in our case 12) is positive. You can easily send bugs into production if your divisor is negative.

But why is there a discrepancy? Consider the positive modulo `19 mod 12` using a clock:

![](https://blog.bigmachine.io/img/mod-clock-1.png)

The end result is a 7, as we know, and we can prove this using some math. But what about `19 mod -12`? **We have to use a different clock**:

![](https://blog.bigmachine.io/img/Paper.Imposter-v2.61-2.png)

Our modulus is -12, and we can't ignore that or change it by multiplying by -1 as that's not the way modular math works. The only way to calculate this correctly is to relabel the clock so that we progress from -12, or spin the clock counterclockwise, which yields the same result.

Why don't I number the clock starting with -1 and moving on to -2, etc? *Because that would be moving backwards*, continually decreasing until we hit -12, at which point we make a +12 jump, which isn't how modulo works.

## This Is a Known Thing

Before you think I'm nuts and start Googling on the subject: [it's been known for a while](https://github.com/ramda/ramda/issues/186) In fact, MDN (Mozilla Developer Network) goes so far as to call `%` the [remainder operation](<https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Arithmetic_Operators#Remainder_()>):

> The **remainder operator** returns the remainder left over when one operand is divided by a second operand. It **always takes the sign of the dividend**.

Eric Lippert, one of the gods of C#, [says this](https://blogs.msdn.microsoft.com/ericlippert/2011/12/05/whats-the-difference-remainder-vs-modulus) about C#'s modulo:

> However, that is not at all what the % operator actually does in C#. The % operator is not the canonical modulus operator, it is the remainder operator.

What does your language do?

## So What?

I can understand if you've made it this far and are scratching your head some, wondering if you should care. I think you might want to for 2 specific reasons:

1. I can see this coming up in an interview question, catching me completely off guard and
2. I can see pushing a bug live and spinning for hours trying to figure out why math doesn't work

It could also be a fun bit of trivia to keep in your pocket for when your pedantic programmer friend drops by.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2018/08/mod_v_remainder.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2018/08/mod_v_remainder.jpg" />
  </entry>
  <entry>
    <title>Transactional Data Operations in PostgreSQL Using Common Table Expressions</title>
    <link href="https://bigmachine.io/posts/transactional-data-operations-in-postgresql-using-common-table-expressions" rel="alternate" type="text/html"/>
    <updated>2018-08-13T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/transactional-data-operations-in-postgresql-using-common-table-expressions</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2018/08/gear-2291916_1280.jpg" alt="Transactional Data Operations in PostgreSQL Using Common Table Expressions" /></p>
PostgreSQL has a ton of amazing features, often overlooked by developers who prefer to use abstractions to work with SQL and their database. This is a big topic and obviously can spark a ton of debate. If you've read my blog before [you know I dislike ORMs tremendously](/2015/02/20/its-time-to-get-over-that-stored-procedure-aversion-you-have/) ... aside from LLBLGenPro because Frans is my friend and he doesn't like it when I trash ORMs.

One of the great features of PostgreSQL is [Common Table Expressions, or CTEs](https://www.postgresql.org/docs/10/static/queries-with.html), otherwise known as "WITH queries". These are simply chained SQL expressions that allow you to pass the result of one query into another, functional style.

I use them a lot for reporting, but I also use them for creating orders when someone [buys something from me](https://bigmachine.io/products). Let's see how.

## Setting Up The Database

Let's create the core of my database. These tables are simplified, but the core of what they're supposed to do is present:

```sql
create extension if not exists pgcrypto;

create table orders(
  id serial primary key,
  key uuid unique default gen_random_uuid(),
  email text not null,
  total decimal(10,2),
  created_at timestamptz default now()
);

create table order_items(
  id serial primary key,
  order_id int not null references orders(id) on delete cascade,
  sku text not null,
  price decimal(10,2) not null,
  quantity int not null default 1,
  discount decimal(10,2) not null default 0
);

create table downloads(
  id serial primary key,
  key uuid unique not null default gen_random_uuid(),
  order_id int not null references orders(id) on delete cascade,
  order_item_id int not null references order_items(id) on delete cascade,
  times_downloaded int not null default 0
);

create table products(
  id serial primary key not null,
  sku text unique not null,
  name text not null,
  price decimal(10,2) not null,
  created_at timestamptz not null default now()
);
```

I'm showing you this code for a few reasons:

1. If you want to play along (which I encourage), you can
2. The defaults and structure make working with CTEs much simpler
3. SQL is straightforward and easy if you take the time to learn it

All of that said, there are a few things I'd love to call out about this design:

- I'm using `on delete cascade` for the foreign keys to ensure that I don't have orphans
- I'm ensuring that null values don't creep into my database
- Whenever I have a `not null` constraint, I try to make sure I set a `default`
- Finally, `gen_random_uuid` comes from the `pgcrypto` extension

OK, let's add some data to our products table:

```sql
insert into products(sku, name, price)
values
('imposter-single','The Imposter''s Handbook', 30.00),
('mission-interview','Mission:Interview',49.00);
```

Great. Now let's get to the good stuff.

## Problem 1: Saving Order Data Transactionally

When a new order comes in, I need to create an `order` record and then an `order_items` record. This _must_ be done in a transaction or Bad Things will happen. This is simple to do with a CTE, **because CTEs execute within a single transaction**:

```sql
with new_order as(
  insert into orders(email, total)
  values ('rob@bigmachine.io',100.00)
  returning *
), new_items as (
  insert into order_items(order_id, sku, price)
  select new_order.id, 'imposter-single',30.00
  from new_order
  returning *
)
select * from new_items;
```

When you insert a new record with PostgreSQL, you can ask for it right back with `returning *`. If you just want the `id`, you can add `returning id`. The first query inserts the new `order` and then returns it, which I can then use in the second query. Obviously: hard-coding values isn't a good idea, but I'll fix that in a moment.

The result:

```
 id | order_id |       sku       | price | quantity | discount
----+----------+-----------------+-------+----------+----------
  1 |        1 | imposter-single | 30.00 |        1 |     0.00
(1 row)

```

Perfect.

## Problem 2: Creating Downloads From Our New Order

I'm starting simple, making sure things work as intended, then moving forward. My next step is to create downloads so that users can download what they've bought immediately. For that, I can chain a new query:

```sql
with new_order as(
  insert into orders(email, total)
  values ('rob@bigmachine.io',100.00)
  returning *
), new_items as (
  insert into order_items(order_id, sku, price)
  select new_order.id, 'imposter-single',30.00
  from new_order
  returning *
), new_downloads as (
  insert into downloads(order_id, order_item_id)
  select new_order.id, new_items.id
  from new_order, new_items
  returning *
)

select * from new_downloads;
```

I tack on a `returning *` from my insert statement for `order_items` and then I can use that to generate the downloads in a third query, this time using a `select` for the insert.

The result:

```sql
 id |                 key                  | order_id | order_item_id | times_downloaded
----+--------------------------------------+----------+---------------+------------------
  1 | 1fa7c369-94c4-4220-83ba-78e35cfc7377 |        1 |             1 |                0
(1 row)
```

Great! The best part of all of this, so far, is that I can feel good about the data going into my database because of my constraints and design, and I can have faith that it's been added correctly because **a CTE is wrapped in a single transaction**.

## Problem 3: Inserting Multiple Order Items

One of the minor drawbacks of a CTE is that you can only execute a single statement with each clause. If you think of this in functional programming terms, it's a bit like _currying_ in that you have a single argument (the result of the previous query) and a single function body that returns a single value.

How, then, would you insert multiple `order_items`? This is where things could get a little tricky, especially if you're not a fan of SQL. I like using it, so what you're about to see doesn't freak me out:

```sql
with new_order as(
  insert into orders(email, total)
  values ('rob@bigmachine.io',100.00)
  returning *
), new_items as (
  insert into order_items(order_id, sku, price)
  (
    select new_order.id, sku,price
    from products, new_order
    where sku in('imposter-single','mission-interview')
  )
  returning *
), new_downloads as (
  insert into downloads(order_id, order_item_id)
  select new_order.id, new_items.id
  from new_order, new_items
  returning *
)

select * from new_downloads;
```

I'm getting around the problem by using a simple `select` statement for the insert. It's going to the `products` table to insert whatever SKUs are given to it. Let's run the query and see what happens:

```
 id |                 key                  | order_id | order_item_id | times_downloaded
----+--------------------------------------+----------+---------------+------------------
  1 | 6e695533-dd8d-407d-bdca-d71f81c666fb |        1 |             1 |                0
  2 | 31f81049-d08a-4f4c-b30c-565169178268 |        1 |             2 |                0
(2 rows)
```

It works! Sort of. We have one last problem...

## Hard Coding, Data Integrity, and Validation?

I'm hard-coding the email address as well as the SKUs, which isn't a Good Thing, obviously. This is where we brush up against what your ORM wants to do for your vs. what you might want to do as a coder. Put another way: _would you really write this SQL in your code?_. **I certainly wouldn't**.

Here are some possible solutions to these issues.

### The SKU Problem

What if a SKU is passed to this SQL that is not in our product's table? The short answer is non-compelling: _nothing_. If a SKU isn't found in the `products` table, it will simply be ignored. This is _sub-optimal_ because we can end up adding crap data to our system!

How can we fix this? My first inclination would be to wrap this routine in a function, passing in the email, SKUs and everything else in a `jsonb` variable called `cart`. In my function, I would make sure the cart totals matched and that the SKUs were present in the database.

This is where you rip me apart for putting business logic in a database. I can understand why people think that, however I can also understand **why I do it anyway**. The answer is simple: I'm more likely to change my platform/ORM than I am PostgreSQL. To me, this kind of thing belongs as close to your data as possible. It's a simple data operation that's not exactly unique to my business, is it?

The other solution is to make sure the cart is validated before it gets pushed to this query. If we can trust the inputs, then we're good to go.

### Blobs of SQL In Your Code

I think SQL is a beautiful thing, but that's _my_ problem. Yours is trying to figure out where to put this stuff! One thing you could do is to store this as a [prepared statement](https://www.postgresql.org/docs/10/static/sql-prepare.html), which offers quite a few benefits. Let's see the code, then we'll dive into the benefits:

```sql
prepare new_order(text, decimal(10,2), text[]) as
with new_order as(
  insert into orders(email, total)
  values ($1,$2)
  returning *
), new_items as (
  insert into order_items(order_id, sku, price)
  (
    select new_order.id, sku,price
    from products, new_order
    where sku = any($3)
  )
  returning *
), new_downloads as (
  insert into downloads(order_id, order_item_id)
  select new_order.id, new_items.id
  from new_order, new_items
  returning *
)
select * from new_downloads;
```

Whenever you write a SQL statement for PostgreSQL, the engine needs to parse the SQL, analyze it, and then optimize/rewrite it for execution. You can skip a number of those steps if you tell PostgreSQL the query you plan on running ahead of time, so it can parse and analyze it **once**. You can do this with the `prepare` statement.

The downside is that you need to use positional arguments, as you see I'm doing here with `$1, $2` etc, which means you lose a little readability. If you can get over that, executing this statement means that you can call it by name using `execute` and some parameters:

```sql
execute new_order('rob@bigmachine.io',100.00, '{imposter-single,mission-interview}')
```

You'll notice the funky `{imposter-single}` syntax - that's how you work with arrays in PostgreSQL. Since I've switched to working with arrays, I've opted to use the `any` keyword, which works like `in` but is specifically for array values.

## Summary

Long post, but I encourage you to explore and see what's possible with your database, even if it's not PostgreSQL. The SQL I wrote above would likely replace 100s of (total) lines of ORM code and orchestration, but yes there is a learning curve.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2018/08/gear-2291916_1280.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2018/08/gear-2291916_1280.jpg" />
  </entry>
  <entry>
    <title>Simple Monthly Reports in PostgreSQL Using generate_series</title>
    <link href="https://bigmachine.io/posts/simple-monthly-reports-in-postgresql-using-generate_series" rel="alternate" type="text/html"/>
    <updated>2018-08-01T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/simple-monthly-reports-in-postgresql-using-generate_series</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2018/08/graph-3033203_1280.jpg" alt="Simple Monthly Reports in PostgreSQL Using generate_series" /></p>
I have a reporting backend for [my book/video business](https://bigmachine.io) that has one chart which I stare at every day: **the daily sales**:

![](https://bigmachine.io/img/screenshot_929.jpg)

I use Google Analytics religiously, but it's not reliable for ecommerce because ad blockers will also block Google Analytics so a number of sales simply aren't recorded.

Anyway: _I need to roll my own reporting_ if I want to see anything of substance, which is fine as love playing with PostgreSQL. When you do that, however, you run into some interesting problems. Such as this one:

![](https://blog.bigmachine.io/img/screenshot_930.jpg)

Today is the first day of the month, so the chart only has a single value and the formatting is completely off. In fact it's off every day! This has been bugging me for a while, so today I decided to fix that.

## Generating a Series of Dates

The problem is straightforward: _I need to see all the days in a given month_. PostgreSQL has [extensive date functions](https://www.postgresql.org/docs/10/static/functions-datetime.html), but nothing (that I've seen) that will just spit out the dates in a given month.

To get around this, I'll rely on an [old friend](https://www.postgresql.org/docs/10/static/functions-srf.html): `generate_series`.

There's no surprise with this function, it does what you might expect, creating a logical series from a seed and bound:

```
rob=# select * from generate_series(1,10);
  generate_series
-----------------
               1
               2
               3
               4
               5
               6
               7
               8
               9
              10
(10 rows)
```

You can also add a step with a third argument:

```
rob=# select * from generate_series(1,10,2);
 generate_series
-----------------
               1
               3
               5
               7
               9
(5 rows)
```

This is where things get usefully mindblowing: _it also works with dates and intervals_:

```
rob=# select * from generate_series(now(), now() + '5 days', '1 day');
    generate_series
-------------------------------
 2018-08-01 14:10:52.380404-07
 2018-08-02 14:10:52.380404-07
 2018-08-03 14:10:52.380404-07
 2018-08-04 14:10:52.380404-07
 2018-08-05 14:10:52.380404-07
 2018-08-06 14:10:52.380404-07
(6 rows)
```

Interval syntax is one of the things I absolutely **love** about working with PostgreSQL and dates. I know that many people don't like arbitrary strings to represent something, but I think you can probably get over that with the obvious "1 day" syntax, don't you think?

## Generating a Series of Days Within a Month

The easiest thing to do is to pass in dates for the start and end of the month:

```sql
select * from generate_series(
    '2018-08-01'::timestamptz,
    '2018-08-31'::timestamptz,
    '1 day'
);
```

That works as expected, but it's cumbersome. This is where PostgreSQL can help us with some date functions. What I need is to "round down" the month to day one, and I can do that using a `date_trunc`, which truncates a date to a specified precision:

```sh
rob=# select date_trunc('month',now());
       date_trunc
------------------------
 2018-08-01 00:00:00-07
(1 row)
```

I can use this same trick to get the last day of the month, using interval syntax:

```sh
rob=# select date_trunc('month',now()) + '1 month'::interval - '1 day'::interval as end_of_month;
      end_of_month
------------------------
 2018-08-31 00:00:00-07
(1 row)
```

That looks nuts, doesn't it? Here's what's happening:

- the `date_trunc` function is returning a `timestamp with time zone` (or `timestamptz`)
- I am then incrementing that `timestamptz`, which is `2018-08-01 00:00:00-07` by a month, making it `2018-09-01 00:00:00-07`
- I don't want the start of September, I want a single day before that, so I decrement it by a day using `- '1 day'`

That's that. I can now plug this into `generate_series`:

```sql
select * from generate_series(
    date_trunc('month',now()),
    date_trunc('month',now()) + '1 month' - '1 day'::interval,
    '1 day'
) as dates_this_month;
```

Which returns every date, in order:

```
...
 2018-08-25 00:00:00-07
 2018-08-26 00:00:00-07
 2018-08-27 00:00:00-07
 2018-08-28 00:00:00-07
 2018-08-29 00:00:00-07
 2018-08-30 00:00:00-07
 2018-08-31 00:00:00-07
(31 rows)
```

## Turning Our Date Range Into a Usable Table

I could plug this SQL into a bigger query and use it straight away, but it's way too useful for that. Let's wrap it with a function, shall we? That way we can pass in whatever date or month we want to use:

```sql
-- this is
create function dates_in_month(the_date timestamptz=now())
returns table(the_date date) as $$
select d::date from generate_series(
    date_trunc('month',the_date),
    date_trunc('month',the_date) + '1 month' - '1 day'::interval,
    '1 day'
) as series(d);
$$
language sql;
```

A few things to note:

- I'm defaulting `the_date` parameter to today's date for convenience
- You can send in any date, and the month of that date will be used in the function
- I'm casting the series return as a `date` because that's what it is; a `timestamptz` here is useless
- To cast that, I need to alias the function to explicitly return it's inline value (`d`)

This works great:

```sh
rob=# select * from dates_in_month();
  the_date
------------
 2018-08-01
 2018-08-02
 2018-08-03
 ...
 2018-08-28
 2018-08-29
 2018-08-30
 2018-08-31
(31 rows)
```

Now I just need to use it in a sales query.

## Joining Things Together To Produce The Chart

I have a view in my database called `sales_fact` that sums up the order totals, their count, and expresses the dates in a number of ways. Here it is:

```sql
create view sales_fact as
  select sum(total) as sales,
  count(1) as sales_count,
  created_at::date as sales_date,
  date_part('year',created_at at time zone 'hst') as year,
  date_part('quarter',created_at at time zone 'hst') as quarter,
  date_part('month',created_at at time zone 'hst') as month,
  date_part('day',created_at at time zone 'hst') as day
from orders
group by orders.created_at
order by orders.created_at
```

I want to join those numbers to my date series so I can have every day represented in my chart, not just a fat blue blob. To do that, I can use a simple left join:

```sql
select
  the_date,
  sum(sales) as sales,
  sum(sales_count) as sales_count
from days_in_month()
left join sales_fact on the_date = sales_fact.sales_date
group by days_in_month.the_date
```

Boom. Works great:

![](https://blog.bigmachine.io/img/screenshot_931.jpg)

PostgreSQL is a joy to work with, and solutions to common problems are often right around the corner.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2018/08/graph-3033203_1280.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2018/08/graph-3033203_1280.jpg" />
  </entry>
  <entry>
    <title>Setting Up a Fast, Comprehensive Search Routine With PostgreSQL</title>
    <link href="https://bigmachine.io/posts/setting-up-a-fast-comprehensive-search-routine-with-postgresql" rel="alternate" type="text/html"/>
    <updated>2018-07-23T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/setting-up-a-fast-comprehensive-search-routine-with-postgresql</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2018/07/maze-2264_1280.jpg" alt="Setting Up a Fast, Comprehensive Search Routine With PostgreSQL" /></p>
One of the joys of working with PostgreSQL is the ability to run full-text searches right out of the box. But how do you set this up? Better yet: when should you use full-text indexing and how would you get it working across your entire site?

I've implemented this a number of times for myself and also my clients. I'll show you two strategies I've used, both spanning multiple tables.

## Annie Admin Needs To Find Something

Let's use the typical ecommerce database for this example. Shopify is a grand example: they have a search bar across the top of the admin site that will search all kinds of information based on what you enter, including customers, orders, help, etc:

![](https://bigmachine.io/img/shopify_search.png)

It's kind of neat. As you can see, I triggered this search by entering the letter "t" in the search box. Kind of meaningless, really, and not something that will work well with a full-text index.

This is where we get to refine our search requirements.

### Use Case 1: Annie Needs to Find Joe User

Let's work with this use case:

> Annie gets an email from Joe User who wants to know where his downloads are. He doesn't have an order number, just a name/email. He doesn't know if the email is the same one he used when placing the order.

There's nothing fancy that needs to happen here, just a fuzzy search. But over what? Annie is a crafty data person as well and knows that she could run afoul of her database admin friend M. Sullivan, so she wonders how she can execute this search using an index.

### Solution 1: Simple Regex with a UNION Query

Annie's right to be worried about index usage, _in general_, but in her case, she's just one person executing a crappy query, and not very often at that. She can get away with a true fuzzy search here.

To do this right, she needs to search over:

- The Customers table, in case she wants to see all the information about Joe
- The Orders table, so she can see all of Joe's orders at a glance
- The Invoices table, which has the delivery information so she can send an email if needed, or correct an errant email address

There could be more, but let's start here.

The first step is to decide the structure of our search _result_. This is going to be the shape of the information returned from the UNION query. For now, I'll keep it simple:

- `id`, the PK of the thing found
- `key`, an identifier of some kind, like order number, email, or the like
- `description`, some kind of longer bit of information, such as a user's name, order number and date, etc
- `type`, this will identify what the resource is (order, customer, etc)
- `blob`, this is a blob of text we'll be searching over

Here's what that query might look like for the orders table:

```sql
select
id,
number as key,
concat('Order ', number, ' placed on ',created_at) as description,
concat(number,' ',email,' ', name) as blob,
'order' as type
from orders;
```

Here's what that returns, using real data from my site (with bits blurred out):

![](https://blog.bigmachine.io/img/screenshot_922-1.jpg)

Great. Now we need to query the other tables, making sure to follow the same structure:

```sql
select
id,
number as key,
concat('Order ', number, ' placed on ',created_at) as description,
concat(number,' ',email,' ', name) as blob,
'order' as type
from orders
UNION
select
id,
email as key,
name as description,
concat(name,' ',email) as blob,
'customer' as type
from customers
UNION
select
id,
number as key,
concat('Invoice ', number, ' created on ',created_at) as description,
concat(number,' ',email,' ', name) as blob,
'invoice' as type
from invoices
```

Ahh joy, an unbounded UNION query that's likely to get Annie fired. Let's fix that.

### Using a Materialized View for Speed and Excitement

If we had to run this query against a live data set, we'd make our DBA mad. Even if it's just "every now and again", we're pulling a giant amount of data into a query and it hurts. How bad does it hurt? Of course Annie has done an EXPLAIN/ANALYZE so she knows how much trouble she'll be if she doesn't optimize:

![](https://blog.bigmachine.io/img/screenshot_923.jpg)

You don't know my data, but I'll just tell you that this is doing not just one, not two, but **three full table scans** and a bunch of other crappy bad things. We can't do this, so we'll turn to one of the greatest things in PostgreSQL: _the materialized view_. It's basically a query that is cached for you, in memory, that you can also throw an index on (which we'll do in a bit):

```sql
create materialized view admin_search as
-- big UNION query here
```

That's it. Now we can run a _much faster_ and simpler query:

![](https://blog.bigmachine.io/img/screenshot_925.jpg)

Annie doesn't like simply trusting her eyes, she want's PostgreSQL to tell her if this query is indeed faster so she uses EXPLAIN/ANALYZE:

![](https://blog.bigmachine.io/img/screenshot_926.jpg)

There is a sequential scan, but the query is much more efficient than before.

### The Downside of a Materialized View

A materialized view is a cached set of data that you can query instead of querying the tables themselves. That cache doesn't get reloaded unless you:

```sql
refresh materialized view admin_view;
```

This will reload our cached data into memory. You can also do this concurrently if you have a long-running view and you don't want to lock the view from use:

```sql
refresh materialized view concurrently admin_view;
```

This will run in the background and is great if you're hitting the view often. There are [some exceptions](https://www.postgresql.org/docs/9.4/static/sql-refreshmaterializedview.html), however:

- This option is only allowed if there is at least one UNIQUE index on the materialized view which uses only column names and includes all rows; that is, it must not index on any expressions nor include a WHERE clause.
- This option may not be used when the materialized view is not already populated.
- Even with this option only one REFRESH at a time may run against any one materialized view.

In Annie's case, an hourly cron should do the trick. Not the most elegant solution, but it serves her purpose.

## Use Case 2: Annie Gets In Trouble Anyway and Needs an Index

Annie got in trouble anyway, which is a bummer. The refresh on the materialized view wasn't that big of a deal, but the sequential scan over 10s of thousands of records made the DBA twitch violently.

### Solution 2: Use Full Text Indexing with a GIN Index

With just a few tweaks, Annie can fix this issue. Instead of using a regex operation (`~*`) in the where clause, she can use the rocket-fueled full-text engine that ships with PostgreSQL.

This is done with a simple tweak, using `to_tsvector` on the `blob` field and then popping a GIN index on that `ts_vector`ed goodness:

```sql
drop materialized view admin_view;
drop index if exists idx_search;
create materialized view admin_view as
select
id,
number as key,
concat('Order ', number, ' placed on ',created_at) as description,
to_tsvector(concat(number,' ',email,' ', name)) as search,
'order' as type
from orders
UNION
select
id,
email as key,
name as description,
to_tsvector(concat(name,' ',email)) as search,
'customer' as type
from customers
UNION
select
id,
number as key,
concat('Invoice ', number, ' created on ',created_at) as description,
to_tsvector(concat(number,' ',email,' ', name)) as search,
'invoice' as type
from invoices;

create index idx_search on admin_view using GIN(search);
```

This creates a materialized view as before, but adds the tokenized full-text bits that PostgreSQL needs in order to run a full-text query:

```sql
select id,key,description,type
from admin_view
where search @@ to_tsquery('joe')
order by ts_rank(search,to_tsquery('joe')) desc;
```

This comes back at lightning speed with the following results:

![](https://blog.bigmachine.io/img/screenshot_927.jpg)

Dig that! Ranked search results that identify 3 customers named "Joe" right at the top. Much more useful and incredibly efficient. Dig this:

![](https://blog.bigmachine.io/img/screenshot_928.jpg)

This loose text term is using a Bitmap scan (which is a good thing) and executes _in under a millisecond_. That's **winning, people**.

Annie gets a promotion and everyone's happy.

### The Downside of a Full Text Index

To be honest, it's not really made for small, loose searches like this and it's really easy to generate a false positive. Full-text indexing really shines over things like blog posts, comment searches, and so on.

It worked pretty dang well here though, didn't it?]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2018/07/maze-2264_1280.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2018/07/maze-2264_1280.jpg" />
  </entry>
  <entry>
    <title>Working Smarter, Not Harder, Part 1</title>
    <link href="https://bigmachine.io/posts/working-smarter-not-harder-part-1" rel="alternate" type="text/html"/>
    <updated>2018-07-18T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/working-smarter-not-harder-part-1</id>
    <summary type="text">This last month has been intense. I&apos;m writing the second volume of The Imposter&apos;s Handbook\uFEFF with Scott Hanselman, I moved back to Hawaii, and I&apos;m trying to finish up a sprint for a contract I&apos;ve been...</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2018/07/son-2935723_1280.jpg" alt="Working Smarter, Not Harder, Part 1" /></p>
I'm sitting here at this very moment in our new, just-moved-in-and-insanely-messy apartment in Kaka 'Ako, with 90 minutes to write this post, according to my calendar on Outlook. My youngest (13) is staring at the walls as my wife and oldest (16) are on a road trip in the Pacific Northwest. Oh yeah: I'm also trying to workout every day with P90X.

To pull this off, I needed to streamline. Seeing that I have a 90-minute break to write a blog post, I thought I would share with you some of my strategies.

## Shell Scripts as Employees

One of the recurring discussions I have with friends who have started businesses centers around a single question: *when do you hire someone to help you?*﻿ One of my friends doesn't want to do anything but sell, so he hires help immediately from Upwork. Another friend refuses to hire anyone at all and enjoys turning work away in an effort to remain small.

I like to write shell scripts.

Let me give you an example. If you go through your day and document the tasks that:

1. Take the longest
2. Are the most repetitive and
3. The easiest to delegate

These, to me, are prime candidates to offload to an assistant. Yes, I realize that's different than an employee who could do specialized tasks (such as edit and render a video, for instance) but right now what I need is an assistant.

I did this last month, and it was surprising how much time I spent rolling together demos for [the book I'm writing](https://bigmachine.io/projects/imposters-handbook-presale/). I'm using Ruby (as well as JavaScript), and for each demo, I'm creating a dedicated project. This is time-consuming!

It's a long story, but I was creating a mess of Sinatra apps and setting up my Gemfile, RSpec and so on was taking way too long. Yes, I could copy/paste, but I decided to do it a bit cleaner and spent 15 minutes writing up a shell script:

```sh
alias spec="bundle exec rspec spec --format documentation"
ruby_app() {
  if [ $# -eq 0 ]
    then
      echo "Need an app name dope"
  else
    APP_DIR=$1

    #create the directory
    mkdir $APP_DIR
    cd $APP_DIR

    #main app bits
    touch app.rb

    #Gemfile
    cat <<gemfile > Gemfile
source 'https://rubygems.org'

gem 'rspec'
#gem 'pg'
#gem 'sinatra'
#gem 'sinatra-contrib'
gemfile

    #Rbenv
    rbenv local 2.3.3

    #Makefile
    echo "gems:\r\n\tbundle install --path=vendor\r\ntest:\r\n\trspec spec" > Makefile
    cat <<makefile > Makefile
all:

app:
    ruby app.rb

test:
    rspec spec

gems:
    bundle install --path=vendor

db:
    #psql stuff here

.PHONY: all web test
makefile

    #get the gems
    bundle install --path=vendor

    #initialize rspec
    rspec --init
  fi
}
```

This script does a few things:

- It creates the application directory and sets the local Ruby using rbenv local
- It sets up RSpec and the test directory
- It tells bundler to use a local directory for gem installs
- It creates my standard Makefile that I like to use with web projects

At the very top there, you can see an `alias` command as well that will run RSpec using bundler. I use [Robby Russel's Oh My ZSH](https://github.com/robbyrussell/oh-my-zsh)! so this entire script is loaded into the shell when I start up the terminal.

I have a few other shell scripts (written in bash and Ruby) that I use to optimize my time. I have one that CURL's out to my Shopify site and pulls down the daily sales numbers, another that creates a Jekyll blog post for me. There is a lot more I can do this way, and I'll share when I can.

## Back to Outlook

This one is probably obvious to a lot of people and repulsive to others. A huge chunk of my day centers around email (still), and I can't afford to use the "just check it twice" strategy as I am also the sole support person for my company.

The best email experience I've had, ever, is with Outlook. I learned how to use it many years ago, and I had a nice mess of rules set up to help me organize my day. There are countless strategies out there, but if you're like me, giving yourself completely over to a single method just doesn't work.

Here are the things I need to have happen:

- Support emails need to bubble up
- List emails run the gauntlet
- Family Friends are moved to "Social Time"
- Everything else dies

This is the process I've pulled together over the last 3 months, which seems to be working:

- Go through my inbox every 2 hours or so and highlight the crap, deleting it immediately.
- If it's not support/family, I decide whether to read it quickly. If I won't, it's gone.
- Family gets a quick read. My wife will always get a reply, others get Shift-CMD-T'd (mark unread) unless I have time to read them.
- Support emails get a quick read/response. If I can't answer a question, it gets a CTRL-1 (create a task from email due today). If it's something that can wait, CTRL-2 (create a task due tomorrow)

I'll take the time to handle support emails when they come in, which means I do them every 2 hours or so. The unread bits from my family get dealt with at lunch and finally at 4 pm, toward the end of my day.

I reply immediately to the support emails that I can't solve and that have been CTRL-1'd, and I let the customer know I'm working on it. I usually handle these things right away, but if it's not pressing I'll wait until lunch or later in the day around 4. If it's really not pressing (like "hey I have some spelling suggestions for you") I'll CTRL-2 it for the next day.

I'm kind of lucky this way, as I don't have a flood of meeting requests and emails from co-workers. If so, I [might have to adopt some of Hanselman's rules](https://www.hanselman.com/blog/TheThreeMostImportantOutlookRulesForProcessingMail.aspx).

## 2 Hour Lunches

Of all the things! How can I possibly justify taking 2-hour lunches when my time is so very limited! **Because I'm not a sack of meat**. I can't burn myself up like I did when I was younger. _I need to work smarter, not harder_ and that means being refreshed and ready to go when I sit down to do something.

I start work right around 8:30 or 9 am, and then stop at 12 (which is in 30 minutes as I write this). During the hours from 12 until 2pm, I exercise, go for a walk or if it's summer vacation, do something with my kids if they're home. Today we're going to Office Depot to get school supplies.

I'll also be sure to flop on my back and read a book for at least 30 minutes. Yesterday it was _The Code Book_ by Simon Singh, where I read up on RSA and asymmetric key encryption. Sometimes I even doze off!

I'm back at it by 2:30 or so, then quit by 6. And yes: _I know I'm lucky_ to set my own hours like this. [I really like working on my own](https://bigmachine.io/products/going-solo/) and I've fought quite hard over my career to be as independent as I can.

OK - time's up! Gotta run to Office Depot and get some school stuff!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2018/07/son-2935723_1280.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2018/07/son-2935723_1280.jpg" />
  </entry>
  <entry>
    <title>A Pure PostgreSQL Document Database API</title>
    <link href="https://bigmachine.io/posts/a-pure-postgresql-document-database-api" rel="alternate" type="text/html"/>
    <updated>2018-07-05T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/a-pure-postgresql-document-database-api</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2018/07/lamp-3121677_1280.jpg" alt="A Pure PostgreSQL Document Database API" /></p>
One of the great things about PostgreSQL is its support for JSON document storage. I've [written](/2016/02/26/jsonb-and-postgresql/) about it [quite](/2015/08/19/designing-a-postgresql-document-api/) a [few](/2015/08/21/postgresql-document-api-part-2-full-text-search-and-bulk-save/) [times](/2015/08/24/postgresql-document-api-part-3-finding-things/), and here I am writing about it once again! I think it's probably the most underrated feature of PostgreSQL, and for a simple reason: _the API is a bit clunky_.

With a little work, however, that can be changed.

## A Typical Document Database API

There are a few things I'd like to see supported right "out of the box", so to speak:

- Creation of a document table on the fly
- Support for upsert
- A simple CRUD scenario
- A find routine that matches on document criteria
- Support for grouping/mapping/reducing
- Support for full-text search

Seems reasonable, doesn't it? This kind of thing is basic for MongoDB, CouchDB, and RethinkDB... aside from full-text search, which isn't supported. I've implemented these things in code before, with [MassiveJS](https://github.com/dmfay/massive-js), [Moebius](https://github.com/robconery/moebius) and lately, [MassiveRB](https://github.com/robconery/massive-rb). I do this for fun, mostly, but also because I use these things in production and I really like the document abstraction.

## Why Would You Do This To Lovely, Relational PostgreSQL?

It's a good question. If you pick Postgres it's likely you want to go with the relational model. If you want a document system, you'll probably go with MongoDB or something similar. The crazy thing is: _PostgreSQL is unreal fast/scalable with document storage_. [Have a Google](https://www.google.com/search?q=postgres+jsonb+vs+mongodb&oq=postgres+jsonb+vs+mongodb) and see what others have come to know: _Postgres document storage is crazy good_. The only problem is the API, which we've already discussed. We're here to figure out _why_ you would do such a thing.

The simple reason is design/development time speed (among other things). Ditch migrations altogether and just store things as documents. When you're ready, move into a relational model you feel good about. This is exactly what I did with the last 3 projects I worked on and it was amazingly helpful.

## First Pass at a Pure PostgreSQL API

A few months ago I spent the weekend putting together a set of functions that extend PostgreSQL and embrace document storage using the API specification above. The first thing I did was to create a schema to keep all of the bits together in one place:

```sql
drop schema if exists dox cascade;
create schema dox;
```

Yes, I decided to call it `dox` because... just because. The next thing was to create a save routine, the CRUD bits, and to implement full-text indexing. Rather than walk you through all of the code, you can just [have a look at it right here](https://github.com/robconery/dox).

It's not a "true" extension written in C or anything; just a set of PostgreSQL functions written in PLPGSQL. To use it, you invoke the functions directly:

```sql
select * from dox.save(table => 'customers', doc => '[wad of json]');
```

The "fat arrow" syntax you see here is using the named argument syntax for PostgreSQL functions, which (to me) makes things much more readable than positional arguments. The `save` function will create the `customers` table for you on the fly if it doesn't exist and save the JSON you pass to it.

Your document will be indexed using GIN indexing, which means you can run queries like this incredibly efficiently:

```sql
select * from dox.find_one(collection => 'customers', term => '{"name": "Jill"}');
select * from dox.find(collection => 'customers', term => '{"company": "Red:4"}');
```

The queries above are flexing the containment and existence operators, which in turn use the GIN index on your document table. You get all of the lovely speed of PostgreSQL with a bit of a nicer API.

## Full Text Search

One thing that other systems don't have which PostgreSQL has built in is full-text indexing. This means you can do fuzzy searches on simple terms with an index rather than a full table scan, which will make your DBA quite happy.

There's nothing you need to do to enable this, aside from following a simple convention. Every document table comes with a `tsvector` search field:

```sql
create table customers(
  id serial primary key not null,
  body jsonb not null,
  search tsvector, --this one here
  created_at timestamptz not null default now(),
  updated_at timestamptz not null default now()
);
```

When you save a document with a "descriptive key", it will automatically get dropped into the `tsvector` search field and indexed:

```sql
-- the save function
search text[] = array['name','email','first','first_name','last','last_name','description','title','city','state','address','street', 'company']
```

You can, of course, change any of this. I thought about putting these terms in a table for lookup but decided that was too slow. It's simple enough to change this `text[]` to have the terms you want.

To use it, all you need to do is call it:

```sql
select * from dox.search(collection => 'customers', term => 'jill');
```

## Is This Production Ready?

Sure - it's just SQL and PostgreSQL. I've been using it and haven't had any issues, but your data/data needs are different than mine and you might find some areas for improvement. If you fork/download the repo, you'll see a `test.sh` file, which you just need to load using `source ./test.sh` and it will run, assuming you have PostgreSQL installed locally with admin rights.

Or, as I'm a fan of doing, just run `make test`, which will use the Makefile in the project.

## Would I Use This Over Mongo, Couch, Database X?

Hell yes. I am a giant PostgreSQL fan and I love the idea that I can "flip relational" at any time. I love the idea that I can do a simple `select * from dox.get(1)` and I'll know it's using a primary key index. I super love the full text indexing too.

## How Do I Install It?

As I mentioned, there's a Makefile in the root of the project. If you run `make`, it will concatenate the `.sql` files into a `build.sql` file. You can then run `psql` to load that into your database:

```
psql -d my_db -f ./scripts/build.sql
```

## Questions? Issues?

If you're up for having a look and want to ask questions, [go for it](https://github.com/robconery/dox/issues). Mostly: play around and see what kind of performance gain you get when you go with PostgreSQL!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2018/07/lamp-3121677_1280.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2018/07/lamp-3121677_1280.jpg" />
  </entry>
  <entry>
    <title>The Logical Disaster of Null</title>
    <link href="https://bigmachine.io/posts/the-logical-disaster-of-null" rel="alternate" type="text/html"/>
    <updated>2018-05-01T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/the-logical-disaster-of-null</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2018/05/nothing_everything.jpg" alt="The Logical Disaster of Null" /></p>
I'm sure answers are jumping to mind, but hear me out, please. The use of Null in a purely logical landscape is problematic. It's been called [The Billion Dollar Mistake](https://www.infoq.com/presentations/Null-References-The-Billion-Dollar-Mistake-Tony-Hoare) and [The Worst Mistake of Computer Science](https://www.lucidchart.com/techblog/2015/08/31/the-worst-mistake-of-computer-science/). The `NullReference` Error is the runtime bane of .NET developers everywhere, and I would assume Java too.

Logically-speaking, there is no such thing as Null, yet we've decided to represent it in our programs. Now, before you jump on Twitter and @ me with all kinds of explanations, please know that I've been programming for 25 years in both functional and OO settings. I find that the people I talk to about this _instantly_ flip on the condescension switch and try to mansplain this shit to me _as if_.

## Update

Somehow this post [hit HN](https://news.ycombinator.com/item?id=17028878) and people have decided that I _of course_ need a mansplanation. These typically go along the lines of "There is more than one type of logic" and "the OP needs to spend some time investigating ternary logic... Aristotle's idea isn't the only one."

Absolutely _fascinating_ responses! I thought I would reply here with as concise a response as I can muster: programming is based on a binary premise, there is no other logical approach we can take. It's a yes/no, 1 or 0 operation and anything else that we throw in there is **something we made up**. An abstraction, and a failed one at that as evidenced by everything you're about to read. **People make up what they think Null means**, which is the problem.

\--

Null is a **crutch**. It's a placeholder for _I don't know and didn't want to think about it further_ in our code and this is evidenced by it popping up at runtime, shouting at us exceptionally saying "ARE YOU THINKING ABOUT IT NOW?".

Scott Hanselman has a great post which [discusses abstraction and our willingness to dive into it](https://www.hanselman.com/blog/PleaseLearnToThinkAboutAbstractions.aspx):

> My wife lost her wedding ring down the drain. She freaked out and came running declaring that it was lost. Should we call a plumber? I am not a plumber and I have no interest in being a plumber. While I advocate that folks try to be handy around the house, I choose to put a limit on how much I know about plumbing. While my wife has an advanced degree in something I don't understand, she also, is not a plumber. As a user of plumbing she has an understandably narrow view of how it works. She turns on the water, a miracle happens, and water comes out of the tap. That water travels about 8 inches and then disappears into the drain never to be seen again. It's the mystery of plumbing as far as she is concerned.

Null is every programmer's sink drain. The point at which they don't want to think further into the business problem in order to decide what a value means, or what it should be. This causes problems.

## Representing Nothing as Something

The next volume of [The Imposter's Handbook](https://bigmachine.io/products/the-imposters-handbook/?utm_source=conery&utm_medium=blog&utm_campaign=blog_post) is all about information: how we create it, deal with it, store it and analyze it. The first chapter sets the foundation for the rest of the book: I go over Aristotle's Laws of Thought, then glide into Boolean Algebra, Binary This and That, Claude Shannon and Information Theory, Encoding, Encryption, Network Basics, Distributed Application Design and finally Analysis and Machine Learning. Believe it or not, all of this goes together and tells a pretty outstanding story!

I'm about 50% done with the writing process and should be done with the book in a month or so. I've been researching these topics for a year and a half - this stuff is deep! The reason I'm telling you all of this is that I found myself trying to explain how Null could exist in a binary world of true and false... and the ground opened up and swallowed me whole.

## Not Logical

As I mention, I wanted to start the book from a solid logical footing so I could build a story from there, so I decided to start with Aristotle's Laws of Thought:

- Identity: something that is true or false is self-evident. Something that is true cannot present itself as false, and vise versa.
- Contradiction: something that is true must also be not false and not not true.
- Excluded Middle: something is either true or false, there is no other value in between

These laws apply to logical expressions about things that exist. In other words: you can't apply these laws to the unknown, which also includes the future. This is where we arrive at the edge of the rabbit hole: _null represents nothingness/unknownness_, so what the hell is it doing in a deterministic system of 1s and 0s?

Computer systems are _purely logical_, but the applications that we write, if they embrace Nulls, are apparently not. Null is neither true nor false, though it can be coerced through a truthy operation, so it violates Identity and Contradiction. It also violates Excluded Middle for the same reason. So why is it even there?

## The Billion Dollar Blunder

The idea of Null was first implemented in ALGOL by Tony Hoare, who [had this to say](https://www.lucidchart.com/techblog/2015/08/31/the-worst-mistake-of-computer-science/) about the whole thing:

> I call it my billion-dollar mistake…At that time, I was designing the first comprehensive type system for references in an object-oriented language. My goal was to ensure that all use of references should be absolutely safe, with checking performed automatically by the compiler. But I couldn’t resist the temptation to put in a null reference, simply because it was so easy to implement. This has led to innumerable errors, vulnerabilities, and system crashes, which have probably caused a billion dollars of pain and damage in the last forty years.

Null pointers and safe references make sense, but it's a machine concern, not a logical one. With this one decision, uncertainty was introduced into programming through the use of the null pointer and, for some reason, we all just went along with it.

At this point, I will avoid falling further into hyperbole and philosophy and, instead, hold up a mirror and show you how three programming languages: C#, Ruby, and JavaScript, all deal with **Null**. You can then decide what you think.

## Ruby

Ruby's handling of Null is interesting. It's handled by a class called `NilClass` that is represented globally as a singleton: `nil`. Here's what happens if you try to do comparisons and math with it:

![](https://bigmachine.io/img/screenshot_854.png)

Ruby throws, which makes sense. Python does this as well with its `None` construct. When you try to do other fun things, however, like ask `nil` if it is, indeed `nil` or convert `nil` into an integer or array...

![](https://blog.bigmachine.io/img/screenshot_855.png)

This is where coercion comes in and the fun begins. We rarely deal with Null directly; it tends to pop up as the value of a variable and is then coerced into logical operations. Here, Ruby is trying to be helpful by converting `nil` into an empty array, 0 and so on. This leads to an inconsistency: if `to_i` will turn `nil` into a 0, why won't that coercion happen when trying to multiply?

I suppose it's helpful if you know the rules, which you absolutely need to know because _you can't rely on logic_ to tell you what's going to happen.

## JavaScript

JavaScript has both `null` and `undefined`, but I'll just focus on `null`. As you might imagine, you can do all kinds of fun things with it:

![](https://blog.bigmachine.io/img/screenshot_858.png)

JavaScript won't throw when trying to do things with `null`. Instead, it will coerce as needed.

Null in JavaScript is a primitive value and is not represented by an object. If you wanted to verify this, however, you would see this:

![](https://blog.bigmachine.io/img/screenshot_859.png)

This is JavaScript lying to you, believe it or not, and is the result of [a bug in the specification (from MDN)](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/typeof):

> In the first implementation of JavaScript, JavaScript values were represented as a type tag and a value. The type tag for objects was 0. null was represented as the NULL pointer (0x00 in most platforms). Consequently, null had 0 as type tag, hence the bogus typeof return value.

Object, not an object... whatever...

## C#

Believe it or not, this is where things get strange. C# is a pretty "hefty" language, for lack of better words, and I thought it would do things that just make sense. I was wrong:

![](https://blog.bigmachine.io/img/screenshot_860.png)

You can evaluate whether `null` is equal to itself, which it is. You cannot, however, apply a negation to it. I think I like this as C# is deciding not to coerce `null` into a truthy value.

Until lines 13 and 14, where I try to run logical comparisons. Neither of those is logically consistent and straight up violate logic altogether.

Finally, when I try to add or multiply, _the result is `null`_, which is weird. To .NET developers, this makes sense somehow, because clearly "anything multiplied by an unknown value is unknown."

Which is what `null` represents in C#: _unknown_. Eric Lippert, one of the language architects, [affirms this](https://blogs.msdn.microsoft.com/ericlippert/2009/05/14/null-is-not-empty/):

> The concept of "null as missing information" also applies to reference types, which are of course always nullable. I am occasionally asked why C# does not simply treat null references passed to "foreach" as empty collections, or treat null strings as empty strings (\*). It’s for the same reason as why we don’t treat null integers as zeroes. There is a semantic difference between "the collection of results is known to be empty" and "the collection of results could not even be determined in the first place," and we want to allow you to preserve that distinction, not blur the line between them. By treating null as empty, we would diminish the value of being able to strongly distinguish between a missing or invalid collection and present, valid, empty collection.

If you're a .NET person, you owe it to yourself to read [another article](https://ericlippert.com/2015/08/31/nullable-comparisons-are-weird/) Eric Wrote on the subject: "Nullable Comparisons are Weird":

> What is the practical upshot here? First, be careful when comparing nullable numbers. Second, if you need to sort a bunch of possibly-null values, you cannot simply use the comparison operators. Instead, you should use the default comparison object, as it produces a consistent total order with null at the bottom.

Eric's explanations are fine, and all, but I had always read that a routine should throw if it didn't have the information it needed to return a result. Surely `10 * null` falls under this guideline, doesn't it?

Why doesn't C# throw? I had to find an answer.

## Going Undercover

I figured that this question deserves a forum that's not Twitter. I wanted to hear thoughts from others, so I decided to turn to StackOverflow.

If you follow me on Twitter, then you know that I recently [aired some grievances](https://twitter.com/robconery/status/974678531832610816) about StackOverflow, rage-quitting the service because it has turned toxic. This is something they have [acknowledged](https://stackoverflow.blog/2018/04/26/stack-overflow-isnt-very-welcoming-its-time-for-that-to-change/), which I think is GREAT. Seriously: _kudos_.

The reason I bring this up is that I received a lot of pushback on my opinion and some people asked me to basically "prove it". Ahh yes: "prove to me how you feel about something so I can judge whether it's valid". Strange that the people asking me this couldn't see the toxicity in the very Discourse we were having.

Anyway: a few of my friends have created "undercover" accounts on the service to see what it's like as a new user with low rep, and I did the same. I decided to [ask about nulls with respect to C#](https://stackoverflow.com/questions/49636514/why-doesnt-the-c-sharp-compiler-throw-for-logical-comparisons-of-null) to see if I was missing something. There were some pointed suggestions that the question was argumentative, so I decided to target it to "Why doesn't the C# compiler throw for logical comparisons of null".

I think that's a good question. You can read through the answers if you want to see the confusion and condescension. You can also see a rather positive experience with a very kind user named EJoshuaS, who left this comment:

> Excellent first question by the way - welcome to the site.

That was kind. Other commenters were a little more direct and more than a bit condescending. More of this please - you _will_ create a better environment for others which will lead to better answers.

Read the question and you decide if it was asked/answered fairly. No one seems to know why C# behaves the way it does. The decision seems... arbitrary at best (to me).

## Escaping Nullism

Some languages don't have Null, like Swift and Haskell. I think it would be great fun to code in a language that embraced logic at every level, and I wonder if it's possible to do this kind of thing intentionally.

I'm not sure Null should have a place in programming. Then again, this is the first time I've ever really thought about it. Maybe someday I'll write a programming language and implement my ideas for Null. Why not? That's what every other language has done, and we get to live with it.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2018/05/nothing_everything.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2018/05/nothing_everything.jpg" />
  </entry>
  <entry>
    <title>Calling Your Own Shots</title>
    <link href="https://bigmachine.io/posts/calling-your-own-shots" rel="alternate" type="text/html"/>
    <updated>2018-03-29T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/calling-your-own-shots</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2018/03/solo_slide_bg.jpg" alt="Calling Your Own Shots" /></p>
In 2001 the company I worked for (and partially owned) went belly up with the rest of the internet consulting agencies out there. We had just enough work for myself and the original founder so I decided to sign my part of the company over to him and go out on my own. We ended things in an amicable way, for the most part, and I decided that I could probably do just fine if I did little contracts with him and for other companies.

Within a year we worked out a great arrangement where he would arrange contracts for me through the clients we used to have. He would keep a cut of what I made and I could eat and pay rent.

A year or so after that I began to raise my rates as the work began to pick up, and one of my friends said something that changed my career:

> You really should establish yourself as a business. Let me set it up for you.

## Scaling Me

I fought this idea. I had just ejected myself from the mess of running (and losing) a business and I was not keen to jump back in. My friend, who's also a CPA, convinced me otherwise and he set me up with an S-Corp and showed me how to do my books.

Looking back on this now a number of things jump to mind. I can still feel the anxiety associated with running a company and having to fire your friends. Of staring at your books and wondering how deep your credit line runs so you can make payroll. I _hated_ this. On the flip side: knowledge is power or, as Peter Drucker says: _what gets measured gets managed_.

We apply patterns to the code we write. Run tests, wrap it in all kinds of processes and basically make sure it's structured as well as possible for the future. Why wouldnt I do the same for my solo career?

## Refining The Process

Over the years I've tweaked and nudged what I do. I took a break from the solo thing in 2006 (through 2009) to work at Microsoft. I enjoyed it, but when I left I remember having a feeling along the lines of "right, let's rev this up again". I feel like I have the process down and I can scaled myself from a solo contractor on up to a [full blown company](https://bigmachine.io).

I thought this might be valuable to others, so doing what I do I decided to wrap up what I've learned over the years into a one hour video [which you can preview here](https://goo.gl/JPXWUQ). If you're thinking of going out on your own and/or maybe someday selling something on the side, you might be interested in this.

I designed this video as a "nuts and bolts" blueprint, leaving the "you can do it!" type of inspiration to your friends and family. To that end, the video is divided into the following sections:

- **Your plan**. Filling out a business plan can be an intense process as you have to ask yourself about what it is that you truly want!
- **Your people**. You can't do everything yourself, you have to hire out the essential services. I show you how to find a CPA and lawyer (if you don't have these) as well as setting up a bank, what insurance to get and what contracts you might want to use (and avoid!)
- **Your financials**. This is the big one! You need a solid set of books so I show you what I've used and what has worked for me as a solo contractor all the way to a successful business.
- **Your network**. Can't make money if you don't have clients, and that means you have to get out there and establish your network. I'll show you a few ways you can do this if, like me, you're not a natural salesperson.
- **Branding**. This is optional but it can also make a gigantic difference when separating you from everyone else out there.

I should add that it's largely based on my career experience here in the US. Other countries will have different requirements so all of the information might not apply.

## Missing PeepCode

I'm hoping to do more small, polished, < $20 1-hour videos like this one. If you're in to the idea, [hit me up on twitter](https://twitter.com/robconery) and let me know! I miss the monthly drops from Geoffrey Grosenbach and friends - it was so much fun to take a break over the weekend and learn something new!

[![](https://bigmachine.io/img/solo_slide.jpg)](https://goo.gl/JPXWUQ)]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2018/03/solo_slide_bg.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2018/03/solo_slide_bg.jpg" />
  </entry>
  <entry>
    <title>Interview On .NET Rocks About A Curious Moon</title>
    <link href="https://bigmachine.io/posts/interview-on-net-rocks-about-a-curious-moon" rel="alternate" type="text/html"/>
    <updated>2018-02-01T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/interview-on-net-rocks-about-a-curious-moon</id>
    <summary type="text">I was interviewed on .NET Rocks! while at NDC London.</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/freely-10103.jpg" alt="Interview On .NET Rocks About A Curious Moon" /></p>
At NDC London I had the chance to sit with Carl and Richard of _.NET Rocks!_ and [record a show](https://www.dotnetrocks.com/?show=1516) about my latest book [_A Curious Moon_](https://goo.gl/tWF5HE).

I loved this interview, mostly because Richard is a major space nut and we got to geek out completely on Enceladus, space, and other things.

I always have a great time talking to Carl and Richard and it's crazy to think that _this is the 10th time they interviewed me_! The [first time](https://www.dotnetrocks.com/?show=196) was show 196 back in 2006 (12 years ago!).]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/freely-10103.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/freely-10103.jpg" />
  </entry>
  <entry>
    <title>The Modern Dev Team</title>
    <link href="https://bigmachine.io/posts/the-modern-dev-team" rel="alternate" type="text/html"/>
    <updated>2018-01-22T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/the-modern-dev-team</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2018/01/IMG_0193.jpg" alt="The Modern Dev Team" /></p>
The other day I was chatting with some friends in Slack, watching them discuss Kafka “stuff” and things that are good and bad about Kubernetes (which I think you’re supposed to call K8?). I made a small quip along the lines of “I think I need to get a real job” as I’m having an increasingly hard time caring about this stuff.

One of the people in the room, Rob Sullivan, had an epic response:

![img-alternative-text](/img/1516635749.png)

At first I was like "oh you can f\*\*\* right off" but then I just turned 50 so I have a bit of a soft spot when it comes to "falling behind". This is captured perfectly by one of my favorite authors, Charles Stross:

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">An unspecified time after the impostor syndrome goes away, over-the-hill syndrome moves in: the irrational conviction that you're a burned-out has-been, phoning it in, best days behind you, a broken-down hack whose audience is losing interest rapidly.</p>— Charlie Stross (@cstross) <a href="https://twitter.com/cstross/status/955403735521521664?ref_src=twsrc%5Etfw">January 22, 2018</a></blockquote>

<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

It really has been a while since I've worked on a "modern" team. I still write code, in fact, I do it every single day. Mostly for myself: I have 3 running projects which are all focused on helping me run [my business](https://bigmachine.io) - but that is hardly the same as being part of a team.

I think the last time I was part of a team like that would be (gasp) 2009 or thereabouts when I worked at Microsoft. The project was called _Orchard_ and I hated every minute of it. That's a whole other story… but I think that's the last time I ever did institutional coding.

My goodness.

## Am I a Burned Out Has-been?

I don't _think_ I am… but then again I do catch myself lamenting over "the good old days" more and more. Obviously, this is a massive cliché and I know it when its happening, but there is a shred of truth to it sometimes.

Trying to be a good person, I decided to face this question head-on. Just last week I headed over to [NDC London](http://ndc-london.com) and had, once again, a _really really good time_. I think this was probably the best NDC London yet, primarily because they moved the venue to the Queen Elizabeth Center, right down the street from Buckingham and across from Westminster Abbey.

![](https://bigmachine.io/img/ndc-stage.jpg)

While I was there, I decided to dig into Rob Sullivan’s thought a bit: _what would happen if I joined a modern dev team_? I remember sitting in the back of the room during [Felienne’s](http://www.felienne.com) amazing keynote, pondering what that would even look like:

> Lots of Docker? _Probably_. Learning new systems and ways to decentralize things? _For sure_. Solving the same problems that teams have had forever but in a new way? _Absolutely_.

That last bit wasn’t me being snarky! Evolution is an iterative process. New tools give us a fresh way to solve old problems, so I’m in! But… am I just fooling myself? Have I been out of the loop for so long that getting up to speed on modern "stuff” would drive me to constant complaining?

After 3 days of sitting in talks and seeing what people are doing at their jobs (which are my favorite talks: the good old _war story_), I decided that **no, I’m not quite burned out yet.** In fact, I’m likely more energized than I’ve been in a very long time.

Why? Because unlike many years ago when I had to learn all this stuff from scratch: **I already know most of the problems trying to be solved**, or at least I feel like I do. It’s fascinating to see how they’re being approached today as well. Let me explain...

## It’s Still a Thing: Reinventing Erlang

OK that _does_ sound snarky, but it’s also true: building concurrent applications invariably leads one to rediscover what Erlang solved so many years ago (and solved well). Yes yes! I know I sound grumpy! But stay with me as I don’t mean to be a jerk about this: _it just is_. If we can accept this truth without tribalism flaring up, we can free ourselves to introspect whether modern solutions might be better.

Deep breath, let me fill in some details.

Here’s a simple truth that I think we all recognize: _[The Free Lunch Is Over](http://www.gotw.ca/publications/concurrency-ddj.htm)_:

> The biggest sea change in software development since the OO revolution is knocking at the door, and its name is Concurrency.

That post was written many years ago by Herb Sutter, and it’s slowly coming true. Concurrency is a thing, and with processor clock speeds reaching the top of their asymptotic rise, the programming industry is trying to figure out ways to catch up.

Architectures are shifting to allow for this. Microservices. Parallel/elastic scaling, message-based architecture, orchestrated containers and serverless functions in the sky - these all focus on the idea of _concurrency_. Programs that grow sideways with more cores rather than up, with faster cores. Or no cores at all, in the case of serverless...

I know you know this. _At least you should_. You probably also know that **this is precisely the problem that Erlang was created to solve**. That doesn’t mean that everyone should stop what they’re doing and use Erlang! It just means that understanding a bit of history will help you know when you’re doing things better, or worse.

## Containers, Processes, and Serverless

I sat through a few talks about container orchestration, the most fun was with Scott Hanselman and Alex Ellis entitled [Building a Raspberry Pi Kubernetes Cluster and running .NET Core](https://ndc-london.com/talk/building-a-raspberry-pi-kubernetes-cluster-and-running-.net-core/). One of the demos showed how Kubernetes will monitor a node and if it dies, restart another one. _Straight from the Erlang playbook_: "let it die". I love that approach to writing programs with Elixir (and Erlang), and its great to see it being used elsewhere.

But you don't need the Erlang VM for this, just a bigger infrastructure to run Kubernetes, Docker and so on. Is this a good tradeoff? I suppose it must be as people are using it.

Other talks I went to discussed serverless “functions in the cloud”. Some used Google’s Cloud bits, others focused on Azure and gave a nod to AWS Lamda. Each of these talks also weaved together a story where you could “write a single function that takes in the data it needs and returns an answer. String these functions together and you have an app”.

In other words: _functional programming using the actor model_. Well, for the most part, I guess. You’re forced to reconsider the notion of state, something you don’t have with a function “in the sky”. Yes, you can use a database, but then your endpoint becomes dedicated so what's the point?

Functional purity (and the actor model) is something I learned well when I started doing Elixir. I don’t think I’m being a crabby jerk by recognizing this. In fact, it’s the opposite: **I’m seeing interesting ways in which existing patterns are being applied with new solutions**, outside the Erlang ecosystem, solved with infrastructure rather than a platform. Fascinating!

## The Next 10 Years

10 years ago I turned 40 and I remember wondering what I would be doing 10 years from then when I turned 50. I found out what I would be doing during NDC London, on a boat on the Thames. A really fun way to celebrate my 50th birthday: surrounded by good friends and a _big ass chocolate cake_.

As we motored along, I remember looking over at the London Eye, all lit up, looking grand:

![img-alternative-text](/img/1516638978.jpeg)

What will I be doing 10 years from now?

I hope these years will be fun, just like the last. I think I’m old enough now to be able to step back from quantifying my value by whether I’m “in the trenches” or in front of an audience. **I love what I do**, whether it’s writing a book about what I’ve learned or stressing out over a breaking build: it’s just stepping from one role to another.

That said, I _did_ walk away from NDC London with the possibility of joining a very, very interesting project. I’d be part of a team - a very _modern_ team at that. I’m quite excited about it! Just as I am about the next volume of _The Imposter’s Handbook_, which is well underway. I guess this much seems obvious: choosing which thing to do is kind of silly.

**Have fun**. It doesn’t matter how you have it. Oh, and recognize when you’re being a crabby jerk :).]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2018/01/IMG_0193.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2018/01/IMG_0193.jpg" />
  </entry>
  <entry>
    <title>My New Book About PostgreSQL, Data and Saturn: A Curious Moon</title>
    <link href="https://bigmachine.io/posts/my-new-book-about-postgresql-data-and-saturn-a-curious-moon" rel="alternate" type="text/html"/>
    <updated>2018-01-17T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/my-new-book-about-postgresql-data-and-saturn-a-curious-moon</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/pg_saturn.png" alt="My New Book About PostgreSQL, Data and Saturn: A Curious Moon" /></p>
About 2 weeks ago I did a "quiet release", if you will, of [my new book](https://goo.gl/tWF5HE): _A Curious Moon_. It's kind of a funky little venture, blending a bit of scifi with the real world concerns of a database administrator/analyst.

Ultimately, **it's a book about data**. How working with it on a daily basis requires you to be an intuitive sleuth as well as a pragmatic engineer, and how it can be _quite fun_ as well!

## An Icy Moon That Might Have Life

![](https://bigmachine.io/img/7796_PIA21346-full.jpg)

I started writing this book in September 2017 as a quick tutorial on PostgreSQL, my favorite database. I intended it to be about 100 pages or so, designed to be a highly-focused, quick read that you could do on moderate plane flight.

I wanted to focus on building a database that you could actually use, so I of course thought "ECOMMERCE!" because... why not. After a few weeks that grew rather boring, so I decided to have a look at a better data set.

It was right around this time that [Cassini plunged into Saturn](https://saturn.jpl.nasa.gov/mission/grand-finale/cassini-end-of-mission-timeline/) and I was reading article after article about the amazing things that Cassini discovered. One of those things was Enceladus, a freaky little moon that very well could be harboring life under its icy shell.

The [story of the Enceladus investigation is astounding](https://www.scientificamerican.com/article/excitement-builds-for-the-possibility-of-life-on-enceladus/) and I was quickly wrapped up in what the Cassini team described as "the greatest cosmic detective story of all time". I just had to use this as the backbone of the PostgreSQL book... but how?

## Inspired By The Martian

Every bit of data from Cassini is public domain. NASA has 24 hours to release whatever they find, so all the data from Cassini is there for the taking. I downloaded about 200G of delicious space data, and I set about trying to figure out how to structure a basic PostgreSQL tutorial using it. Needless to say: _I was overwhelmed_.

**There is over 20 years of data** from Cassini. TONS of readings from its 12 different instruments! In short: it just wasn't possible.

![](https://blog.bigmachine.io/img/10.jpg)

I then recalled an interview with Andy Weir, where [he talked about](http://uk.businessinsider.com/andy-weir-the-martian-science-crowdsourcing-2015-10?r=US&IR=T) how he put together the story for _The Martian_:

> My research created interesting plot points. Like when I researched potatoes and found out how much water he'd (Watney) need in the soil. Then I realized he'd have to make water. And that led to one of the coolest plots in the book...

Basically: _he solved one problem at a time_ and let the science drive the storyline. I figured I could do the same thing! Treat this as an investigation - the detective story that it truly is. Start at the beginning and let the data drive.

Doing this necessitated something more than me telling you all about Enceladus and Cassini - so I decided to create some fictional characters that work at my fictional startup, Red:4.

My main character, Dee Yan, is a newly-promoted DBA who has to assemble and normalize loads of Cassini data relative to Enceladus. She uses PostgreSQL to do this, and shares her code with you journal style.

She has some big wins and a few massive failures, but most importantly she learns how to spot her biases and focus clearly on what the data is telling her.

## The Most Fun I've Ever Had

I'm not a fiction writer and this is the first time I've ever tried something like this. So far the feedback has been incredibly positive! People enjoy reading about PostgreSQL, shell scripting and so on, having it all wrapped up in a fun story.

I've been telling people about it while I'm here at NDC London, and quite a few had no idea that I wrote a new book! I announced it (sort of) [on Medium](https://medium.com/@robconery/adding-some-scifi-fun-to-a-book-about-databases-82825aca0b14), but decided to pop it here as well, with a little more detail as to the content.

It was easily the most fun I've ever had creating something. I hope that comes through! Working with data (especially using PostgreSQL) is so rewarding at times - it's quite different from writing code and building apps. You have to have some really solid detective skills to sniff out inconsistencies and also solid database skills to make sure the data is correct.

Hopefully that all comes across in the book, and if you're interested [you can pick it up here](https://goo.gl/tWF5HE).

![](https://blog.bigmachine.io/img/image-20170411-26720-1avikn7.jpg)]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/pg_saturn.png" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/pg_saturn.png" />
  </entry>
  <entry>
    <title>My Writing Process (This Week)</title>
    <link href="https://bigmachine.io/posts/my-writing-process-this-week" rel="alternate" type="text/html"/>
    <updated>2018-01-12T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/my-writing-process-this-week</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/typewriter-801921_1280.jpg" alt="My Writing Process (This Week)" /></p>
The first book I wrote on my own ([The Imposter’s Handbook](https://bigmachine.io/products/the-imposters-handbook)) was written in 5 different applications:

- iBooks Author
- Gitbook
- Scrivener
- Middleman (blogging app)
- Softcover (from Michael Hartl)
- Back to Scrivener

Why did I move so much? Simple: _formatting_. Each of these apps/platforms does things that the others do not. Some have outstanding WYSIWYG capabilities (such as iBooks Author) but _blow_ when it comes to code layout. Others (Softcover and Gitbook) are great at that but confine you to their structure.

With [my latest book](https://bigmachine.io/products/a-curious-moon), however, I think I have it figured out. Someone asked me about this on Twitter so I figured I’d dust off my blog and put it down in some detail.

## Scrivener

![img-alternative-text](/img/1515811760.png)

When it comes to assembling your thoughts and structuring your manuscript, there is nothing that beats Scrivener. The functionality it gives you is astounding, and you can write little snippets or big hunks - its all up to you.

It successfully detaches the idea of text from presentation. You can kick up a manuscript and then compile it for various outputs such as epub, mobi, docx, and pdf. The compiler takes time to get used to, but once you do you can have a _serviceable_ book.

By "serviceable" I mean the text will show on screen as well as the images, and if you're lucky maybe some fonts will show up. I played with the compiler for days (literally), trying to get my epub layout flow the way I wanted. Line height, title leading, first paragraph non-indent… all of this is tricky or non-existent with Scrivener.

It's not Scrivener’s fault really. It’s not designed to do this stuff and, moreover, epub is really just some whacky HTML under the hood. You would _think_ that my being a web programmer would mean I could get in there and do some twiddling which is sort of true. Scrivener has opened up the CSS completely to people who understand it, but … it just doesn’t do what’s needed.

Which is fine! Scrivener is good at structuring long-form text. No need to ask it to do much else!

## Proofing and Grammar

Scrivener has some decent spell checking and grammar checking but it doesn’t come close to the power of Word. I actually started writing A Curious Moon using Word, but that quickly became annoying for long-form needs. You could write an entire book inside of Word and it would handle it, but try finding a given passage or sentence… painful.

Yes, yes it’s _possible_ but when you’re flying around between sections, you want some metadata to help you out, which is what Scrivener is amazing at. For the best of both worlds, you can export from Scrivener directly into Word, which is just what I did. I then plugged in my new favorite thing: [Grammarly](https://www.grammarly.com). In fact, I'm using it right now to edit this post :).

![img-alternative-text](/img/1515811820.png)

I paid for a year without thinking twice and then installed the Word plugin (which only works on Windows). I had it do its thing and YIKES! I make a lot of mistakes when I write!

For 3 days I went through and checked the _thousands_ (yes, literally) of mistakes caught by Grammarly. It's not perfect, and sometimes would offer a correction and then correct the correction, GOTO 0. That's OK, it found a ton of stuff.

## Editing

Your book is only as good as your editor. My wife bought me Stephen King’s amazing memoir called _[On Writing](https://www.goodreads.com/book/show/10569.On_Writing)_, and there are some great quotes, including:

> The road to hell is paved with adverbs. Kill your darlings, kill your darlings, even when it breaks your egocentric little scribbler’s heart, kill your darlings.

And, my favorite:

> In many cases when a reader puts a story aside because it 'got boring,' the boredom arose because the writer grew enchanted with his powers of description and lost sight of his priority, which is to keep the ball rolling.

Writing is lonely, and I think writers go a little crazy during the process. That’s why there are editors and editors are like producers for hit songs: they _squeeze the amazing out of it_.

I was going to go with Upwork on this one, there are a lot of editors there for a reasonable price, but I got lucky that a friend of mine has some insane literary skills _and_ she’s a PostgreSQL DBA.

I popped the chapters out as PDFs and loaded them into Dropbox where she was able to use their online editor to leave comments, which worked perfectly. Yes, I could have done all of this in Google Docs, but this worked great.

## Design and Formatting

Finally, we come to the pain. I’m very driven by formatting and presentation. The things I make don’t need to look overly fancy, I just want them to look (as Steve Jobs once said about the iPhone) _like you could lick it_. Pixel-precision makes people like what they’re looking at.

![](https://bigmachine.io/img/indesign.png)

For that, I took my edited final draft and loaded it into Adobe’s InDesign. I had to take 4 (seriously: 4) classes from Lynda.com to figure how this thing worked, but once I got it down it was off to the races. Yes, I could have farmed this out to Upwork too but I’m kind of a nut about this stuff.

It’s not perfect, but it’s good enough. Once output, I put the epub (reflowable epub 3.0) into Calibre so I can create a Kindle version (KF8). Kindle people always have problems no matter what I do which is because of Kindle’s use proprietary garbage instead of standard epub, but… I won’t get started on that.

That’s it!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/typewriter-801921_1280.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/typewriter-801921_1280.jpg" />
  </entry>
  <entry>
    <title>The Imposter&apos;&apos;s Handbook, Print Edition Now Available</title>
    <link href="https://bigmachine.io/posts/the-imposters-handbook-print-edition-now-available" rel="alternate" type="text/html"/>
    <updated>2017-11-21T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/the-imposters-handbook-print-edition-now-available</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2018/01/splash_1600.jpg" alt="The Imposter&apos;&apos;s Handbook, Print Edition Now Available" /></p>
After a good year and a half of editing, polishing and shoving [The Imposter's Handbook](https://bigmachine.io/products/the-imposters-handbook) into shape, **it's now ready for print**. I'm selling it through Blurb print-on-demand [and you can get it here](http://www.blurb.com/b/8278746-the-imposter-s-handbook).

A few things about this.

## Price

It's expensive to print books and I did my best to drive the price as low as I can, but it comes in at $49 USD. That's not that expensive, I suppose, when it comes to print books like this one.

The main reason it's priced this high is because I needed color. Not quite picture book color, but something that would make my stellar drawings look stellar:

![](https://bigmachine.io/img/phys_3.jpg)

I think they look pretty good!

## Length

I trimmed a little here and there to get the book size to fit Blurb's required page limit (480 pages). It's exactly 459 pages in length, in case you're wondering.

## Quality

As I mention, I opted for "Economy Color" paper. I wasn't sure how it would turn out, but it looks pretty damn good! Even the code samples print clearly and legibly.

![](https://blog.bigmachine.io/img/phys_2.jpg)

## Size

I went with 6x9 "Trade Book", which is a bit smaller than normal hardcover books. It fits really well into a bag and looks fashionable too:

![](https://blog.bigmachine.io/img/phys_1.jpg)

## Can I Have \[Discount/Ebook/Something\]

I can't offer any discounts to existing owners of the book as the expense of printing this thing is... expensive. To be honest I would lose money and that's not fun.

Blurb is the seller, however, and they _do_ put on sales from time to time. I wouldn't be surprised if they did something for Black Friday or Cyber Monday. If you want, you can [check my twitterz](https://twitter.com/robconery) and I'll tweet it out if they do.

Regarding the digital version: I don't have access to anyone's information, so there's simply no way to bundle this in. I'm trying to figure out a way to make this happen but so far I've had no luck.

[Hope you enjoy it!](http://www.blurb.com/b/8278746-the-imposter-s-handbook)]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2018/01/splash_1600.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2018/01/splash_1600.jpg" />
  </entry>
  <entry>
    <title>Dumb CS Problems And Your Next Job</title>
    <link href="https://bigmachine.io/posts/dumb-cs-problems-and-your-next-job" rel="alternate" type="text/html"/>
    <updated>2017-09-26T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/dumb-cs-problems-and-your-next-job</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/fib_serously.jpg" alt="Dumb CS Problems And Your Next Job" /></p>
## Congratulations!

I'm happy to let you know that seed round you've been dreaming about - that one that you wanted for that dream startup you've always had - well I'm going to fund you!

Here's a million bucks - now go make that amazing idea happen!

This is a fun daydream - let's run with it for a bit shall we? Just like everyone living in LA has a screenplay they want to write, I think everyone in the software industry has a startup they'd like to launch _if only_. OK fine HN troll - _not all programmers_ but I think most of them do.

If you don't, here's some money now go dream one up!

## Let's Build This Thing

You've got a million of my bucks in the bank. You've had your parties and livestreamed a tour of your new furnished office space on Twitch. You also asked a friend you trust (ha ha ha ha ha ha) to be your cofounder - now let's get down to the business of getting this thing off the ground.

_You need a staff._ This means you'll have some choices (in no particular order):

1. network with the people you know, asking friends if they want to work with/for you
2. hire a recruiter
3. take out an ad
4. cherrypick top talent and be prepared to "1 for 2"

I've had to staff up an office twice in my career and each time I loathed the process. The first time I did it I started asking friends whom I trusted if they wanted to learn to program (I would show them what I know) if they promised to work hard and learn. This worked only once and I ended up losing 3 good friends along the way.

We hired a recruiter once and instantly regretted it. Every person they sent us was a cold call - just a joke.

Craigslist and cherrypicking worked the best. Keep in mind this was back in the early 00's when startups were closing down right and left - it was pretty simple to reach out to people who were afraid of losing their jobs.

Then came the fun part: _asking them what they know_. I didn't know them and I wanted to, so I needed to ask some questions, which is where things started to suck.

## Lying F\*\*ing Liars

In 2001 I had to fire 90% of the people I had hired over the previous 4 years. I remember one conversation particularly well (not his real name):

**Me**: _I think you know what this conversation is about so let's get to it. We'll pay you for the next month while you find a different job._

**Joe**: _Yeah I know. It's not like I did anything here anyways. In fact I don't think I've written a line of code in like 6 months (giggle giggle) and I have no problem taking more of your money. This whole place is a joke - I don't know if you thought you were fooling anyone but –_

**Me:** _Before you go on, Joe, yes I know you haven't been doing anything. You weren't completely worthless to me, however, as that acquisition that almost happened 4 months ago (which we stupidly passed on) was worth $36 million to us, which means you were worth roughly $1.2 million to me just keeping a seat warm. Enjoy your month off._

You could say I was kind of an asshole in that conversation, _but I wasn't lying_. We passed on an acquisition of our company that was worth exactly $36 million. We were a consultancy and developers at desks was how we were evaluated (at 8 to 1 I might add).

Why do I bring this up? **Because every one of the people we laid off lied right to our faces**. In fact in the interviews I've done since, I could tell that most of the developers sitting across from me lied in a sort of _du jour_ way. A fun little game where I needed to spot where they were lying and wink so they could lie some more and I could wink some more...

I asked one interviewee once what the 5 objects were in ASP classic and they looked at me directly and said "when's the last time you wrote a COM object for an ASP site?". Had to admit he had me there.

It's all lies. All of it. Not only that, **it's a legal minefield of lies**. You need to get to know this person as you'll be spending a large amount of your waking hours with them should they get hired. You need to figure out if they'll be a committed team member or a distraction, a cornerstone or liability - all without asking _anything_ that doesn't directly have to do with their responsibilities.

Good luck.

## What Application?

Yes, astute HN troll, _not every programmer lies_. Just most of them. Some don't even have to _try_ to find work - as [Joel Spolsky pointed out](https://www.joelonsoftware.com/2005/01/27/news-58/) many years ago (emphasis mine):

> ... one thing I have noticed is that the people who I consider to be **good software developers barely ever apply for jobs at all**. I know lots of great people who took a summer internship on a whim and then got permanent offers. They only ever applied for one or two jobs in their lives. On the other hand **there are people out there who appear to be applying to every job on Monster.com**. I’m not kidding. They spam their resume to hundreds or thousands of employers.

I know a few good friends who get cherrypicked constantly. They're not disloyal, but at some point the money is so compelling, the job so interesting that not taking it would be a bad move for their career!

If you're a founder and can cherrypick like this, good for you. Most of these "alpha devs" (to use an extremely tired, but accurate term) don't care for instability - which is what your startup will bring.

So now you get to interview the rest of the pack.

## Fibonacci... are you like... serious?

This is a standard (fantastical) [reaction to a basic interview question](http://pythonforengineers.com/the-programming-interview-from-hell/):

> I: Let’s start with the technical stuff, shall we? Do you know what a linked list is? X: (Tells what it is). I: Great. Can you tell me where linked lists are used? X: Sure. In interview questions.

Get it! Programmers don't like interview questions.

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">"Well, despite your repeated demonstrations of incredible success in our field, you can't breadth-first search on a whiteboard! No hire!"</p>— Paul Betts (@paulcbetts) <a href="https://twitter.com/paulcbetts/status/912360863083855872">September 25, 2017</a></blockquote>

<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

Paul is right. He's one of the most amazing developers I know and there are quite a few large companies out there that would "DNH" him if he didn't know breadth-first search. _I think Paul knows BFS_ however, but his point remains.

I would hire him on the spot if I were you, by the way. For the other developers out there, you might need to ask them a few questions. Really. Annoying. Questions.

I failed an interview once (in spectacular fashion I might add) by answering a question thus:

> **Them**: So, Rob, how would you \[do this whacky thing with Docker\]? **Me:** I wouldn't. That's completely ridiculous, won't scale, and will suck money, time and talent from the company's primary business goal. I think I would probably just crater the entire effort.

I didn't get hired, which is probably a good thing. **Or was it?** I can sit here and let my ego inflate over "telling it like it is" while not having a job, or I could, you know...

## Just Answer The Damn Question

When I asked that developer back in 2000 about the 5 objects in ASP classic I didn't have a choice - if he wanted the job it was his; that was the state of things in 1999. I just wanted to pretend that I was at least _trying_ to interview him.

We needed bodies in chairs and coders were being hired with absolutely _no experience at all_, they just said "I'm a coder!" and boom, they had a job. No, I'm not exaggerating.

I asked my interviewee (who actually turned out to be pretty good) later at our holiday party if he in fact knew what the 5 objects in ASP were, and of course **he did**:

> I just thought it was a stupid question. Who care's about using COM in ASP classic? Oh - unless you're one of those Dino Esposito drones who does everything MSDN magazine suggests...

That was a good one, I must admit.

But now we come down to it: **I didn't know him or what he knew**. For him to assume that I should _just know_ his awesomeness is ridiculous. More than that: it's so arrogant that it's likely covering something else: _they probably don't know as much as they think they do_.

That's the problem you face when you pop off in an interview: _it sounds like you're covering up your ignorance_.

## Paul's Point: Github

I gave you that seed round and you're still sitting here! You need to staff up and there's a pile of resumes on your desk. You pick one of them up and it has my name on it, with my blog URL and [Github repo](https://github.com/robconery). You don't know who I am or the things I've done.

What can you make of my repo? There are 1500 followers, quite a few repositories and 6 total stars - _pretty mediocre really_. What story does my repo tell about me? I tend to give repos away to others so I've lost most of the 5K stars I used to have (boo hoo) but there is a smattering of code you can read. Some elixir, some JavaScript...

Here's a repo that I picked randomly. It's called [meteor-shop](https://github.com/robconery/meteor-shop/blob/master/lib/shopping_cart.js) and there's some shopping cart code - let's have a look:

```
if(Meteor.isClient){
  //the cart relies on this global key, which could be a problem!
  //refactor as you see fit!
  userKey = localStorage.getItem("user_key");
  if(!userKey){
    userKey = Meteor.uuid();
    localStorage.setItem("user_key", userKey);
  }
  getCart = function(next){
    Meteor.call("getCart", next);
  };
  addToCart = function (sku, callback) {
    Meteor.call('addToCart',userKey, sku, callback);
  };
  removeFromCart = function (sku, callback) {
    Meteor.call('removeFromCart',userKey, sku, callback);
  };
  updateCart = function (sku, quantity, callback) {
    Meteor.call('updateCart', userKey, sku, quantity, callback);
  };
}
```

_Hmmm, looks like Rob is a Meteor fan... have to ask him about that. Probably likes MongoDB too and, unfortunately we use PostgreSQL. This code is interesting but he's not using ES6 and moreover he's grabbing a cart key from localStorage??? That seems problematic. I don't know..._

My point is simply this: I thrash when I think about new ideas and I routinely save my thrashing to Github so others can help unthrash me or just see what I'm thinking. I honestly don't worry about polish as **I'm not trying to impress anyone**. Maybe a future employer will understand that, maybe not.

Who knows - _maybe it's already cost me an interview or two_. One thing I do know for damn certain is that **I'm no [Paul Betts](https://github.com/paulcbetts)**. That man is metal.

So I hate to contradict you Paul but... yeah please don't use my Github repo as a measure of my skills...

## Back To The Whiteboard

I've wandered in and out of a number of opinions so let's tie this up, shall we? To do so I'll reiterate what I said in the very beginning: _they don't know you and they want to, so let them_. They're going to pay you money and hopefully treat you well; I don't think it's too much for them to ask you a few critical thinking questions is it?

This means that, yes, you will need to know basic CS data structures and algorithms that _you indeed won't use ever_ during your job. You'll need to know Big-O, recursion, and what the stack is vs. the heap.

**Again, these will likely never ever come up during your employment**. Neither will a great many CS things that you've learned over the years.

**The point is not what you know, it's what you can figure out**. That's what these questions are all about: _what skill do you have when it comes to breaking a problem down and building a solution?_

No one wants to feel stupid or inadequate so it's natural to counter that with frustration and anger _which is precisely what they're also looking for_. You're going to be challenged by your peers on the job. You're going to be wrong, you're going to struggle. You're also going to kick butt and do amazing things. In other words: _you're going to be part of a team_ and it's critical to know just how you'll fit in.

Will you meet those challenges with frustration and anger or with professional curiosity? Will you blame others when you fail, or take responsibility for your choices?

**How can your future employer know any of this about you without asking some basic, logical questions**? Github isn't enough. Your blog just isn't enough. The only thing that's left is the very basic CS "stuff" that transcends language, platform and a practice. Things you might have learned getting a degree or picked up along the way.

For what it's worth: _yes_, if Paul walks into your startup and wants a job you damnwell better give it to him. You would be quite lucky to have him too... the one's you don't know, however...

## Shameless Plug

All of this comes to mind because I just went through the Google interview process myself. Yes, **it was very very hard** and I had to study my butt off. The good news is that all of that studying was _rather fun_ and I learned buckets about interviewing at big companies. Yes, I swore a lot too. Some of these questions are just so, so, so dumb. They're also challenging and if you can let go of the ego and view them as puzzles they actually become kind of fun. In fact a few patterns begin to emerge and you can develop some strategies...

Which I decided to wrap up and put into a video because I like making videos. It's what I do and you can buy it here:

[![Mission Interview](images/slide_sm.png)](https://goo.gl/GvQRwP)

I made the thing that I wish existed when I was prepping for the Google interview - something more than just problem after problem; something to keep me focused and stop my frustrations from boiling over.

OK, /shameless_plug.

## Always Be Interviewing

A friend of mine said this to me a month or so ago:

> I try to interview at least once a year to keep my skills sharp. It's a necessary evil but it's also kind of fun in a self-harm sort of way.

Interesting idea. Big companies cycle candidates through constantly, in fact it's common to interview multiple times at a place like Google before you get in the door.

You just have to contain your frustrations.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/fib_serously.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/fib_serously.jpg" />
  </entry>
  <entry>
    <title>Hooking A Web Page To Firebase With VueJS</title>
    <link href="https://bigmachine.io/posts/hooking-a-web-page-to-firebase-with-vuejs" rel="alternate" type="text/html"/>
    <updated>2017-07-12T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/hooking-a-web-page-to-firebase-with-vuejs</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/skitterphoto-1161-default.jpg" alt="Hooking A Web Page To Firebase With VueJS" /></p>
In the [the last post](http://bigmachine.io/2017/06/29/wiring-together-serverless-routines-with-firebase/) in [this series](http://rob.conery.io/tag/firebase/) I set up a bunch of functions that go off in response to a set of events. This is dandy, but how will the client know when the transaction is completed? What does "completed" even mean?

## Defining Done

At a high level, here are the things that we need to do when an order comes in:

- capture the customer's money
- create a sales order, which is our record of sale
- fulfill the order (digital downloads)
- create an invoice which is the client's record of sale
- email the customer with links to their downloads and the invoice
- create reporting entries _somewhere_
- optionally notify the store owner

There could be more or less than this, but I think this is a good start.

To a store owner, "done" might be when the customer is notified and they download the goods; at that point the order status might be sent to "closed" or something.

To the customer, "done" means _I paid now gimmeh_. In an old school ecommerce site things are typically synchronous, so there's no joy for the customer until all things are done. If there's an error along the way, support is going to get a call and we're not going to look very good.

In an evented/asynchronous/realtime system we can do better.

## Charting The Progress

If you recall, every function that we're firing is updating a `progress` record on our sale:

```
exports.stripe_charge = functions.https.onRequest((req, res) => {
  //captures the charge
  //updates as sale record with a transaction:
  //sales/{id}/transaction
  //update progress
});
```

That record is a literal one, as you see here:

![](https://bigmachine.io/img/progress-1.jpg)

**This is the key**: if our functions keep updating this `progress` field, we'll be able to listen to it and react realtime on the client. We can then decide when the order is "done" from the customer's perspective.

## Hooking Up VueJS

Time to jump back over to our static site, which I'm building out with [middleman](http://middlemanapp.com). Again: you can use whatever static app you like, or none at all! To me this is one of the best aspects of working with Firebase (the entire suite): _I'm not tied to a single framework to do everything_. I can build my website however I damnwell please.

So let's plug in VueJS along with a plugin for Firebase called [VueFire](https://github.com/vuejs/vuefire):

```
  <!-- Vue -->
  <script src="https://unpkg.com/vue/dist/vue.js"></script>
  <!-- VueFire -->
  <script src="https://unpkg.com/vuefire/dist/vuefire.js"></script>
```

### Initializing Firebase

The next step is to initialize Firebase from the client. It's triple-important to remember _that this is the client SDK_, not the admin one we've been using. All of the rules will be applied that we've created previously.

To get a quick script for our setup, we can go to the Firebase console for our app, click "Authentication" in the nav menu, and then "WEB SETUP" which is in the top right. This will pop the code you need:

![](https://blog.bigmachine.io/img/credentials.png)

Put that at the top of the page.

### Initializing The App

I already made a checkout page that I like, and you can [see it here](https://app.redfour.io/order/). The easiest thing is to view source on the page if you want to see it all. The first thing to notice is this bit of markup at the very top:

```
<div id="order" class="ui container">
  <div class="ui horizontal divider">
    <h2 class="ui header">Your Order</h2>
  </div>
```

I'm wrapping everything in a `div` tag with the id of `order`. I'm doing this so I can initialize VueJS thus:

```
<script>
Vue.use(VueFire);
var db = firebase.database();
var app = new Vue({
  el: "#order"
});
</script>
```

Like Angular, Vue will now treat this `div` as a template wrapper. In addition I've hooked up VueFire and created a reference to the root of my Firebase database.

## Listening To The Order Progress

Now we get to the good stuff! If you recall, I'm creating the order's ID on the client. I could use a GUID for this, but I decided to do something a bit more meaningful using [MomentJS](https://momentjs.com) and a random string generator:

```
function shortid(length) {
  return Math.random().toString(36).substring(2, 4) + Math.random().toString(36).substring(2, 4);
};
generateId : function(){
  const formattedDate = moment().format('YYYYMMDD-HHmm');
  const key = shortid(4);
  const id =  `RED4-${formattedDate}-${key}`;
  this.orderId = id;
}
```

This will give me an ID like `RED4-20170520-1016-zgb6` that is a bit more meaningful. At a glance I can tell which store this order is from and the date/time. There's also a good bit of entropy at the end there so order numbers aren't guessable. I suppose it could be better, but I like this.

The important thing is that _I know the order number on the client_. Doing this will allow me to listen to the progress of that order, **even though it doesn't exist yet in the database**. To do so, I have to tell VueFire what to listen to:

```
var app = new Vue({
  el: "#order",
  firebase : {
    progress: {
      source: db.ref(`sales/${Cart.orderId}/progress`),
      asObject: true
    }
  }
}
```

Before this will work, however, we have to make sure we've set the rules so we can listen:

![](https://blog.bigmachine.io/img/rules.png)

Firebase rules aren't the easiest thing to get used to, but once you write a few of them they get easier to understand. Here I'm saying "for every sale, the `progress` field can be read but not written to".

Now comes the fun part!

## A Realtime Checkout UI

There are probably better ways to do this! I'm not a CSS expert nor am I graphically inclined; so feel free to have some fun! I decided to use [Semantic UI](https://semantic-ui.com), specifically the [steps bits](https://semantic-ui.com/elements/step.html) to show what's happening when.

There are four total steps (order received, payment received, order fulfilled, emailed) that I want to show to the customer and I can do that by changing the CSS according to what's happening with the `progress` field at Firebase. Here's the HTML/VueJS snippet for doing that:

```
<div v-bind:class="{completed: progress.captured, active: true, step: true}">
  <i class="hand peace icon"></i>
  <div class="content">
    <div class="title">Payment Received</div>
    <div class="description">Processing...</div>
  </div>
</div>
```

I won't go too deeply into VueJS in this post, just know that that it's toggling the `completed` class based on whether `progress.captured` is true. VueJS knows what `progress` is because we bound it above, in our app initialization.

The really nice part about all of this is that we can show a download button once the order is fulfilled; the customer doesn't have to wait for the email to be sent: hooked up to Stripe, but no emails will go through (I disabled that part).

```
<div v-if="progress.invoiced" class="ui container" style="margin-top:52px">
  <h3 class="ui centered header">Order Number: {{orderId}}</h3>
  <p>Thank you for your order! The downloads are below, please note that they are limited. We also ask you
  kindly to not share.
  You will receive an email shortly with your download information. Please hang on to it as it is your record of sale.
  </p>
</div>
```

Here's a checkout that's complete, as far as the client is concerned, but has not yet completed as far as we're concerned (no email sent or reporting created):

![](https://blog.bigmachine.io/img/sale_complete.png)]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/skitterphoto-1161-default.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/skitterphoto-1161-default.jpg" />
  </entry>
  <entry>
    <title>Wiring Serverless Routines With Firebase</title>
    <link href="https://bigmachine.io/posts/wiring-together-serverless-routines-with-firebase" rel="alternate" type="text/html"/>
    <updated>2017-06-29T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/wiring-together-serverless-routines-with-firebase</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/register.jpg" alt="Wiring Serverless Routines With Firebase" /></p>
In [the previous post](http://bigmachine.io/2017/06/22/thinking-in-events-with-firebase/) we sent Stripe Checkout information to an HTTPS-triggered function - basically an API endpoint. Now we need to execute the charge and we can do that with Database-triggered functions.

## Evented Functions: What Goes Where?

One of the most underutilized tools of Node, in my opinion, is the `EventEmitter`. There's [a lot of good stuff](http://rob.conery.io/2012/04/04/cleaning-up-deep-callback-nesting-with-nodes-eventemitter/) you can do with the thing, but to use it you need to shift the way you typically write code. In short: you don't orchestrate logic, you _respond to events_. Orchestration is a side effect, in a way.

The same is true with Firebase Functions. Our goal is to have small, concise little functions that do a thing based on some criteria. For instance:

- The `stripe_capture` function might get triggered by a write to the `sales` path
- A `fulfill_order` function would be triggered when a `transaction` is written
- A `notify_customer` function might get triggered when an `invoice` is written

This all makes sense, logically, but is it the "right thing to do"? Theory often clashes with reality, so let's have a think.

### Invocations and Timing

You don't want your customer to wait while your routines are queued and triggered. If we divide up everything we need to do (capture the charge, generate invoices, rights to downloads, account creation, notification, etc) into little functions, each of those will need to fire in order to complete the order. Are those invocations instant?

Probably not. There is a triggering mechanism that works from a queue, the more we involve this mechanism (as fast as it is), the longer things _might_ take. From my experience, the invocations happen rather quickly but, as I mention a few posts ago: _Akamai_. Let's not introduce a possible problem if we can avoid it.

### What Needs To Happen When?

One of the problems with doing event-based programming is that you often need to do things in a serial fashion. For instance: you don't want to send an email to a customer before their invoice (and fulfillment) is generated; that would introduce a race condition.

If we divide everything into "micro routines" then we'll need to think a lot about what happens when and where. This begs the question: _why are we doing event-based stuff in the first place?_

This line of thinking opens the door to a couple of options we should consider fully.

### Option 1: Everything Needs To Happen Now

Some businesses consider the entire sale to be a transaction. From the moment you're handed the money to the point where you notify the customer and write the reporting entry - it all needs to happen inside of a transaction. If _any of it fails_, it all needs to fail.

If this is the case, then one function with ordered steps is what we need. The minute money comes in the door we do the things we need to do, in order, and we're done. We'll need a rollback mechanism of some kind (which could be a simple delete command) which we could use a simple `try/catch` block for:

```
exports.sale = functions.database.ref("sales/{id}/checkout").onWrite(ev => {
  return co(function*(){
    //capture the charge
    try{
      const transaction = yield stripe.charges.create(...);

      //generate the invoice

      //fulfill the order

      //notify the customer

      //save to reporting

      //update the sale record and close the order

      return {success: true} //whatever you need
    }catch(err){
      //rollback everything
      return {success: false, error: err};
    }
  });
});
```

I'm using `co` with generator functions to orchestrate the serial stuff, but you could use whatever tool you like (such as `async`). With `co`, you can use a `try/catch` block to handle async errors, which is what we're doing here.

This works and has the benefit of being _fast_ - but it also means that you can't _gracefully recover_. Any error in the chain here will cause the sale to fail, which to me is a really bad idea.

### Option 2: Synchronous Little Chunks

Errors happen and I think it's better to build a system that let's you recover if there's a problem. For instance: the customer might have accidentally entered an invalid email. Let them know that right at sale time so they can fix it!

Maybe they entered there name as 👻 and your database isn't setup to handle that kind of string encoding - does that mean you should lose a sale? No way! Fix the problem on your end and resume the sale.

But what are these little chunky functions supposed to be? For me, I have a rule: _take the money and run_... the rest of the functions :). Here are my functions:

```
exports.stripe_charge = functions.https.onRequest((req, res) => {
  //captures the charge
  //updates as sale record with a transaction:
  //sales/{id}/transaction
  //update progress
});
exports.fulfill_order = functions.database.ref("sales/{id}/transaction").onWrite(ev => {
  //create deliverables
  //set the access rights
  //create an invoice and write it to /sales/{id}/invoice
  //update progress
});
exports.notify_customer = functions.database.ref("sales/{id}/invoice").onWrite(ev => {
  //email the customer their invoice and a link to downloads
  //update progress, close order
});
exports.update_reporting = functions.database.ref("sales/{id}/invoice").onWrite(ev => {
  //email the customer their invoice and a link to downloads
  //update progress
});
```

A number of things are going on here, so let's step through it.

First, I'm making sure that the sale gets recorded when the transaction is captured by Stripe. This is something you don't want to forget about :). When the transaction record is captured I write it to the `sales/{id}/transaction` path in Firebase. Doing this triggers the next function: `fulfillment`.

The `fulfillment` function does a lot of stuff. This might rub a few of you the wrong way as it violates SOLID, but I really don't care :). Divide it out into smaller functions if you like, I prefer simplicity. When this function completes, I write the `invoice` to the `sales/{id}` path, which triggers the final two functions.

At this point the order is complete, as far as the customer is concerned. They've paid us, we've generated their invoice - let's not make them wait until we send off an email and create a reporting entry.

But how do we do this? All of these functions are happening "in the background" if you will; how do we let the customer know what's happening?

**Firebase is a realtime database**. The client SDK can listen to any changes in the data at any path! We know the order id because we generated it on the client – this means that we can listen to the progress of the order, and when the deliveries are ready we can let our client access them directly. _Even if the order hasn't finished processing_.

I'll tackle that next time.

---

## [See this series as a video](https://goo.gl/pPpemy)

Watch how I built a serverless ecommerce site using Firebase. Over 3 hours of tightly-edited video, getting into the weeds with Firebase. We'll use the realtime database, storage, auth, and yes, functions. I'll also integrate Drip for user management. I detest foo/bar/hello-world demos; I want to see what's really possible. That's what this video is.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/register.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/register.jpg" />
  </entry>
  <entry>
    <title>Thinking In Events With Firebase</title>
    <link href="https://bigmachine.io/posts/thinking-in-events-with-firebase" rel="alternate" type="text/html"/>
    <updated>2017-06-22T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/thinking-in-events-with-firebase</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/newton_balls-1.jpg" alt="Thinking In Events With Firebase" /></p>
_Image credit: [CCPixs.com](http://www.ccpixs.com/)_

The first two posts in this series were philosophical musings on "why serverless" and "why _not_ serverless". With this post let's write some code.

## Rules...?

I was talking with [Matthias Brandewinder](http://brandewinder.com) recently at [NDC Oslo](http://ndcoslo.com). He does a lot of serverless stuff, but mostly with Azure Functions. One of the things we discussed was the absence of "patterns" for how to build things in a serverless environment.

Not the Gang of Four stuff - we were talking about _architectural_ patterns. The question was a simple one:

> Do we build things like we would otherwise? Or is there some type of pattern out there yet to be discovered?

I don't know - and that's kind of fun! I'll ponder this question repeatedly as I add posts to this series. For now, let's take the first step on our journey...

## The First Function

I'm building an ecommerce app, nothing terribly surprising about how it works. Product pages, a cart of some kind and finally a checkout page powered by Stripe:

![](https://bigmachine.io/img/checkout.jpg)

This here is [Stripe Checkout](https://stripe.com/checkout). You could write your own checkout form if you want, but I dig this.

Stripe works by sending the raw credit card/address information directly to Stripe, bypassing my app entirely. What I get back is a "token", which is a reference to the checkout information stored on Stripe's servers. I don't want to go too deeply into all of this right now - have a look at their docs if you're confused.

Once I have the token, I need to execute the charge. I'm not going to do this on the client, obviously, which makes this scenario perfect for Firebase functions.

## To AJAX or Not?

Here is my Stripe configuration code:

```
var handler = StripeCheckout.configure({
  key: 'MY PUBLIC KEY',
  image: 'https://app.redfour.io/img/icon/apple-icon-180x180.png',
  locale: 'auto',
  zipCode: true,
  billingAddress: true,
  token: function(token) {
    var payload = {
      order: {
        id: Cart.orderId, //RED4-20170622-k4kdls
        processor: "stripe",
        items: Cart.items
      },
      payment: token
    };
    //now what?
  }
});
```

The `token` callback is the thing we're most interested in. This is the code that fires once Stripe has processed and stored the user's checkout information. Here I'm creating a `payload`, which has order information (including an order id, which is very important) and the order `items`. Finally I'm attaching the `token`, as that's what we'll use to capture the charge.

The question is: _how do I get this to Firebase?_

### Option 1: Writing to the database

We know that Firebase functions are triggered off of events. The most logical one at this point is to use an "https event", which is triggered when a specific URL is called. I _could_ do that, or I could lean on Firebase completely by simply writing this `payload` directly:

```
firebase.database().ref(`sales/${Cart.orderId}/checkout`).set(payload);
```

This is using the browser-based firebase SDK to write my `payload` to the path specified. Notice how I'm using the `Cart.orderId` here? This serves two purposes:

- I can use it to define a path to a sale in the database
- I can use it to _listen_ to a path to a sale in the database

This is where _thinking in events_ starts to take shape! By writing this checkout information, we'll be able to trigger a cascade of events that will, hopefully, culminate in a sale.

### Problem: Security... Hello?

If you're wondering if I've fallen and hit my head - I don't blame you. Allowing the public to write to your database is completely lame! The good news is that we have some rules we can define to help us out. Let's take a look at them now and I'll explain more as we go:

![](https://blog.bigmachine.io/img/rules_1-1.jpg)

Every firebase database allows you to specify a set of _rules_ for working with data. These rules dictate what can be written, read, how to validate the data and finally how it should be indexed. I'll get to all of this later on, for now, focus on the highlighted area.

I'm specifying rules for a given path using a JSON structure. This isn't the easiest thing to get used to, but it only takes a few Googles to understand it. Here I'm specifying the path to the checkout data should be governed by a few conditions:

```
"sales" : {
  ".read": false,
  ".write": false,
  "$sale" :{
    "checkout" : {
        "payment" : {
          ".read" : true,
          ".write" : "newData.exists() && !data.exists()",
          ".validate" : "newData.child('token').exists() && newData.child('email').exists()"
        }
    },
}
```

The first thing to notice is that **rules cascade**. At the very top I'm specifying that the public can't read or write to the sales path. I'm overriding that on a per-sale basis by using `$sale`, which is a placeholder for every child of the parent `sales` key.

I override the parent read/write settings at the `checkout/payment` path, allowing reads, but only allowing writes when there's new data and a record doesn't already exist (`newData` and `data` are predefined variables). This will prevent people from changing their payment information.

Next, I have some simple validation, which ensures a token and email exist. By the way: if you're wondering what the difference is between `.validate` and `.write` (as they basically do the same thing) - `.validate` _does not cascade_, which `.write` does.

Is this enough? Would your serverside code do more validation than this? You can build a pretty extensive validation expression here - it's just JavaScript that gets eval'd every time a write happens.

### Does It Matter?

Imagine you have a new CTO. Or maybe your company was just bought or transferred to a new division. You're sitting in a code review and this new CTO (or tech manager, whatever) looks at your code and asks:

> Let me get this straight... you're letting the public write directly to the database?

Imagine if this meeting was taking place after an incident of some kind. It wouldn't even need to be related to the checkout process. Heads will need to roll, and there's very little you can say to get yourself out of this kind of jam.

Technically speaking, Firebase's rules are pretty solid and you can do a number of things to mitigate bad guys. Politically speaking you might be putting a noose around your neck.

Your call.

## Option 2: Make an AJAX Call

The easiest thing to do here is to just make an AJAX call:

```
var checkoutUrl = "https://some_firebase_function_url.com/stripe_charge";
$.ajax({
    type: 'POST',
    url: checkoutUrl,
    data: payload,
    dataType: 'json'
}).done(res => {
  console.log(res)
}).fail(err => {
  console.log(err);
});
```

This function will receive the payload, write it to the database and kick off the same cascade of events that you would otherwise. There are some other advantages as well:

- You can examine an IP address and stem flooding
- You can use anonymous authentication to do the same (which I'll talk more about in a later post)
- You can write more comprehensive validation code

The downside to this is that these validations are in code at the app level, not nestled happily in front of the data. The best choice, probably, is to do a combination of the two things:

- Keep the validations discussed in Option 1, above
- Protect flooding using Option 2

Like I mention up top: **there are no patterns here yet**. This is all sort of new-ish, and this answer seems good to me.

## Now What?

Now we get to write our receiver function - the _entry point_ to our serverless backend. We'll use an https triggered function to write the new order, and a further set of functions to process it both serially and concurrently.

We'll do that in the next post.

---

## [See this series as a video](https://goo.gl/pPpemy)

Watch how I built a serverless ecommerce site using Firebase. Over 3 hours of tightly-edited video, getting into the weeds with Firebase. We'll use the realtime database, storage, auth, and yes, functions. I'll also integrate Drip for user management. I detest foo/bar/hello-world demos; I want to see what's really possible. That's what this video is.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/newton_balls-1.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/newton_balls-1.jpg" />
  </entry>
  <entry>
    <title>Are You Really Doing BDD?</title>
    <link href="https://bigmachine.io/posts/how-behavioral-is-your-bdd" rel="alternate" type="text/html"/>
    <updated>2013-08-28T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/how-behavioral-is-your-bdd</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/good_behavior.jpg" alt="Are You Really Doing BDD?" /></p>
## Arm-Waving

I find that when you discuss BDD or DDD a mix of jargon and definitions is thrown around until no one understands each other. This type of thing plagues software development ([see REST](/2012/02/28/someone-save-us-from-rest/)) and makes it extremely difficult to discuss... **well anything**.

Behavior-driven Development, however, seems to be pretty straightforward **as long as we adhere to the idea**. But what is that idea and why does BDD exist in the first place?

Let's start from the beginning - _what problem are we solving_. [Here's Dan North (the guy who created BDD)](http://dannorth.net/introducing-bdd/):

> ... I kept coming across the same confusion and misunderstandings. Programmers wanted to know where to start, what to test and what not to test, how much to test in one go, what to call their tests, and how to understand why a test fails. The deeper I got into TDD, the more I felt that my own journey had been less of a wax-on, wax-off process of gradual mastery than a series of blind alleys.

I've felt the same way. If you've done TDD, it's likely you've been very confused too. So how does BDD solve this stuff? Again, Dan:

> I started using the word “behaviour” in place of “test” in my dealings with TDD and found that not only did it seem to fit but also that a whole category of coaching questions magically dissolved. I now had answers to some of those TDD questions. What to call your test is easy – it’s a sentence describing the next behaviour in which you are interested. How much to test becomes moot – you can only describe so much behaviour in a single sentence. When a test fails, simply work through the process described above – either you introduced a bug, the behaviour moved, or the test is no longer relevant.

I like it. You're describing **what your app does**, not what it is (which is how TDD always felt to me). By focusing on what the app does you're putting yourself in the user's chair and this is critical.

## Flexibility and Maintenance

In its simplest form, the classic definition of BDD asks you to:

- Define a story
- Create scenarios for that story
- Specify behaviors of your application for those scenarios

_(I'll be referring to this list repeatedly below)_

That latter part is also called "Acceptance Criteria" and (this part is important) **the acceptance criteria should be executable**. This means that you can execute something that says "why yes, my application does behave this way".

This approach is fantastic for a number of reasons, but the number one reason as far as I'm concerned is that you're entire testing story is centered around Real World, Actual Use of your application! This keeps you focused on what's important: **User Happiness**.

Another benefit here is that changing your tests around is mandated _only when your application's behavior changes_. You can re-implement/refactor major chunks of your application but as long as you don't change the behavior (which is usually the goal) - **you don't have a fleet of tests breaking**.

This means that, over time, your tests remain nimble and flexible - meaning that changing things isn't a massive headache. I like it.

## Modern BDD And Confusion

BDD has changed over the years given the rise of specialized tools and frameworks. I like many of these tools - but you can still write Unit Tests with them, and indeed I think that's what most people are doing.

Let's take a look at a few modern examples of BDD starting with RSpec and the [example found on the RSpec home page](http://rspec.info/):

```ruby
# bowling_spec.rb
require 'bowling'

describe Bowling, "#score" do
  it "returns 0 for all gutter game" do
    bowling = Bowling.new
    20.times { bowling.hit(0) }
    bowling.score.should eq(0)
  end
end
```

This code seems pretty clear, but is there any behavior specified here? What I see is a Unit Test that is verifying the `hit` method increments a `score` attribute. Let's take a look at another example from a [BDD Game Kata](https://vimeo.com/53048454)

![Bowling Kata](/img/bowling_kata.png)

The wording is a bit different in this example and there's a notion of a Game (this is Mocha and NodeJS) but what behaviors are under test is not clear. Once again - these tests are making sure that a score variable is set to 0 when 0 pins are knocked down.

[That's a Unit Test](https://www.google.com/search?q=define%3Aunit+test).

So what's the difference?

## Focus On Behavior

I think all of these tests are valid, and all of these approaches work. But I think they don't really fulfill Dan's vision of "a story, with scenarios, that specify behavior". Again: they work, but I think we could do better.

Let's see how - and I'll stick with the bowling context.

We're trying to specify how our application will behave when something happens. In every example given so far, we don't really know what we're testing. In BDD you start with a "feature" and then think about that feature in terms of "scenarios".

The feature it seems the above tests are focused on is Scoring. A scenario to consider is ... no score at all! So how would we convey this with BDD? We could try something like this:

```ruby
#bowling is not a feature of our application, scoring is
#and even then, scoring is different during and after - so let's be specific
describe "Final Scoring" do

  #now we come up with a scenario
  describe "No pins knocked down" do

    #what happened?
    it "is 0" do
      #...
    end
  end

end
```

Just for fun let's do this in XUnit and C# as well. We don't have the descriptive ability that we do with RSpec - but we _can_ put this in a file called "FinalScoring.cs" in a directory called "Scoring":

```csharp
[Trait("Scoring Completed Game", "No pins knocked down")]
public class NoPinsKnockedDown(){

  [Fact(DisplayName="0 is the final score")]
  public void ZeroIsFinalScore(){
    //...
  }

}
```

The difference here might seem subtle - and indeed it might seem like we're testing the exact same code. But look again at the first example:

```ruby
# bowling_spec.rb
require 'bowling'

describe Bowling, "#score" do
  it "returns 0 for all gutter game" do
    bowling = Bowling.new
    20.times { bowling.hit(0) }
    bowling.score.should eq(0)
  end
end
```

How would you think differently about building your app if you had a suite of tests like this? Here we have the notion of `Bowling` as a model which has a method `hit` and an attribute of `score`. This doesn't make much sense does it?

But this is "Model as Feature" way of testing that is fairly common with Rails.

Using a re-worded RSPec suite (my examples above) - you start to see the things that you would need to build in order to make this behavior happen. Different classes interacting that reflect the real world (and the problem at hand) present themselves much more readily.

## Thinking In Scenarios

Let's say you come over to my house one weekend and I share a beer that I made from a Super-Secret Recipe which I think you'll love. You taste it and say "MMMMMMMMMM" and then you ask me more about the recipe.

I tell you some specifics RE temperature, hops additions and timing etc and, being a good Beer Geek you ask something along the lines of:

> What would happen if you gave the bittering just 10 minutes more? And maybe did a hopback with a whirlpool and...

This is a natural dialog. I have an idea or concept and you want to question it so that a) you understand it better and b) you might help me improve it!

**This is how BDD works too.**

In Tekpub's next video (where I work up a Membership library for .NET) I do exactly this: **I have an idea, I lay out what should happen right up front, and then I think about all the different ways it can break** :

![Membership Specs](/img/membership_specs.png)

I find that thinking in scenarios helps me tremendously _because it's a natural way we think_. What you see in this screenshot above is XUnit written in the style of the examples above. I don't believe that you need a specialized BDD framework to do BDD - **just think in terms of "what happens to my app when I do this"**.

The video should be out in a few weeks as we're putting the final polish on it.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/good_behavior.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/good_behavior.jpg" />
  </entry>
  <entry>
    <title>JavaScript Frameworks Are Amazing and Nobody Is Happy</title>
    <link href="https://bigmachine.io/posts/js-frameworks-are-amazing-and-no-one-is-happy" rel="alternate" type="text/html"/>
    <updated>2013-08-22T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/js-frameworks-are-amazing-and-no-one-is-happy</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/rotary-phone.jpg" alt="JavaScript Frameworks Are Amazing and Nobody Is Happy" /></p>
Because **everything is amazing right now and nobody’s happy**. Like, in my lifetime the changes in the world of programming have been incredible. When I was a kid we had no DOM abstractions and JavaScript was a like a rotary phone that you had to stand next to, and you had to dial it. Do you know how primitive – _you’re making sparks in a phone_!

And you actually would hate DOM elements with long IDs because it was more – you’d be like

> uh this DOM ID is too hard to type, screw that shit call it X

And then when your code ran in a browser that didn't properly support JavaScript you’d just go

> well, I can’t do any more things now. I can’t do any more things.

_Now_ we live in an amazing, amazing world with so many fascinating frameworks to choose from and it’s wasted on the crappiest generation of just spoiled idiots that don’t care, because this is what people are like now – they’ve got their framework of choice and they’re like _“uh! It won’t…”_

> Give it a second! Give - it’s going to space! Can you give it a second to get back from space!?

I was on an airplane sitting next to a guy coding with some new framework – it's the newest thing that I know exists. And I’m sitting on the plane watching him and he writes 3 lines of code and **boom** there's two way binding, event delegation, and it’s fast – it’s amazing! – and we're in an airplane!

And then things start to break and he has to read a bit more to figure out how to use this API correctly. And then he goes

> phff - this is bullshit

**Like how quickly the world owes him something that didn't exist six months ago.** Just take 10 minutes and read the documentation! 10 minutes!

JavaScript coding is the worst one because people come back from coding for an hour and they tell you their story and _it’s like a horror story_ – they act like their hour of coding was like a cattle car in the forties in Germany – that’s how bad they make it sound.

> That was the worst framework I've ever used - pure shit. First of all, it took me 10 minutes to install, and then the package manager couldn't understand which package I wanted it to go out and find and install for me... and then I sat on Google for another 30 minutes reading answers to all my questions on StackOverflow...

Oh really what happened next? Did you write code with that framework in an hour that we could only **dream of writing** 10 years ago ? Did you partake in the miracle of computer programming _you non-contributing zero?!_

_You’re creating a browser application in JavaScript with barely any code!_ It’s amazing! Everybody writing JavaScript in the browser should be looking up from time to time saying

> oh my God! Wow!

You’re coding in JavaScript! You’re on a boat, in the sky!

Here’s the thing – people like to say whatever framework their not using sucks. "Sucks", really? Two-way binding, route handlers, and dynamic events pushing data into a remote API within an hour - _that used to take thirty years_ to do that and **you thought you would die on the way there and have a baby**. You’d be a whole different group of developers when you got done with that project!

Now you write a little code and shit's flashing all over the screen, you get bored so you take a dump on Twitter and read Hacker News and then fuck off for the rest of the day. Next thing you know, you’re home.

<hr>
<i>Credit: http://www.dailymotion.com/video/x8m5d0_everything-is-amazing-and-nobody-i_fun</i>]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/rotary-phone.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/rotary-phone.jpg" />
  </entry>
  <entry>
    <title>Knowing More Programming Languages Will Make You Smarter</title>
    <link href="https://bigmachine.io/posts/knowing-more-programming-languages-will-make-you-smarter" rel="alternate" type="text/html"/>
    <updated>2013-05-13T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/knowing-more-programming-languages-will-make-you-smarter</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/language_map.png" alt="Knowing More Programming Languages Will Make You Smarter" /></p>
_image from http://visual.ly/network-graph-programming-languages-influence_

## Words as a Conceptual Map
There's a small bit of setup here - bear with me. [I relistened to one of my favorite RadioLab podcasts ('Words')](http://www.radiolab.org/2010/aug/09/) this weekend and had my mind blown, once again. They dug into **what words mean** to us and, according to the show, words are much more than expression vehicles.

**Words are the very structure of our higher reasoning**.

In other words: _if you can't say it, you can't think it._ The words themselves are cognitive variables strung together to represent feeling, emotion, memory - abstract things made concrete by the words we put to them.

High-minded stuff and rather than regurgitate the whole show here - go listen if you haven't, it's very good. It left me quite inspired and I wondered if the the ideas in the show could extend to programming languages. Why not? Code is simply syntax laid on top of process that a machine needs to understand... hmmm....

We tell machines how to think - which directly reflects how we think. Writing code always seemed like such a "machine-y" thing to do, but I think that's not the case: **Programming languages are true languages** written for us to describe problems that a machine can understand.

These languages are stripped of nuance and imbued with functionality (loops, terse conditionals) for a myriad of problems. _How well do you speak code?_

## A Thought Experiment
Code is language, and how we solve problems with a given programming language directly relates to **how we think in that language**. This doesn't seem like an amazing revelation in and of itself - in fact I think you probably already understood this quite well.

But what if we reversed it? What if you couldn't think any other way - other than what that language _allowed you to think_. What if your coding language of choice **was the abstraction over your programming thought process** - that you actually lacked the mental faculties that lie outside of what that language can represent?

Let's say you walk into an interview and you get hit with a classic interview question (no, not FizzBuzz): **write a routine that spits out Fibonacci numbers**. It's something every dev has had to do at some point.

Now, slow your brain down and rewind. What language are you using to express this? What is that language **allowing you to do**.

If the only programming language you know is C# you're probably writing some LINQ in your mind and running a modulus. If you use JavaScript you're probably doing something similar but maybe spread over two functions utilizing callbacks. 

Rubyists out there might do something similar - favoring a more idiomatic approach with blocks/yield rather than a terse one-liner.

**Now write it using a Monad**.

**Recursion?**

I'm sure a few of my readers could do all the above (LINQ, callbacks/blocks, Monad and Recursion) but I doubt that most of you can. This is due to the languages you know, the idioms they use, and **the way those languages allow you to think about a problem**.

## Expanding Your Thought Process
I don't know anything about monads but I've done recursive programming in the past (sometimes not-intentionally). When I started using Ruby, I began to value the idiom of passing control back and forth using blocks and yield. You can do this in many languages - but for some reason it wasn't something I routinely did in C#.

The asynchronous nature of JavaScript has awakened a part of my mind to evented programming. Again you can do this in many languages but it's idiomatic in JavaScript and Node.

I can "think" in these terms and solve problems in ways I could never do when I knew only one language well (C#). At the time I liked the idea of "going deep" - of knowing a language so thoroughly that I could solve any problem with it and not clutter my mind with the different syntaxes of other languages.

I'm glad I didn't. And listening to that Radiolab podcast this last weekend made me understand why. I used to think it was that learning new languages would help me "think differently" but now I know it's more than that: **I'm smarter now**.

I'm not an egotistical guy and it hurts a bit to write that last paragraph - as if I'm suggesting that I'm smarter than people who know only one programming language - **which is clearly not true**. My point is only that _I'm smarter than I was 4 years ago_ and the reason I think that is because I can solve problems in more ways than I could before and if my definition of "intellect" is accurate...

![Definition of 'Intellect'](/img/intellect.png)

... then yeah: I think it's OK to say I'm smarter than I was with respect to programming.

The thing I enjoy most about all of this is that I would never have thought about this until I heard that Radiolab podcast. They put the words to the very idea that words are our higher reasoning.

A lovely bit of recursive fun.

**Would I have recognized that recursion a few years ago? Hmmmm....**]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/language_map.png" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/language_map.png" />
  </entry>
  <entry>
    <title>EmberJS Confuses Me</title>
    <link href="https://bigmachine.io/posts/ember-confuses-me" rel="alternate" type="text/html"/>
    <updated>2013-03-06T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/ember-confuses-me</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/spinning-dizzy.jpeg" alt="EmberJS Confuses Me" /></p>
## I Tried

Saying something confuses me is no great claim - many things do. It's entirely likely that I didn't study Ember long enough, or maybe I didn't give it the "time to soak in" that it deserved. Either way I punted on my efforts to bring an Ember title to Tekpub.

Normally I'd just go about my business and sidestep posts like this one - but today I had a conversation with Trek Glowacki (core dev on the Ember team) and he was confused as to my confusion. I offered to blog my thoughts - he couldn't understand the points I was making about my confusion.

So here's that post, stripped of hyperbole and "it's ON FIRE" statements. It's my honest take on this framework.

## You Keep Telling Me It's MVC...

Yes, the concept of MVC has been contorted over the past decade as web frameworks have adopted it and massaged it to their own ends. What does this mean?

- Controllers in web MVC have a very short lifespan, and there are a lot of them typically
- Views dont' communicate directly to a controller - they have to use an intermediary layer in HTTP (headers, querystring, etc)
- Models are injected into views by Controllers
- Controllers don't command the views directly

In more classic MVC (aka "Desktop") your controllers are typically spun up when the app starts, and configured one time, right there. A controller's job is to command aspects of the view as data changes, and to change the model when things on the view happen that mandate it.

The model encapsulates the business logic for whatever its modeling, the view presents the data in a nicely formatted way to the user, and the controller sits between. I think we agree on this much - so what's the problem?

OK this is MVC as far as I have come to know it. There are lots of denominations I am sure - I have to think the church of MVC is about as crowded as the church of REST on any given holiday but there's room for all of us so let's move on...

## Separation. Only Not.

In EmberJS, the [controller's job is changed a bit](http://emberjs.com/guides/controllers/):

> A controller may have one or both of these responsibilities:

> - Representing a model for a template.
> - Storing application properties that do not need to be saved to the server.

> Most of your controllers will be very small. Unlike other frameworks, where the state of your application is spread amongst many controllers, in Ember.js, we encapsulate that state in the router. This allows your controllers to be lightweight and focused on one thing.

For me, this is where the confusion begins. A controller proxies the model, and then exposes itself to the rendering engine so that the view can consume it (you may need to read that sentence twice). As a result, we have view code like this (this is Handlebars):

{% highlight html %}
{% raw %}
{{#each person in personController}}

  <h1>{{name}}</h1>
{{/each}}
{% endraw %}
{% endhighlight %}

This can be shorthanded to this:

{% highlight html %}
{% raw %}
{{#each controller}}

  <h1>{{name}}</h1>
{{/each}}
{% endraw %}
{% endhighlight %}

**This is where I hit a conceptual wall**. First: let me say that I'm not the best programmer in the world and I could be dead wrong on this. But as far as I understand it, this is about as tightly bound as we can make things.

As stated above, a controller "proxies" the model onto itself for use in the view. I'm trying my best to reconcile this with the notion that a controller (classically speaking) is supposed to ... well **control** the view. Here, it's not doing that.

If I'm not mistaken, the view is controlling the controller and the model is nowhere to be found. Well that's not true I spose: **the model is the controller**. Sort of. Well it's not literally the same thing in concept but in practice the controller and the model are one. Bound. Together. Tightly.

In Angular this separation is achieved by a "scope" object that is, basically, a vehicle of sorts that gets passed between the controller and the view:

```javascript
var PersonCtrl = function ($scope) {
  $scope.people = [{ name: "Rob" }, { name: "Mary" }];
};
```

In your view, you declare which controller you're using and then consume the information that's on the scope:

{% highlight html %}
{% raw %}

<div ng-app>
  <div ng-controller='PersonCtrl'>
    <ul>
      <li ng-repeat='person in people'>{{name}}</li>
    </ul>
  </div>
</div>
{% endraw %}
{% endhighlight %}

The downside to this approach is that your HTML is "compromised", if you will, and many developers don't like that. My thought is that **it's already compromised using Handlebars** so what's the difference here? Personally I have no issue using the ng-\* directives. Some people do, and I respect that.

But let's get to the controller code. Notice that the $scope is injected? That's Dependency Injection at work and is a core concept in Angular which greatly helps testability.

Now you might be saying "but ROB! The controller is declared directly in the view! That's the same problem!". And that's true - until you loop in routing (below) and you can remove that directive since it's handled explicitly by routing:

{% highlight html %}
{% raw %}

<div ng-app>
  <ul>
    <li ng-repeat='person in people'>{{person.name}}</li>
  </ul>
</div>
{% endraw %}
{% endhighlight %}

In Backbone a view is created (optionally) with the model (or collection) that it needs for data. There are no controllers (classically) in Backbone - though you can surely wedge one if needed and have it deal with the view.

These concepts align with my understanding of MVC. Not the case with Ember - if anything it confuses me greatly.

## Routes, Objects, and More Confusion

I can get past the controller issue - I'm not a purist and if there's a reason the team wanted it this way, then I believe them. [It's Yehuda](http://yehudakatz.com/) and I think Yehuda is a very nice and smart person (as is [Tom Dale](http://tomdale.net/) - the co-creator of Ember).

I respect these guys so much I want to be very careful to **not suggest** that they don't know what they're doing. As I keep saying: **this is my confusion**. I own it.

Which brings me to Routes. Here's [one way to define a route](http://emberjs.com/guides/routing/defining-your-routes/) using EmberJS that looks rather familiar:

```javascript
App.Router.map(function () {
  this.resource("posts", { path: "/posts" }, function () {
    this.route("new");
  });
});
```

Rails devs will recognize this straight away as a Resourceful Route. But what does a "resource" have to do with a Desktop app? When talking routes, urls, and resources - that's a RESTful consideration and involves stateless state "stuff" (sidestepping the REST debate here). **What is this concept doing in a desktop app?**.

Here's another way to define a route:

```javascript
App.Router.map(function () {
  this.route("about");
  this.route("favorites", { path: "/favs" });
});
```

This is straightforward. Client MVC apps can manipulate browser history with "hashbang" URLs for bookmarking convenience. To manage that, we use a router and define routes.

And if you know that Ember relies heavily on naming conventions you could probably assume that you'll need an AboutController and a FavoritesController (like Rails - and Yehuda is a Rails core dev too so it would make sense...).

But that's not quite what needs to happen. You need to also define a RouteObject (which you can think of as a route handler):

```javascript
App.FavoritesRoute = Ember.Route.extend({
  model: function () {
    return App.Favorites.find();
  },
  setupController: function (controller, model) {
    controller.set("content", model);
  },
});
```

I've been repeatedly told that "Ember uses MVC in the classical Desktop way" following Cocoa/Smalltalk style. OK I can buy that - but if that's the case **why are controllers being tied to routes and models**, conceptually? That might make sense for the web, but hardly for a desktop app.

Now I completely understand that we can throw theory at this [and that Smalltalk-80 MVC](http://st-www.cs.illinois.edu/users/smarch/st-docs/mvc.html) was instrumental in forming the concept behind Ember.

But even then - one model could be used in many controllers (and the reverse) depending on the need. I'm happy to shift my mind into Desktop mode - but it seems Ember still wants to retain some conceptual likeness to Rails, which I find most ... strange.

I remember watching [Geoffrey Grosenbach](http://blog.peepcode.com) explaining this in [Peepcode's very well done video tutorial on EmberJS](https://peepcode.com/products/emberjs) and feeling rather confused. I rewound the section and played it back, to see if I could understand what I was seeing.

And rewound it a few more times after that. It wasn't sinking in. I could not understand the difference between declaring a route, and configuring a RouteObject. Why not have the URL be a setting on the Ember.Route?

Now it could be that it was this way in the past - I've only gotten to know Ember 1.0 rc1 and I also know that the API changes rather dramatically from time to time.

But this is really confusing.

Here's how you do it in Angular:

```javascript
//declare the app and any dependencies
var App = angular.module("App", []);

//configure the app, injecting the Route Provider
App.config(function ($routeProvider) {
  $routeProvider
    .when("/about", {
      templateUrl: "about.html",
      controller: "App.AboutCtrl",
    })
    .when("/favorites", {
      controller: "App.FavoritesCtrl",
      templateUrl: "favorites.html",
    });
});
```

Angular's approach is interesting and somewhat "concept-y" if you're not familiar with Dependency Injection. Once you get past that (or maybe you love it already) it becomes straightforward to understand that "here's a route, use this template and this controller". This is straight up configuration: **which is exactly what the method name is**.

I love parity between method names and the code I'm writing.

Here's how you do it in Backbone:

```javascript
App.Router = Backbone.Router.extend({
  routes: {
    "/about": "showAbout",
    "/favorites": "showFavorites",
  },
  showAbout: function () {
    //render the view, or call to a controller
  },
  showFavorites: function () {
    //render the view, or call to a controller
  },
```

The Router in Backbone is also a bit confusing - there's no doubt about it. It can quickly turn into a mess of configuration AND instrumentation - in other words doing a controller's job.

I think this is why the Ember team tried to separate the configuration of the routes from the route itself. But in doing so they marginalized the controller (which was their intent, if you read the controller's definition).

With Backbone I know what I'm getting into. I can use the Router as a controller (it used to be called that btw) and when things get a bit nuts, I can create a controller of my very own ([or just use MarionetteJS that has this concept built in](http://marionettejs.com)).

## Naming

Ember relies heavily on naming conventions that must not only follow [syntax guidelines, but also casing guidelines](http://www.emberist.com/2012/04/09/naming-conventions.html).

The reason for this is straightforward: Ember will generate code for you if you just need basic, boilerplated stuff, alleviating you from tedious coding which is rampant in other frameworks like Backbone.

The problem I have with this is that the relationship between things is in name only, and that seems like it's a rather brittle rule. It also tells me that I have to know the naming conventions **as well as** how to use the framework - which is fine, typically, as long as there's a big payoff.

And that's where I get stuck (again): **what's the payoff, aside from implicit code generation, for following the naming conventions here?**. Consider the routing code, above:

```javascript
App.Router.map(function () {
  this.route("about", { path: "/about" });
  this.route("favorites", { path: "/favs" });
});

App.FavoritesRoute = Ember.Route.extend({
  model: function () {
    return App.Favorites.find();
  },
  setupController: function (controller, model) {
    controller.set("content", model);
  },
});
```

The only way I know that one thing depends on the other is by name. Now I know that should seem obvious and you might be thinking "well duh, Rob, that's the point!". Yes, but it's also a great way to have a highly confusing API.

What's the problem with something like this:

```javascript
App.FavoritesRoute = Ember.Route.extend({
  url: { name: "about", path: "/about" },
  model: function () {
    return App.Favorites.find();
  },
  setupController: function (controller, model) {
    controller.set("content", model);
  },
});
```

This, to me, is a whole lot more apparent and I'm not reconciling functionality through a naming filter. For some that might be much easier than reading the code (or reading the tests) - to me it seems arbitrary and confusing.

## You Asked, I Answered

In wrapping this up, I would like to say that I am writing this as a way to make my points a bit clearer to Trek as Twitter doesn't work well for this kind of thing. He didn't understand my confusion about Ember (specifically why I didn't think it aligned with my understanding of MVC) so I told him I'd write something that explained my confusion.

It would be easy to see this as "Rob ranting on Ember" and I'm sure it will be taken that way by many. Not my intent, really. I have 4 videos to edit in front of me, and a book to write. I took time out of my day to respectfully respond to the Ember team with my thoughts - even if they're not glowing and exciting with praise.

If Ember works for you I think that's great. It doesn't work for me.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/spinning-dizzy.jpeg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/spinning-dizzy.jpeg" />
  </entry>
  <entry>
    <title>Screencasting Like a Pro: The Demos</title>
    <link href="https://bigmachine.io/posts/screencasting-like-a-pro-the-demos" rel="alternate" type="text/html"/>
    <updated>2012-08-31T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/screencasting-like-a-pro-the-demos</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/bullwinkle.jpeg" alt="Screencasting Like a Pro: The Demos" /></p>
## Hopefully, You've Mused
The first two posts of mine on this subject dealt with creating an outline and then executing the basics of a script. This is, more or less, supposed to be a thought exercise on your part - focusing you on what it is you're trying to say and whether it's worth it for them to hear it.

I've already received feedback from people along the lines of

> Love the tips! Maybe if I want to sell my screencasts someday I can use your tips in full!

I know that it's hard to do what I'm suggesting here. I know it takes time - believe me... **I know**. If you're thinking along these lines (that a paywall will dictate the quality of your content) - please consider:

 - [Ryan Bates](http://railscasts.com) has issued over 300 screencasts of enormous quality, only erecting the paywall in the last few months. And it's well worth it.

 - Your voice, your code, and (basically) **you** are recorded for all to see, for years to come. For personal and professional reasons I might suggest giving it **everything you've got**.

 - The less you give, the more no one will care.

I wanted to start off with this because it's only going to get harder from here. Not in terms of tools, vocal work, or clever words. **It's going to get harder because the polish is created up front - not afterward**. 

Let's go.

## Care. Give it.
We have a script and we need to underscore our point with demos. These demos should revolve a singular truth:

> Shut up and show me the code

Given that, let's consider what makes a good demo:

1. The viewer knows what they are about to see as you've told them
2. The viewer sees the code that makes your point
3. The viewer is left with something to think about and may, or may not, agree with you

We need to focus on a beginning, middle, and end to all of our demos. We can't spend time on tangents, and **we can't take time out to preach**. This is yet another thing I learned from [Scott Hanselman](http://hanselman.com).

I have a decent number of "bad" demos on very popular sites that I like to point to as "what not to do" - but I want to keep this positive and drive home the point I'm trying to make: **Stay On Target**. 

Consider these options when explaining Dependency Injection:

> Dependency Injection is a software pattern that many seasoned developers use for typed languages, and from that they've decided to use Inversion of Control Containers because it makes things a lot easier. Just take my word for it guys... it does. If you haven't used one yet you really should because it makes your life simpler and you'll stop writing crappy code that no one can debug. This one time...

Take two:

> Dependency Injection is a software pattern that helps you write smaller classes that can do more, and also keeps you from writing code that can, like yarn, turn into a messy ball in the future. The idea is that you send dependencies into your classes through their constructor - "injecting" them - rather than instantiate them inside the class (think data access). 

Notice how I didn't involve another concept here? By "staying on target" I'm able to craft a demo that's laser-focused and doesn't waste anyone's time. Remember: **this is video, rewinding is easy.** 

Make your point clearly with as few words as possible, and don't insult your viewer with pedantic tangents.

## Code. See it.
We've crafted our demos - we know what we want to say and we haven't polluted it with opinions unless conveying the opininion is your point. Let's rock this!

We're going to do our demos in multiple takes - focusing on small, 1 - 2 minute demo increments. This lets us go backwards if we mess up.

I use [Screenflow](http://www.telestream.net/screen-flow/) for all my recordings. It's Mac-only, but if you're on your Windows then don't even think about it and use Camtasia. The software isn't important right now - I'll cover that later.

Have your outline open in a window next to you, and think about the "takes" for your demo. You might be trying to show Dependency Injection, but don't try and cover it all at once! Slice it into smaller parts that you can do in 60 seconds or so.

If you screw up, type stupid, monkey-finger your keyboard, have your IM pop up, accidentally show your email password, or have a crash - you'll want to back up. This is why you want to do this in 60 second recording pops.

## Don't Talk and Type
No one ever believes me - but this is so crucial: **please don't try and speak while coding**. Unless you're Ryan Bates or Scott Hanselman - you won't be able to do it. No one wants to hear your keys clicking either. 

We just want to capture the code going down and working - we'll add the voice work later. It's much easier that way!

You won't believe me. So many people don't - good friends of mine who sell their screencasts still think I'm a nut when I say this! Let me just say that:

> Hearing you studder while you type is not why I'm paying for your screencast. I'm not that into you, I just want to know what you know - please make this as easy as possible for me!

If you still don't believe me - just try what I'm suggesting once and listen to the difference. If you're not motivated, then perhaps you're just as good as Ryan or Scott...

## Voiceover
Time to move on to voice? Not quite so fast!

There's no way you'll make it through your set of demos without realizing you forgot something. Which is OK! Embrace it and take some time to rethink your script and what you're doing! 

Or maybe you screwed up and decided to redo something - **STOP!** Is that screwup something you can show someone? It most likely is and you should remember: failing is how people learn! 

Take some time to think through what you've just done, and be happy you didn't record your voice because now... NOW you can jigger your demos to refine your story. Maybe go back and reshoot the one part that you screwed up - and you can inject it at a given point...

This, finally, is the last time that we will need to go over the script. We have to stop at some point and get the stuff out!

Next time - we record ourselves...]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/bullwinkle.jpeg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/bullwinkle.jpeg" />
  </entry>
  <entry>
    <title>Lost at Sea</title>
    <link href="https://bigmachine.io/posts/lost-at-sea" rel="alternate" type="text/html"/>
    <updated>2012-08-27T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/lost-at-sea</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/lost_at_sea.jpg" alt="Lost at Sea" /></p>
## Drifting
In 1986 I dropped out of college. I was 2 years into a meaningless attempt at a degree without a major. I lived in Los Angeles and attended L.A. Valley College taking 12 units of meaningless general education, swimming 4000 yards from 3-5pm every day trying to get in shape for water polo tryout at UCLA.

I was 19 and, like most people my age felt bewildered and lost. Or that's what I wanted to believe. I was pissed off at myself for screwing up when I was in high school and I got to visit my friends at Cal Poly San Luis Obispo, UCSB, Pepperdine, Occidental, USCD... and go to the parties thrown by people who's lives were on track towards... somewhere.

I bobbed around. Swimming, studying, surfing... adrift.

## One Hand
That's what you keep on the boat when you're out at sea: one hand, at all times, holding something. Preferably not another person - you want the wheel, a railing, or a hand-hold. When the boat moves it won't ask you first.

This becomes painfully clear when your motor's dead and the seas are corking you about: **hold on to something, always, and stare at the horizon**. Your world becomes rather immediate and small, the imperatives are simple: don't puke and don't panic.

I look over at Grant, who's staring at the throttle as I was. It's his brand new boat - a 26 foot Trophy Fisherman decked out with fish finder, GPS, counter-rotating stainless props, a brand new chevy 350 big block engine... and right now it shares more with a cork then a boat.

We're driving it home from Oahu to Kauai; a 110 mile trip from Wai'nae harbor to Hanalei on the North Shore of Kauai. We're 60 miles into the trip with a dead engine, and all I'm thinking about are dolphins...

## Fox
The phone call was brief - strangely brief for something so important:

> We'd love to have you - when can you start?

It was the Glendale YMCA. They ran a camp on the lee side of Catalina Island called Camp Fox and I had been there a few times when I was younger. I loved the place, dearly, and a friend told me they had a position for a winter caretaker... just like The Overlook. I jumped at it.

![Camp Fox](/img/camp_fox.jpg)

My job was simple: I cleaned up after snot-nosed Indian Guides who clogged the toilets, I fixed broken lights, and once a week I crossed the 26 mile channel to San Pedro to pick up supplies.

I left at 7am sharp every Wednesday morning and headed exactly 1 degree North - that would put me into San Pedro 1 hour or so later. I drove a funky diesel-powered cargo boat called "The Foxy Lady", and I didn't know a single thing about boats.

One bright, clear winter morning I was 12 miles out and couldn't see land in any direction as the fog hadn't yet lifted for the day - and in front of me I saw hundreds and hundreds of "Common Dolphin" surrounding me. Not quite like Bottlenose (Flipper) - a bit smaller, but a lot more jumpy.

I was so taken by it, I stopped the motor in the middle of the sea and climbed on top of the front console to watch them. The sea was glass, the sun was just coming through the fog and I was alone for miles and miles around, save for the dolphins...

## Gas
Grant is a mechanical wizard and his head is deep in the engine well surrounded by the gut-punching fume of gasoline. He's re-routing the fuel lines, trying to figure out which of the fire triangle has been violated. We have fuel, we have oxygen, and we have spark... 

He pulls back and his eyes are squinted, staring up at me with a bit of a weary look. My feet are planted firmly on the deck, riding the teeter-totter motion of the wild ocean that surrounds us on all sides for miles and miles.

> I think I might need a break pretty soon, he says.

I'm an OK mechanic, but I can't do what Grant is doing. Pieces of boat engine are rolling around the deck and I look over at the radio - wondering when to make the call.

The radio...

## Are You a Fucking Moron?
By the 8th trip across the channel I was pretty good at navigating the ocean waters between Los Angeles and Catalina island. The morning was always the most fun: that's when the dolphins would come and jump all around the boat. Hundreds and hundreds of them.

I brought my wetsuit with me and after we loaded the boat up with supplies for that weekend's camp, I would put it on, knowing that the ride back was up wind and very, very wet.

I had just put the wetsuit on and was lazily winding my way through massive oil tankers and car freighters anchored in San Pedro harbor when I heard a siren call. It was so out of place that I ignored it - I'm on a boat, not on the freeway...

And there it was again - but closer. I turned around and, sure enough, I was being "pulled over" by the San Pedro harbor police. I didn't know there was such a thing being 19 and incredibly stupid - but I was to learn a lot that day.

I slowed and "pulled over" and they tied off to me. They asked to come aboard and see a number of documents - none of which I had, or even knew if they existed. A toxic combination of my young stupidity, mixed with a boss that didn't bother with "silly details" - led to me being in very, very big trouble.

No life jackets, no working radio, no flare, no fire extinguisher... the list went on, and on. The policemen went from incredulous, to pitiful in a quick turn and pretty soon just flat out asked if I was a "fucking moron". I told him I didn't think so...

They wanted to tow the boat in, but decided against it since it was full of perishable Indian Guide food. They let me go, but made sure I understood that I was in massive trouble.

I drove straight back that day, and quit 2 weeks later. I started at UC Santa Barbara 3 months after that - graduating in 2 and a half years with a degree in Geology and a love of writing computer programs...

## It Doesn't Stop
Grant took a break for a bit, and I looked over his shoulder - out to the north where the next stop was, literally, Alaska. I remembered the last time I was in the middle of the ocean, feeling helpless...

In 1990 I took part in Semester At Sea - a round the world ocean voyage where you visit countries and learn about life. We crossed the Pacific and outran a major typhoon, and I remember standing on the top deck of the ship, staring to the north to Alaska which was 2400 miles away.

> I can't swim there. I can't swim anywhere...

The ocean doesn't stop. It keeps going as far as your eyes can see and your mind can conceive. It moves in every way imaginable and it will drive you insane if you try to understand its movements.

There comes a point, when you're out there alone, that you're forced to ride it and take what it gives. To shut off your expectations, swallow your fear and carry on.

Or you can puke. And never stop puking...

## Give It All We Got
I asked Grant if there was anything I could do to help aside from running for tools. "At this point... call in a few favors from whatever you believe in I spose." Not terribly reassuring.

I made the choice to come out here and I embraced the risk. It brought some excitement to my life of staring at computer screens all day. I didn't think we were in mortal danger, but it was a safe bet we wouldn't be home that night as we drifted slowly into the backside of Kauai, sick to our stomachs.

The Coast Guard won't come out unless you're in immediate danger - something like drifting out to see or Boat on Fire. We were neither - just slowly being pushed by the trades and swells into Nawiliwili harbor 50 miles away.

We would land around midnight.

> Let's give it all we have. Burn up the starter and the battery - let's just see if we can turn it over. Then we'll make the call and let the wives know we'll be late.

So I did. I turned the motor over for about 2 minutes straight. The battery was strong and kept pushing the motor... driving it to start. But I could hear the revs start to slow...

And then it kicked. And kicked again - the motor caught and pushed, and pushed once more... and then boom: **it started**.

I backed away from it slowly and Grant yelled "don't touch it!" as he ran forward. Slowly he revved the engine ... and we were back to life, and on our way.

## Themes
I thought over these stories tonite, and I was wondering what they said about me as a person. I've been piecing together the next [This Developer's Life](http://thisdeveloperslife.com) (which is taking me forever) and it's made me think a lot about things that motivate us from time to time.

Something that seems to pop up for me at certain points in my life is being, literally, Lost at Sea. There's tranquility there, mixed with immediacy and the truth of your small existence. I hadn't noticed it until I was putting together the audio for our show.

I don't think there's much to say about it, other than it's a pattern - or perhaps a "theme" to my life. A need to dissolve the noise and distractions and feel as if... there's nowhere to swim to. I don't think it's destructive, per se, it just... is.

As I close this story I'm thinking about the next time I'm "Lost at Sea". I know it's coming because, typically, there's little you can do to change your nature. I want to "wrap it up" somehow - to offer a point or some kind of "and therefore" - but I don't think there is one.

Sometimes the beauty is in the narrative - the things that come between our summations and "therefores". I think I tend to forget that, occassionally.

I'm sure I'll remind myself again someday. I'll probably think about this post as I'm staring north, towards Alaska...]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/lost_at_sea.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/lost_at_sea.jpg" />
  </entry>
  <entry>
    <title>Screencasting Like a Pro: The Script</title>
    <link href="https://bigmachine.io/posts/screencasting-like-a-pro-the-script" rel="alternate" type="text/html"/>
    <updated>2012-08-24T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/screencasting-like-a-pro-the-script</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/hamlet.jpg" alt="Screencasting Like a Pro: The Script" /></p>
## Know What You're Saying
In [the last post I did on the matter](http://wekeroad.com/2012/08/22/screencasting-like-a-pro-beginning-middle-end), we organized our thoughts into a coherent story with a beginning, middle, and end.

We have some structure, but we're far from being done with creating our masterpiece. Just like a gymnast: depth of knowledge conveys a sense of ease in your voice, which the listener will pick up on completely.

Note: **this does not mean you have to be an expert**. It means you need to know what you're talking about. These are two different ideas if you can believe that and we'll explore this idea more in this post.

## Filling Out the Outline
Let's get back the outline we made last time. I mentioned that we're now going to write everything we want to say out - and I mentioned that you won't want to do that. It will take you, literally, 6-8 hours to do this.

And then it will take another 6-8 hours as you rewrite half of it.

And do it again.

And again.

This is where the polish is born, and where we separate ourselves from the keyboard-pounding, mouth-breathing "ummmmm"-filled mess that screencasts can (and tend to) be. Stay with me.

Just like an essay, start off with your abstract - but remember you're writing dialog here, not an essay :). For example, lets say we're doing a screencast on something like BackboneJS - and you've created an outline that you like.

Here's one way to do it:

> BackboneJS was written by Jeremy Ashkenas of Document Cloud and is a popular MVC (Model, View, Controller) framework for JavaScript. Its focus is to help you create compelling, feature-rich JavaScript applications, providing much-needed structure at the right level of abstraction.

That's a pretty concise, complete description of what Backbone is - and is something you might find in an abstract somewhere. Now, **try reading it aloud - as if to a friend**. Can you imagine? You're out having a beer and someone says "Hey what's Backbone all about, anyway?". Read it aloud again... framing it as your answer.

Let's try this again - remembering that we're introducing Backbone to people and giving them a reason to be interested in our screencast:

> BackboneJS is the most popular JavaScript MVC framework - loved because it's small, simple to use, keeps you out of trouble when things get complex (which is pretty normal for any JavaScript application). It was written by Jeremy Ashkenas - the same Jeremy Ashkenas that created CoffeeScript - and was extracted from the work he was doing for the New York Times and Document Cloud.

Do you see the difference? Hopefully you do - we've conveyed the same core ideas: Backbone is popular, it's an MVC framework written by Jeremy, and it's small.

The first paragraph is dry, the second has a soul - like a human has a soul - and that's what we want here. Human to human communication.

## Then SAY IT
Giving your words soul and meaning isn't difficult: it's what you do naturally every day. Your task is to push your soulful grooviness beyond your mouth and into your fingers. Here are some tips if you have trouble with that.

**"Fuck Your Filters."** When I was in college I hated writing - but you have to take writing classes so they let you out. A professor was reading one of my papers once and he said "you really hold back when you write" and I said "I have no idea what you're talking about."

He said "same here, dude. Now, take some of that fire and put it on paper - and fuck your filters". It was shocking to me, that a professor would swear like that. But why not? He's human, and he was trying to emphatically get his point across. He finished by saying: "think about what you're trying to say - and then say it. I'm not interested in anything beyond your inspiration".

I got an A that year in my English Comp classes, and every year afterward (I kept taking them - I loved writing). I felt like I found a secret - but it's something you already know: **people love truth**. So give it to them.

**Repetition. You've said it, now leave it**. I'm guilty of this constantly - be happy with knowing your audience heard you and hopefully understood you.

**Rule of Threes**. I don't know why it is - but if you have more than three demos, more than three supporting points - **more of three anything** it's likely you're saying too much :). 

## Let it flow
Man, so many rules! Let's write something. Go to a quiet place and put some music on that matches the mood and overall "sense" of your screencast. It helps to keep the energy level moderate (FFS please no Korn) so your typing flows.

Write out every word you want to say, and while you do it, imagine you're speaking to your best friend. Not your kids/brother/sister/someone you just met: your best friend. And use a voice in your head that want to emulate.

I have two that I use consistently, depending on the topic:

- [Ira Glass](http://en.wikipedia.org/wiki/Ira_Glass). Host of This American Life and a natural at what he does.
- [Terri Gross](http://www.npr.org/people/2100593/terry-gross). You have to know who this woman is

I like their tone and pace - especially Terry Gross's. Note that you're not trying to copy them - you're hearing them say the words you type. Be careful here - it's easy to pick up their affectations (Ira sucking through his teeth, Terry's stutter/stop).

Once I heard [Geoffrey Grosenbach](http://peepcode.com) in my head as I wrote out the script for one of Tekpub's [Real World MVC3](http://tekpub.com/productions/mvc3) productions. I don't recall why I did - I think it was to change up the tempo or something - and I ended up sounding just like him.

It was embarrassing - someone noticed and called me out on Twitter. Weird the way the brain works.

As you're writing out your script, remember it's a dialog. Hear your words in the viewer's mind and try to answer questions they'll have:

> BackboneJS is just one of many JavaScript app-building frameworks. Many love it because it's small, but some feel that it doesn't do enough for the developer so other frameworks - like EmberJS and Angular - popped up later on.

Along these lines it really helps to know your audience. At Tekpub we have a large Microsoft customer base - so I might throw in something like this as well:

> If you're a .NET developer you no doubt are wondering the difference between Backbone and Knockout, from Steven Sanderson. The simple answer is that...

Don't be afraid to use as many words as possible to get your point across - you're writing, not recording here. We'll fix that up next.

## Demos
I usually just flag the parts of the dialog where the demos are going to go - and what those demos are going to convey. When writing your script out just assume that you gave an amazing demo of whatever point you're trying to make - I'll come back to this in the next post when we talk about recording equipment.

## Editing
Now comes the fun - you've spilled a ton of words out - now let's tighten it. Reread everything you've done, again with your favorite narrator's voice. Your goal is to get away from wooden, George Lucas-style dialog and to be able to say things that people actually say to each other.

Edit freely. You can't imagine how valuable this part is: you'll hear yourself sound unsure, say silly things, or (worse yet) say something completely wrong. This is where you start to polish up the "know what you're talking about" part.

Be very sensitive to this. You can hear it in your voice, see it in your writing. If you don't know something for sure, **go find out**. If it's not there to be found - leave it out. Or, if you can speak to it and say you don't know - that's good too!

This is what I alluded to earlier: **you don't have to be an expert** - you just have to have knowledge of a topic and, hopefully, you've been thorough with it!

Take three passes at it and polish it down. You may rewrite it a few times, but your attention here will show tremendously.

Now, on the last pass go through and weed out the repetition. This should be the very last pass - and you should remember this is video! People can rewind if they didn't catch what you said.

## It's All Preparation
Have you noticed **I haven't said anything about tools yet**? That's coming next - right now you should have a solid understanding of what you're doing and what you're going to say.

Also - I hate to drop this bomb on you but you're likely going to alter your script by 20 to 30% ... **again**. Demos never go as planned, and you'll likely find something in there worth talking about.

Or, more likely, you'll realize that many things you thought were either wrong, or that you should add a whole mess of detail to one part of your screencast.

More on that next time...]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/hamlet.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/hamlet.jpg" />
  </entry>
  <entry>
    <title>Screencasting Like a Pro : Beginning, Middle, and End</title>
    <link href="https://bigmachine.io/posts/screencasting-like-a-pro-beginning-middle-end" rel="alternate" type="text/html"/>
    <updated>2012-08-22T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/screencasting-like-a-pro-beginning-middle-end</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/goodnight_screencasts.jpg" alt="Screencasting Like a Pro : Beginning, Middle, and End" /></p>
## Start With a Story
I learned this from [Scott Hanselman](http://hanselman.com): 
  
  - Have a beginning, middle, and end
  - Don't waste people's time
  - Start with the punchline

In other words: tell a good story. It's amazing how often people overlook this simple truth: we're built to listen and learn from stories. This is at the root of making a good screencast (or blog post or presentation or... anything you're trying to convey to someone else).

All the rest are the details - where the devil lives :).

## Truth, From a Certain Perspective
Let's assume you know what you're trying to convey to someone: perhaps how awesome your startup is, how to use a tool or programming language, or maybe how to create a screencast :). Now comes the structure that sits under the story.

This basic structure needs to get out of your brain, onto paper quickly. I usually open a text editor (like Sublime Text 2) and start throwing ideas down on what I want to convey. That's what I did for this post here.

I thought about concept, tools, process - all of it and I wrote it down. Then I decided which was most important (the underlying concepts) and started with that - knowing I might change my mind as this all came together.

## The Outline
Our ideas are coming out of us - now let's get it down somewhere. After I scribble ideas in a text editor (or on a piece of paper) - I wrap some structure around it. I used to Google Docs for this but have moved to Pages/iCloud. It doesn't matter the tool - **just be sure you have access to it when inspiration hits**.

I really can't emphasize that point enough. Inspiration will strike you and you'll think "what a neat idea for a demo!" - write it down or lose it!

My outlines grow and grow - sometimes stretching 4 or 5 pages! This is what you want - everything out of you, into an outline. 

## Tangent: Give Yourself Time
One anxiety that I have to keep in a cage is my need to push content fast. I've learned over and over that rushing a creative process triples the suck. The opposite of this is also true - and is something you need to keep in mind: **patience triples the awesome**.

When you read my tips here, please keep this in mind. I'm fairly certain many reading this will say "that's fine for you - I don't have that kind of time". I'll rewrite that sentence for you thus:

> That's fine for you, I don't respect my audience as much as you do.

If that sounds harsh to you - consider that you are, literally, trading the quality of the user experience over the lifetime of your screencast for a few hours of your time. 

Be patient. It pays off.

## Paring The Outline
We have 4 or 5 pages of ideas - demos, concepts, whatnot. It's time to consider just how much we need to say. One thing I find helpful when deciding what to keep is to ponder how I learn - and what things make the biggest differences to me.

I've found that I learn the most when:

  - I screw up and fix something
  - I watch someone else screw up and fix something
  - The demo echoes something I've tried to do - from problem to an inspired solution.

The rest is noise. Literally. More on this below.

This is where you need to stare at your "story arc" - your beginning, middle, and end. Remember: you're trying to walk someone through a story, not impress them or sell them something. It's always a temptation to "show someone how cool a tool is" - and no one cares.

They want to know how to use it - assume they think it's awesome. Given that, how do you filter out the "noise"? Here are my tricks:

  - **Does this demo convey something "real" from the user's perspective?** Instead of "look what you can do with KnockoutJS", focus on "this is what Knockout does - I've used it to do this". It's a very subtle difference, but you know it when it happens.
  - **Is this preachy?** The biggest trap of all. You don't know everything.
  - **Does this fit?** - you wouldn't believe how simple it is to pare things out that don't fit your beginning, middle and end.

You should, literally, lose half your outline. Now's the time for review - and some hard questions.

## Tangent: Clear Glass
You are the story teller. The lead actor in a story written by you, about a thing. Consider that the best actors "melt" into their characters - you just can't see them. There's [Daniel Day Lewis  as Abe Lincoln](http://boingboing.net/2012/08/07/daniel-day-lewis-as-lincoln.html) and, of course, [Heath Ledger as the Joker](http://www.youtube.com/watch?v=u8PxG5zvgOM).

We're not great actors - but we can use the same tricks:

  - **Check your ego**. This isn't about you. Your vocals, wording, demos: focus on the story you've written.
  - **Listen to yourself**. Repeatedly. If you find yourself following along, you're doing a good job. If you cringe at your own voice... well nothing is a better indicator!
  - **Be Human**. You're not speaking in front a room, you're right in someone's ear. It's intimate - so be human, warm, and friendly.

The way I've put this to people is that you're a human made of clear glass. The great narrators all have very warm, engaging tone in their pace and delivery - yet nothing that might convey who they are as a person. [David Attenboro](http://en.wikipedia.org/wiki/David_Attenborough), for instance, is my hero.

## The Final Review
A beginning, middle, and end. A commitment to be patient and focus on posterity - knowing this video will live on for years and the time you invest now will pay off in a big way. We've checked our egos and have a clear focus on the topic... let's do a final review.

You won't want to do this. That's OK, it will show in your final work. You'll fight me on it, but all I can tell is you that it's worked well for me: **write it all out**.

Yep. Everything you want to say - make it hit paper. Let your muse fly and see if you can put words to your outline, keeping in mind all the things we've talked about. If you can't, then you have a problem which usually means your story is boring.

**And you want to find that out now**. Not later when no one watches what you've done. You'll be the first person to be uninspired by what you thought was good!

There's a lot to writing good dialog - word choice, pacing, not repeating yourself. I'll go into that in the next post...]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/goodnight_screencasts.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/goodnight_screencasts.jpg" />
  </entry>
  <entry>
    <title>Avoiding Messy Situations With KnockoutJS and JavaScript</title>
    <link href="https://bigmachine.io/posts/avoiding-messy-situations-with-knockoutjs-and-javascript" rel="alternate" type="text/html"/>
    <updated>2012-08-12T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/avoiding-messy-situations-with-knockoutjs-and-javascript</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/knockout_slide.png" alt="Avoiding Messy Situations With KnockoutJS and JavaScript" /></p>
## Steve, Jabber, and Changing My Mind

I went to [NDC 2012](http://www.ndcoslo.com) in June, 2012 and as I drove from my brother's house in San Diego, CA to Los Angeles to catch my flight to Europe, I listened to [JavaScript Jabber](http://javascriptjabber.com/013-jsj-knockout-js-with-steven-sanderson/) - the one with Steve on, who was there to basically defend Knockout's existence in the face of a tidal wave of JavaScript snottiness.

Like the amazing English gentleman that he is - Steve remained polite, self-effacing, and utterly brilliant as he dismanted the wave of condescenscon coming in from the "panel". Listen to it. You'll know what I mean.

It was such a brilliant interview that I found myself feeling very badly about my existing opinions on Knockout. Frankly: I don't think Knockout is used (primarily) by devs that care much about keeping their javascript clean. It's a simple tool - plug it in, run up some bindings, and you're done.

Simple tools are fun but simple tools are the simplest to abuse. That **is not Knockout's fault**.

So, as it turned out, I had a 2 day layover in London so I sent Steve an email and asked if I could buy him a Chiswick's... and he agreed. I know that, with some discipline, I can keep my Knockout code clean - but there were a few things that were still bugging me.

## Taking about Models in Bath

I took a train to Bath (amazing area) and met Steve in an old, creeky pub that I think was built by the Romans. I had a lot questions to ask - and we got right to it. One thing that I've felt was "left out" from Knockout was the concept of the "model". There isn't one - yet it's one of the "Ms" in MVVM!

So I asked him:

> Where's my model!

Steve thinks for a moment, then says:

> Yes I spose you might call Knockout a "VVM" since we don't have much in the way of a model. Personally, I like to think of the model living on the server. Why should you have to duplicate that logic on the client?

**Clang.** That... makes... a whole ton of great sense.

I've been comparing Knockout to what I know of other frameworks like Backbone and Ember - and **of course it doesn't make sense**. That's not what Knockout does! It doesn't manage your URLs and routes, it doesn't fetch or update data; it's focus is clear: help you manage data in your DOM effectively.

If I let the whole client MVC idea go... Knockout all of a sudden started making a load of sense.

## Try Again

I went home and opened up the app that I was working on with [Sam Saffron - Tekpub's Speed Series](http://tekpub.com/productions/speedmvc) and put together an Order Fulfillment page that did all kinds of stuff.

My goal: **use Knockout without creating Yet Another Crappy Knockout Example.**

[I daresay it worked](https://github.com/tekpub/mvcmusic/blob/master/MvcMusicStore/assets/js/order_editor.js).

Many people don't care much for the data-binding approach - and I understand that. In my opinion however, the data-binding solution is much, MUCH more elegant than using a templating system like Handlebars or jQuery templates. I can't see how using templates is less... "aesthetically offensive" than having an HTML5-compliant data-bind tag.

I don't dislike JavaScript templating solutions - not at all. I just like Knockout's DOM templating better.

## See The Results

I know many people might have some strong opinions about this... I would just ask you to take a look at what we've put together. If you don't want to watch the video, [have a look at the code and let me know what objections you have](https://github.com/tekpub/mvcmusic/blob/master/MvcMusicStore/assets/js/order_editor.js).

I recorded everything I did and pushed the result today: [Tekpub's latest series: Practical KnockoutJS](http://tekpub.com/productions/knockout). The focus is two-fold:

- I show how to use Knockout to do some very compelling User Experience stuff
- I take the time to **not make a huge mess**. At the end I ask [Derick Bailey](http://lostechies.com/derickbailey/) to review what I've done - and together we tighten things up really nicely.

I'm immensely proud of what we put together for Knockout, and I'm happy to say that I've completely changed my point of view after having spent some time with the tool - using good habits and keeping the focus on a clean solution.

Hope you enjoy it!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/knockout_slide.png" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/knockout_slide.png" />
  </entry>
  <entry>
    <title>PostgreSQL Rising</title>
    <link href="https://bigmachine.io/posts/postgresql-rising" rel="alternate" type="text/html"/>
    <updated>2012-07-19T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/postgresql-rising</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/pg.jpg" alt="PostgreSQL Rising" /></p>
## Postgres.app
Some buzz going around the web today about [Postgres.app](http://postgresapp.com/). Most people don't understand why it's interesting - so here's a quick explanation.

Postgres can be configured with a lot of interesting options - the ability to run Geospatial indexing (PostGIS), creating functions with Ruby and JavaScript, and extending the query engine to go up against remote sources like GMail. These features need to be compiled with the core code - something that can be a bit of a pain even when using Homebrew on your Mac.

Homebrew simply grabs the source and compiles it based on common usage patterns replicated in the "recipe". Postgres.app is precompiled and runs as a little daemon that you can turn on and off when needed. Just download it, drag it into your Applications directory and you're done.

Compare that with a SQL Server or Oracle installation.

It's lovely, and you should use it. If you're running a Mac you'll also want [to use Navicat](http://www.navicat.com/en/products/navicat_pgsql/pgsql_detail_mac.html). There's a free version up there, but you can also just grab the demo to play around.

## Say Hello to an Old New Friend
PostgreSQL has been around forever. If you've read this far the one thing that's probably sticking in your brain is **WTF with the name?**. [Tom Lane explains](http://archives.postgresql.org/pgsql-novice/2006-07/msg00063.php):

> The name is PostgreSQL or postgres not postgre... Arguably, the 1996 decision to call it PostgreSQL instead of reverting to plain Postgres was the single worst mistake this project ever made. It seems far too late to change now, though

Postgres was adapted from a very old database system [Ingres](http://en.wikipedia.org/wiki/Ingres_(database)) with the goal of producing a pluggable, scalable, fast database system that was (for the time) user friendly and smart. The name (POSTgres) was supposed to be a reflection of the former project - "After Ingres" - but ... well you get it.

## Who Cares?
That's the thing - not many people. With the rise of PHP the friendlier MySQL platform was adopted much faster than the stricter, rules-based Postgres platform. And by "friendlier" I mean "stupider".

That's a heavy word. Let's back up that statement. [This is a video](http://www.youtube.com/watch?v=1PoFIohBSM4) that I put together for Tekpub called "The Perils of MySQL":

<iframe width="640" height="360" src="http://www.youtube.com/embed/1PoFIohBSM4" frameborder="0" allowfullscreen></iframe>

If you don't want to watch the whole thing, here's a summary:

* MySQL will happily ignore your defaults and constraints as an effort to "help you" by not being terribly strict
* It will insert "" into non-nullable columns if nullable values are disallowed
* It will insert nonsense dates (0000-00-00) into date columns if nullable values are disallowed
* It returns NULL for 1/0
* It returns NULL for "THIS IS NOT A NUMBER"/0
* If you try to insert 1000 into a column that only allows a length of 2, it will round that number down to 99

This is why you should care about using MySQL. It doesn't safeguard your data by default - it tries to help you out by bending data integrity. Which in my mind is stupid.

If none of that freaks you out, then I have one word for you: **Oracle**.

## Fast, Scalable, Fun
Postgres is packed full of features - many of which people don't know about. The system is full of syntactic niceties such as:

* The keyword "infinity" - which means "bigger than any value entered here". This works for numbers and dates and can be set positive or negative
* Sensible date keywords like "today", "tomorrow", "yesterday". In Postgres 9.2 this gets even better.
* Amazing data types like Arrays, IP addresses which understand IPV6. Spatial types like lines, squares and circles.
* **Table Inheritance** which is a freaky feature which allows you to have one table, literally, inherit from another.
* Natural Language Full Text searching - out of the box

These features are meaningless unless the system is fast and scalable. Which Postgres very much is. My DBA friend [Rob Sullivan](http://datachomp.com) and I put Postgres to the test - loading in a StackOverflow data dump with 6 million text records.

We fired up our query tools and started optimizing our system against SQL Server (which had the exact same data load). Not only did Postgres stand right up to the speed of SQL Server - in many cases it eclipsed it (again - based on our measurements - both db's on a Windows box).

**Indexing wasn't the only story here**. We were able to partition the tables using inheritance (you can partition SQL Server too - but it's ad-hoc and you have to pay an enterprise license) and squeeze out even **better performance** due to reduced index size.

As if that's not enough - Postgres comes (out of the box) with TOAST tables - which is a weird name for "Automatic Table Compression". I show this in a demo (see the end of the post) but essentially Postgres will compress your data on disk - reducing RAM usage as well as disk space. 

This reduced our SO data dump from 24 gigs down to 6 - which is a huge savings. This feature is free and included with Postgres. With SQL Server it costs an Enterprise License.

## 5 More Things You Didn't Know
There's so much more to write about what Postgres can do -  instead I'll just link to a presentation I gave at NDC 2012 - [5 Things You Didn't Know About PostgreSQL](https://vimeo.com/43536445). I cover things like:

* Querying Twitter with Foreign Data Wrappers
* Writing Functions in JavaScript using Google's V8 Engine
* Dumb MySQL tricks
* Avoiding locks with Postgres default, built-in snapshotting
* Table Inheritance
* Crazy Datatypes

Hope you enjoy it:

<iframe src="http://player.vimeo.com/video/43536445" width="680" height="382" frameborder="0" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/pg.jpg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/pg.jpg" />
  </entry>
  <entry>
    <title>Try It Quiet</title>
    <link href="https://bigmachine.io/posts/try-it-quiet" rel="alternate" type="text/html"/>
    <updated>2012-07-04T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/try-it-quiet</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/enjoy-the-silence.jpeg" alt="Try It Quiet" /></p>
## New. Not New.

NDC is dominated by Microsoft developers - many of whom are curious about other technologies like Node and Rails. When trying to describe Node to a Microsoft developer a natural Q&A starts that can go a number of ways - but I found, oddly, that the outcome was quite predictable.

A number of .NET developers kept on me about "how is this new?" I think that's a **great question**. The concepts behind Node are not new - people were doing this stuff with VB of all things (I can't remember the link or the name). Unfortunately these conversations took a not-so-fun turn.

> NPM. NuGet. RubyGems whatever it's all the same. There's nothing new about a package management system and nothing new about Node.

That's usually where the conversation would end by me smiling and wishing them a good conference.

On the more positive side (which happened more often) - people would want me to contrast Node with ASP.NET MVC. I don't like contrasting ... anything - you end up sounding like you're "selling" an idea rather than discussing the differences.

## Don't Talk

The best way to get around these conversations? **By not talking at all**. That's what I did for the NDC talk - I sat down and coded [to some fun music](http://itunes.apple.com/us/album/robot-rock-oh-yeah-live/id267509560?i=267509568) for 50 solid minutes.

My goal was to **show you** rather than regale you with a wall of words, arm-waiving, and claims of code salvation. In short: **I want you to make up your own mind.**

Many people left. In fact there was a stream of people leaving in the first 5 minutes or so, and many of them gave me "red cards" - the NDC scoring system method for saying "you really suck". One person sent me a message on Twitter that told me "You just waisted my time".

I was nervous (to say the least) and if you watch the video you'll see me wipe my sweaty palms on my pants - trying to steady my nerves. I didn't look up. I couldn't look up - I thought I would see a mostly empty room.

As the talk went on, people started filling in. Then more people... and more people. I heard a lot of murmurs as people were helping others near them understand...

Finally - when the talk was almost over I looked up to see a room of eyes staring at the code projected on the screen. Some helped me debug things - others cocked their heads sideways - solving problems with me as I wrote stuff out.

I received some nice feedback on the talk - so I thought I would share it with you. It's a bit rocky in the beginning - the talk was pushed up 24 hours as another speaker missed her plane to the conference and we had to do a last minute shuffle (literally, I found this out at midnight the night before).

I was terrified, I was underprepared, and I wouldn't recommend doing this style of talk to anyone. But it was fun (as odd as that might sound) and many people enjoyed it - and yes I think I very well might do this talk again because clearly I enjoy torture. That's what happens at conferences :).

Anyway [here's the talk](https://vimeo.com/43548699):

<iframe src="http://player.vimeo.com/video/43548699" width="680" height="383" frameborder="0" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/enjoy-the-silence.jpeg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/enjoy-the-silence.jpeg" />
  </entry>
  <entry>
    <title>Leaving Your Mark</title>
    <link href="https://bigmachine.io/posts/leaving-your-mark" rel="alternate" type="text/html"/>
    <updated>2012-04-17T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/leaving-your-mark</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/makersmark.png" alt="Leaving Your Mark" /></p>
## Slop
The rampant amount of errors you run across (if you use IE with script error notification enabled) will make you sad for the internet. It lead my friend Miguel deIcaza to postulate:

>[Guess a sloppy language encourages sloppy practices](https://twitter.com/migueldeicaza/status/192364208007036928)

A broad stroke, but you can't deny that JavaScript has a number of pain points. Inexperience writing in a painful language on top of shifting platforms (browsers) with changing standards... it's easy to agree with Miguel's sentence.

But it made me wonder: 

> Do we get mad at JavaScript so much because we can see the problems so readily? 

It's right in front of our noses (especially if you have script errors on) - and the source is there for everyone to read and run through JSLint (JavaScript validator).

_**Side note**: for fun I ran Don's blog through JSLint and it was red red and more red. This isn't a critique of Don, more of a **wow dude might be more right than he knows** kind of thing._

## Your Slop
What if you could lift the lid on every site out there and peer inside at the source code? **What if you could flip a switch that says ignore every try/catch, every begin/rescue and let the errors flow freely**?

I think the internet would stop working.

I think blaming a language for being sloppy is probably a bit off base - [as Phil put it:](https://twitter.com/haacked/status/192364637461811201)

> sloppy developers promote sloppy code. It's just most sloppy code is hidden from us more than JavaScript is. ;)

I think Phil's right. But it brings up a question: *when you write code, how many errors do you write per hour?* And, more importantly, how do you know they're there and then fix them?

## Forged From The Fiery Pits of Exception Hell
Think of all the code you've written over the years - JavaScript, Ruby, PHP, C# - whatever. Where is it now? How many bugs, logic errors and overall madness have you given birth to?

About 1 time out of 9 I can write a routine that works the first time. And when that happens I usually let out a "YIP!" - and then immediately distrust whatever it was that I wrote.

There's no way I can get it right the first time - Odds are that I can't. I know that so I test my stuff as much as I possibly can. Even then I know I'm letting crap out the door. It's human nature.

Our brains are wired to improve upon millions of years of pain and suffering (if you believe in evolution). Why shouldn't our code be the same way? What do we know of any other way to create... anything?

_If you have kids and were present at their birth, you very much know what I'm talking about_


## The Mark of the Maker
Our code is forged from the pain of misunderstandings, miscommunications, hubris, bullheaded ego and a smidge of inspiration and love. After a period of slow-torture and water boarding, our applications take shape.

The true question is:

> Do you forge your code carefully, fold it gently, and pound the finest edge found anywhere in the land? Does your code bear the marks of its maker?

The steel of the sword is only valuable as a measure of vanity - a tree limb can take a man's head if it's properly swung. It's the Mark of the Maker the tells the true tale of the weapon.

What does your Mark look like?]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/makersmark.png" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/makersmark.png" />
  </entry>
  <entry>
    <title>Cleaning Up Deep Callback Nesting With Node&apos;s EventEmitter</title>
    <link href="https://bigmachine.io/posts/cleaning-up-deep-callback-nesting-with-nodes-eventemitter" rel="alternate" type="text/html"/>
    <updated>2012-04-05T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/cleaning-up-deep-callback-nesting-with-nodes-eventemitter</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/ChristmasTree.png" alt="Cleaning Up Deep Callback Nesting With Node&apos;s EventEmitter" /></p>
## The Registration Problem

Consider this: you want customers to register with your site. When they do, a number of things need to happen:

1. The information needs to be validated
2. The customer record inserted
3. An email sent to say "thank you"

In a typical scenario, there's probably more - but let's use this for now.

You might know straight away how to do this in Ruby or C# - but how would you handle this with Node and JavaScript?## O Christmas Tree

This is some code that you might see in a Customers module:

![](https://bigmachine.io/img/Screen-Shot-2012-04-05-at-11.22.43-AM.png)

Yuck.

This code is not only hideous-looking, it's also synchronous and a nightmare to maintain.
Node allows you to do this much better with EventEmitters. Let's see how to use Events to clean this code up.

## Emit It

There are two ways to do this: encapsulate the eventing, or make your entire object an EventEmitter through inheritance. I'll do the latter.

The first thing to do is reference Node's event module and the util module as well - it has some helpers we'll need. Then we rewire the module to handle the events - I'll explain in a second, but here's the final code:

![](https://blog.bigmachine.io/img/Screen-Shot-2012-04-05-at-11.28.17-AM.png)

So what's going on here? Well first - there are no more callbacks - we don't need them! We have events to listen to.

I'm using Node's built-in EventEmitter object to "graft" on some functionality to my Customer object. JavaScript doesn't have inheritance, per se, but you can take the prototype of one function and pop it on another.

Node helps you with this using the "util" library. On line 42 we're telling the util to push the prototype from events.EventEmitter onto our Customer function. Notice that this is a function, not an instance of a function as I had in the first example above.

Next, on line 7, I had to invoke the "base" constructor to be sure that I don't miss any internal instancing or setting of values. Turns out for EventEmitters you don't need to do that and you can omit this line - but it's safe to just do it, no matter what.

In the body of each method I'm simply "emitting" an event to all listeners (there's obviously some code missing here - pretend that I have an insert routine and so on). I can emit an event for whatever happens along the way - a successful validation fires "validated", a failure might fire "validationFailed". This frees up our code to do what it needs to do and no more, making it much cleaner and clearer.

On line 29 I've added a final event trigger if everything works out: "successfulRegistration". This is what calling code will really be interested in the most - either that or "failedRegistration" - and we pass along the customer record (more on that in a second).

On lines 35 through 38 we've implemented a bit of workflow. This doesn't need to be inside the Customer function - you can arrange these events wherever and however you like. The calling code can remove every event listener and replace it with its own if it wanted to reorganize the flow here.Speaking of calling code, here's what it might look like:

![](https://blog.bigmachine.io/img/Screen-Shot-2012-04-05-at-11.40.12-AM.png)

You hook into the events, then run register() and respond as needed.

## All Over The Place

Node is built on top of EventEmitters - you'll find them everywhere. Understanding them is key to writing cleaner code that's more functional and maintainable - and it also helps keep things asynchronous.

n the first example, we had a synchronous drop all the way down - even though we were using callbacks. Our code above isn't synchronous at all - we've hooked into an event and when Node is ready, it will process the emitted event callback.

Neat stuff!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/ChristmasTree.png" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/ChristmasTree.png" />
  </entry>
  <entry>
    <title>Something Borrowed, Something New</title>
    <link href="https://bigmachine.io/posts/something-borrowed-something-new" rel="alternate" type="text/html"/>
    <updated>2012-03-08T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/something-borrowed-something-new</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/postgresql_logo-555px.png" alt="Something Borrowed, Something New" /></p>
## 30 Seconds. Just 30 Seconds.

I always say - if you can't give me the pitch in a single sentence, in 30 seconds, your idea isn't worth what you think it is. So here's what I got for you:> PostGres will blow your mind; given its Enterprise features (Compression, partitioning, Full Text indexing, etc) , ease of use and configuration, and intelligent feature set - it's likely you'll want to use it tomorrow.

I'm not trying to sell you anything - [I just want you to consider what's out there besides your DB of choice](http://tekpub.com/productions/pg).** I'm doing what Tekpub is here to do: open your mind** to new ideas and ways of doing things. The rest, as they say, is up to you.If you're still interested: read on.

## Lack of Tooling

In the very first video **I put Entity Framework on top of the "world" sample database in PostGreSQL using the EF designer**. I'm using the [DevArt "dotConnect" ](http://www.devart.com/dotconnect/postgresql/)driver to do this which isn't free - but if you use EF it's likely you won't have read this far anyway.**Massive, PetaPoco, NHibernate, OrmLite - most data access tools can work directly with PostGreSQL** no problem. PostGres also comes with pgAdminIII which is functional and works just fine. I like Navicat and use it for Tekpub.com - worth every penny.There's lots of tooling. It's not SQL Server -[ but it's worth a look](http://tekpub.com/productions/pg). Stay with me.

## Enterprise

Developing a web site is easy. For .NET developers it usually involves some flavor of SQL Express or SQLite/CE and "my company has licenses to SQL Server". That's fine - SQL Server is an amazingly capable database.The nice thing about PostGres is that it come with enterprise features right out of the box:

- **Compression** - large tables are automatically compressed to save you disk space and RAM
- **Partitioning** - another SQL Server Enterprise Edition feature: comes ready to roll with PostGreSQL
- **Full Text Indexing** - try using this on Azure. You can't.
- **Memory/File Caps** - none
- **Replication** - yep

It's easy to disregard this stuff when you're starting out a project. When you grow quickly and hire a [dickhead DBA](http://datachomp.com) - that's when you start paying the Big Bucks. Many times it's worth it if you know and are comfortable with SQL Server - but there are all alternatives - this is my only point.

## Not Just My Opinion

We just launched [our latest production at Tekpub](http://tekpub.com/productions/pg). It didn't start out as a PostGreSQL tutorial - but it turned into that because I couldn't keep [Rob Sullivan](http://datachomp.com) from foaming at the mouth during every single recording - the dude was tweaking over the moon about PostGreSQL.

Rob S. is a SQL Server DBA. He thought PostGres was "non-existent" and "who cares" - like most .NET/Microsoft folks. I asked him to look into it since we'd be working on a production about Mono/ASP.NET and... he lost himself.

## Christmas Presents

That was the way Rob described working with the administrative/DBA features of PostGres.** It just does things - intelligently** - that you wouldn't expect. One that I really liked was Table Inheritance.

The first time I heard about it I thought "neat trick - don't think I want OOP in my database though". Turns out that PostGreSQL doesn't use Inheritance just to make the Gang of Four happy - they use it to keep your database performant and lean.

When your database grows rather large (think Stack Overflow) - indexes become difficult to maintain. At this point you need to partition your tables - literally "break the big tables apart" along some lines in order to maximize indexing.

StackOverflow, for instance, has one massive "posts" table - and a lookup that describes a post as a Question or an Answer. This works out well until you have 500 million rows - then your indexing goes into orbit.

PostGreSQL, on the other hand, allows you to setup an inheritance scheme that is backed by table partitioning. You can do this, literally:

```sql Table Inheritance with Postgres
create table posts(id serial primary key, body text, created_at timestamp);
create table questions(question_id serial primary key, owner_id integer, answered boolean)
  inherits(posts);
create table answers(answerer_id serial primary key, question_id integer)
  inherits(posts);
```

Look weird? I thought so too - until you consider that each of those tables are full-blown tables with their own primary keys and indexing - yet they have common data between them (body ) - which can be Full Text indexed.I know many people will be thinking - "why the hell would I do this?". The answer, as it turns out, comes from the real world.

## Performance.

PostGreSQL is fast. Very fast. This wasn't always the case but with version 8.0 they devoted everything to speed - getting away from disk space consideration as disk storage became very, very cheap.

StackOverflow keeps everything in a Posts table - so [on their home page how many answers do you see](http://stackoverflow.com/)? Answer: 0. All you see are questions. As it turns out, there is (roughly) a 4:1 ration between answers and questions (meaning there are, on average, 4 answers per question).

If they used an inheritance scheme as above - this would mean an instant 75% reduction in what the indexer has to crawl in order to serve that main page. It also makes the query more semantic - which is a nice bonus.

In addition to that - we're saving disk space! We don't have empty rows in our denormalized posts table taking up space (answercount doesn't matter on answers!). This is a win for everyone.We were pleasantly surprised by the performance we saw on Windows - that said [it doesn't run as quickly as it does on Linux](http://stackoverflow.com/questions/1162206/why-is-postgresql-so-slow-on-windows).

It's not nearly as slow as MySQL on Windows and Rob and I were able to pull just as good (and in many cases better!) performance in a side-by-side comparison with SQL Server (using the SO data dump).

All of that said - running your database on a Linux box offers some nice scaling alternatives if you're sensitive to cost.

## Doing Our Job

Tekpub's job is to show you what's out there - not to talk you into things or sell you on a concept. This post might come across as a sales pitch - **but these are the lengths it's necessary to go to to get people to even think about alternatives** to what their tooling (VS and SQL Server) allows them to do.

I invite you to set aside your allegiances and what you're familiar with - and just have a look at what is quickly becoming a favorite data storage tool for many, many developers. Even if you're never going to use PostGreSQL - you should know what it does and how, just in case you run into a Neck Beard somewhere and want to show off.

Rob and I had a great time making this production - I hope you enjoy it.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/postgresql_logo-555px.png" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/postgresql_logo-555px.png" />
  </entry>
  <entry>
    <title>Someone Save Us From REST</title>
    <link href="https://bigmachine.io/posts/someone-save-us-from-rest" rel="alternate" type="text/html"/>
    <updated>2012-02-28T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/someone-save-us-from-rest</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2809.png" alt="Someone Save Us From REST" /></p>
## ZOMG Did You Hear The News?!?!?!?

> **[PATCH is the new primary HTTP method for updates](http://weblog.rubyonrails.org/2012/2/26/edge-rails-patch-is-the-new-primary-http-method-for-updates)**. Think files, for example. If you upload a file to S3 at some URL, you want either to create the file at that URL or replace an existing file if there's one. That is PUT.

>Now let's say a web application has an Invoice model with a paid flag that indicates whether the invoice has been paid. How do you set that flag in a RESTful way? Submitting paid=1 via PUT to /invoices/:id does not conform to the semantics, because such request would not be sending a complete representation of the invoice for replacement.

Do you know who this is important to? **No one.** Well maybe except [this guy](https://github.com/rails/rails/issues/348) (from the Rails issue list): 

![](https://bigmachine.io/img/Screen-Shot-2012-02-27-at-3.18.47-PM.png)

Can you imagine? **How in the hell did Rails make it this far without properly implementing PATCH?** 

As opposed to piling on to [the "Rails 3 Sucks" bandwagon](http://gilesbowkett.blogspot.com/2012/02/rails-went-off-rails-why-im-rebuilding.html) (which[ I don't think at all](http://wekeroad.com/2012/01/12/understanding-the-rails-asset-pipeline/) - I quite like it) **I'd like to openly ask the Rails team to [remember what got them here](http://rubyonrails.org/quotes)**.

Which, I daresay, was a bit of attitude and daring as well as the ability for the team to Give The Heismann to the Spec Police and Enterprise Engineers of the world.

![HEISMAN](/img/heismann.jpeg )

But as I say - **I'm not writing this to "Occupy Rails"** as it were and suggest that "we are the 99%" (even though we are... ) - no I'd rather look at some of the shockingly strange things that people do in the name of REST (like [push a verb change past 743 other issues in the list](https://github.com/rails/rails/issues)). 

**It's time to stop being afraid of the REST police.** 

![I find your lack of REST disturbing](/img/lackoffaith.png)

## Did You Just Make The Jump to l33tSpeed?

That's one of the most appalling things about any REST discussion. Otherwise interesting and intelligent people flip on the Pedantic Switch and decide that quotes and explanations (also known as [Appeal to Authority](http://en.wikipedia.org/wiki/Argument_from_authority)- see what I did there?) are the best way to explain something. 

Because it needs explaining. Because clearly you don't know REST.

**Did you know Rails 4 is *_finally_* using PATCH?**

Feeling Antsy? Should I bring up [Big O notatio](http://www.hanselman.com/blog/BackToBasicsBigONotationIssuesWithOlderNETCodeAndImprovingForLoopsWithLINQDeferredExecution.aspx)n or [P=NP](http://www.codinghorror.com/blog/2008/11/your-favorite-np-complete-cheat.html) to round out the douche-quotient here? Let me guess...

 - You've already formulated the first part of your comment - and how you're going to politely address what I don't know.
 - You want to help me - to explain what I don't know... to "enlighten" me with all that you've found out about REST and how it works.
 - You know a person who "really knows REST" that I should talk to if I'm confused.
 - You just went over to Wikipedia to brush up on definitions
 - You read that Wikipedia article for the first time in the last 6 months and, since it's fresh in your mind, feel rather clear on the concept so it's worth your time to "help me out".
 - You've written a REST API before and "do it every day" so you really understand it well.
 - You know Roy Fielding personally and helped him write his dissertation.
 - You just checked Wikipedia to see if it was Roy, or Ray - so as to point out just how in the dark I must be.

This was my experience last week. This has been my experience last year. This will continue to be my experience well into the future because REST is something everyone must know (like HTML5, CSS3 and CoffeeScript).

Yet they don't. Neither do you. Certainly I don't. We're all going to get sucked into the void without truly knowing what Fielding was trying to tell us... and in the end... there's only...![SOAPBOX](/img/soapbox_9.jpeg)

A Soapbox. Because it's not "knowable" even though I'm sure you really do know it. It's not a thing that two people will come together and agree upon but the conversations are always fun:

> Oh yes yes - that's a lovely example of a nice RESTful API: using HTTP verbs to operate on a resource represented by a unique URL... lovely

> Right, yes but it [doesn't support PATCH](https://github.com/rails/rails/issues/348) **and it's using Rails. Hardly RESTful...**

> Right, yes but - they really should be using CONNECT and TRACE with Node and dropping into a socket to really do REST the right way

> Right but oh no no no - CONNECT should really only be used with HTTPS and one shouldn't need to rely on a single framework to define their RESTful API. And moreover... are you trying to suggest that REST over HTTP has anything to do with WebSockets? Cause ummm....

> Right, yes but you just used Rails to say something is not RESTful did you not? May I not do the reverse? And WebSockets are a viable alternative then creaky old HTTP so why can't we discuss --

> Right, yes but in doing so your knuckles become that much closer to the ignorant ground. Clearly you haven't read [Section 6.3.3.1 in which Fielding clearly states](http://www.ics.uci.edu/~fielding/pubs/dissertation/evaluation.htm#sec_6_3) that the HTTP protocol, as it stands, is not enough to support high-performance concurrent connections and, in fact, proposes two new protocols: MGET and MHEAD which **deal more directly with MIME response** which is a much nicer implementation then --

>Right, YES BUT YOU should know that **MGET and MHEAD were REJECTED** because they VIOLATED many RESTful conditions - specifically that before a RESTful request could be made, the requestor would have to make all of its requests at ONCE because it wouldn't actually know the length of the request prior to sending and requests are bound by length and it's fundamentally --

>WELL ACTUALLY I understand that but you're not hearing me: Fielding never wrote this dissertation with WebSockets in mind and --

>DUH IDIOT YES OF COURSE HE DID HE KNEW IT WAS COMING**

>You presume to know the mind of Fielding**? Well then - that certainly puts us into clever territory! No one can presume to --

>RIGHT YES My dear fallacious, mouth-breathing idiot of a friend: my brother went to UC Irvine, where Fielding studied. I've read his dissertation's abstract at least 2 times. I know Fielding, sir, and you're no Fielding...**

>Right YES BUT...

<span style="font-size:80px">**AMIRITE?**</span>

## Blasphemy.

I'll come to the point: REST is a fascinating andilluminatingset of ideas and, as it turns out, is a handy guideline for effectively preparing an API. As it turns out - digging deep into what you think REST means runs the perils of digging deep into any religious philosophy: **adherence turns into devotion**.

I like REST like I like any [Religious Doctrine]. I dislike the people who drop Fielding Passages in some apostolic attempt to save my non-RESTful soul. **My relationship with REST is a personal one** and, frankly, I like to think REST guides me in my application development walk in the way Fielding intended for me, and my web app.

**I like to think that no one truly knows the mind of Fielding** and, even then, [his dissertation was written](http://www.ics.uci.edu/~fielding/pubs/dissertation/top.htm) in the very ancient tongue of **12 year old internet technology. 12 YEARS!** How can we possibly think we understand, today, what Fielding was sharing back then?

> Isn't it possible that Fielding speaks to each of us? In his own way?

## No, Really
A good friend of mine is a Fielding devotee and he regularly will try to "help me out" with my apparent lack of devotion. I know I shouldn't answer the door when these people come knocking - but he was making the rounds and as I mention - he's a friend of mine.

He shows me some samples to read over - and against my will - I found myself drawn in.

**UPDATE** - turns out this was written by my friend Glenn Block. He [followed this post with one of his own](http://codebetter.com/glennblock/2012/02/28/why-are-there-2-controllers-in-the-asp-net-web-api-contactmanager-example-rest-has-nothing-to-with-it-2/), explaining why he did what he did.

And I asked the question. I shouldn't have. But I couldn't help it. The Controllers... there were two of them, and they had the same name, and their code was roughly the same...

![](https://blog.bigmachine.io/img/2012-02-27_1431.png)

I should have known I was doomed:

> Why are there two Controllers here - one singular, one plural?

And madness ensued.

## Well, Actually...
No seriously: **Madness**. My question about the Controllers in the above example came from **the standpoint of a developer, trying to understand what the application does**.  Two Controllers make no sense to me in a small application of this size.

Some suggested the developer saw the list of Contacts (dealt with in the pluralized controller) as a separate Resource... which means(?) an separate Controller.

Really?

And the defense of the the extra controller doubled back on itself ... and me as well:

>Controllers don't dictate REST anyway what's wrong with you!?!!

**What's wrong with me is that I have a dumb habit of asking questions.**

And a worse habit of asking more questions when the answers don't make any sense. Or are simply quotes and citations from a dissertation that's over 12 years old and has nothing to do with the Modern Web.

**Where have we come when the first headline of Rails 4 is a proclamation that "We're Using PATCH Instead of PUT!"** I can guarantee you that 95% of the audience had absolutely no knowledge of the PATCH verb prior to reading that passage.

And they promptly forgot about it when they navigated away.

<del>The developer who wrote those two Controllers will avidly defend them to me, citing Fielding and quoting Wikipedia.</del>

Glenn, as smart and amazing as he is, defends this practice by saying "a list of a resource is semantically different then a single resource - but this has nothing to do with REST".

But... you... I ... but... but...We will consume ourselves... slowly into RESTful Madness and ...

## Someday... this war's gonna end.

![He Comes](/img/zalgo___white_version_by_amindfuckingusername-d4ahwnj-1.png)

Admit it. *You think I don't know REST...*]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2809.png" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2809.png" />
  </entry>
  <entry>
    <title>NodeJS Callback Conventions and Your App</title>
    <link href="https://bigmachine.io/posts/nodejs-callback-conventions-and-your-app" rel="alternate" type="text/html"/>
    <updated>2012-02-25T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/nodejs-callback-conventions-and-your-app</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/>-" alt="NodeJS Callback Conventions and Your App" /></p>
## Did It Work Or Didn't It?

I have a method on my Customer module where I add a customer to MongoDB:

```javascript
var customer = require("./lib/customers");
customer.register("test@test.com", "password", "confirm", result, success);
```

The last two arguments here are callbacks. If you're new to JavaScript - keep in mind it's asynchronous which means that nothing really happens _right now_. If you need to know when something is done, you sending a callback function (which is probably better thought of as a continuation) which gets fired when the operation completed.

**But how do you structure your callback?**

In some circles (like Backbone and jQuery) you use a success/fail approach - like I did above. You pass one callback in that fires if everything succeeds, another that fires if it fails. This convention is rather clean:

```javascript
//you can also make these anonymous functions inline
success = function (result) {
  alert("Yippee Skippy: " + result.email + " was added to the DB");
};
fail = function (err) {
  alert("Something bad happened: " + err);
};
customer.register("test@test.com", "password", "password", success, fail);
```

Node's convention is a little different and (keeping in mind that I'm very new to Node) I can only guess that this convention came about to avoid deep nesting and excessively callback-heavy code. In Node, the style is thus:

```javascript
//inline this time
customer.register(
  "test@test.com",
  "password",
  "password",
  function (err, result) {
    if (err) {
      alert("Oh noes! " + err);
    } else {
      alert("yippee skippy...");
    }
  }
);
```

This code is clean and readable - and it's also **less** so that makes us happy. But in our method we have to do this... which is a bit wonky:

```javascript
var register = function (email, password, confirm, callback) {
  //pseudo code - instance and assign vals
  var newCustomer = new _model();
  //save it
  newCustomer.save(function (err) {
    if (err) {
      callback(err, null);
    } else {
      callback(null, newCustomer);
    }
  });
};
```

Now if the save() method on newCustomer had followed the Node convention - then I could have just passed the callback directly into it:

```javascript
var register = function (email, password, confirm, callback) {
  //pseudo code - instance and assign vals
  var newCustomer = new _model();
  //save it  newCustomer.save(callback);
};
```

... which is almost certainly why this standard was adopted (and I like it!). But it doesn't so we have to write some extra code to handle it. And I could spend another few paragraphs musing on the goods and bads - but I don't know nearly enough to do such a thing, so let's move on...

## What To Do?

If I learned anything from working with Rails over the last 6 years - it's to let Rails be Rails. This isn't a bad thing. Every time I try to get clever, I pay for it.

On the other hand, when I was talking to Batman yesterday (Dave Ward) he suggested the success/fail thing was a more widely-accepted way of doing things. Which I think I agree with... but the Node community doesn't. Thus my post.

Anyway - if the convention for NodeJS is to use callback(err,result) and I'm using NodeJS well - that's what I'll do. In fact I've already run into an interesting situation with Vows (the testing framework I talked about yesterday) where it **expected that I was using this structure**.

I'll have more to say about Vows in another post (I like it a lot) - but the fact that it (basically) wouldn't execute more than one callback nudged me into this entire discussion.I'd love to hear your thoughts.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/>-" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/>-" />
  </entry>
  <entry>
    <title>How To Backup Your Postgres DB To Amazon Nightly</title>
    <link href="https://bigmachine.io/posts/how-to-backup-your-postgres-db-to-amazon-nightly" rel="alternate" type="text/html"/>
    <updated>2011-11-01T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/how-to-backup-your-postgres-db-to-amazon-nightly</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/postgres.jpeg" alt="How To Backup Your Postgres DB To Amazon Nightly" /></p>
## Hello Postgres

You've setup Postgres and your new awesome website is banging away at one of the top database engines out there - hurrah! But that's not the end of the story - you need to back up your data before you do anything else! So let's get to it...

There are two ways to dump the data from Postgres - the first is dumping the entire database contents (tables, sequences, indexes, logins - aka "roles" - etc) - the second is just your database:

```
  pg_dumpall #dumps everything
  pg_dump myDb > my_backup.sql #dumps schema and data from myDb into my_backup.sql
```

Simple enough. Yes, it's command line. Run for the hills.

## Automation

One of the nice things about Postgres is that it tries very hard to get you to do the right thing. MySQL doesn't care as much. Again, another post - but the thing we care about here is that Postgres won't let you dump the contents of a database out unless... gasp... you have permission to do so.

The problem is that, unlike MySQL, you can't just pop the username and password inline like you would with MySQL:

```
  mysqldump -u root -p[OH NO YOU DIN'T] myStolenDatabase > dumpfilename.sql
```

As I said, MySQL is cuddly and sweet. Postgres won't stand for this shit and wants you to do your best to try and not shoot yourself in the foot. Now, for some this means enduring some pain. For others... well it's kind of nice to know Postgres has my back.

"But wait a minute - if I can't send in the password to my database... well how do I automate the backup?" Good question. There's an answer...

## Setting Up .pgpass

Postgres has an interesting way to allow you access to a database, and it's through a hidden file in your home directory called ".pgpass". Simply put - it's a bunch of settings that you can protect as needed (of course allowing Postgres to see it) - and mainly has that password that is required for running backups.

Here's what it looks like:

```
  hostname:port:database:username:password
```

A very utilitarian, Linux-y file format but... it works. The deal with this file is that Postgres will access it whenever the local user tries to do something with the Postgres server, and it will try to match up hostname, port, and database. If everything is a match, it will grab the username:password combination and see if you have rights to do what you want to do.

So if you want to set yourself up to have access to your super_whammadyne database running locally, you would pop this command into your terminal:

```
  echo "localhost:*:super_whammadyne:joe_user:secret_password" > .pgpass
  sudo chmod 0600 .pgpass
```

<p class="note">The * character means "any"</p>

That little command will create a .pgpass file for you (make sure you're currently in your home directory) and set the appropriate permissions on it so that another user can't just waltz in and steal it (that's the CHMOD part). In fact Postgres will ignore this file entirely if you didn't set permissions to 0600. Neat.

Now, when you SSH into your system and want to play with your database:

```
  psql super_whammadyne
```

Postgres will snoop your home directory for a .pgpass file. Then it will see which host you're calling in from... which in this case would be "localhost" since we've SSH'd in. We've matched up localhost, the port (since it's a wildcard), and the database name - now Postgres will try to login using "joe_user" and "secret_password" - and it will work nicely.

This works for DB interactions using the psql shell - but you can use it to do other things ... assuming you have rights. Like back your stuff up!

## Running pg_dump with Rake

Let's assume you have an Amazon S3 account and you want to use it to back up your Rails app. There are a number of ways to do this - but one way I like is using Ruby and Rake since I can use the aws-s3 gem to talk to Amazon.

First thing to do is create a task for this (assuming you're in the root of your Rails app):

```
  mkdir lib/tasks/backup.rake
```

Next, in your Gemfile make sure you add references to the "aws-s3" gem, and the "zip" gem. We're going to Zip our DB up and push it to S3 every night:

```ruby
  gem 'aws-s3'
  gem 'zip'
```

Now, let's create the Rake task:

```ruby
  desc "PG Backup"
  namespace :pg do
    task :backup => [:environment] do
      #stamp the filename
      datestamp = Time.now.strftime("%Y-%m-%d_%H-%M-%S")

      #drop it in the db/backups directory temporarily
      backup_file = "# {RAILS_ROOT}/db/backups/db_name_# {datestamp}_dump.sql.gz"

      #dump the backup and zip it up
      sh "pg_dump -h localhost -U joe_user super_whammadyne | gzip -c > # {backup_file}"

      send_to_amazon backup_file
      #remove the file on completion so we don't clog up our app
      File.delete backup_file
    end
  end
```

All in all a pretty straightforward operation. But what about pushing to Amazon? That's in the send_to_amazon method:

```
  def send_to_amazon(file_path)
    bucket = "db-backups"
    file_name = File.basename(file_path)
    AWS::S3::Base.establish_connection!(:access_key_id => 'YOUR KEY',:secret_access_key => 'YOUR SECRET')
    #push the file up
    AWS::S3::S3Object.store(file_name,File.open("# {file_path}"),bucket)
  end
```

And your done.

## Run It Nightly

We're using Linux, which means that at some point we'll have to put our rubber gloves on and stick our fist deep inside Cron. Some people like it, some don't. I'm the don't people.
.note
There's also the [whenever gem](https://github.com/javan/whenever) that's a gift from above when it comes to working with Cron and Rails. I love it and you can use it easily to run this backup. I'm going to show you the hard way :).

SSH into your server and open up Cron:

```
  crontab -l
```

If you don't know what "cron" is or does, it's simply a text-based set of rules for running periodic tasks. The incantations can be a bit opaque - so if you can use Whenever to write this for you, you'll be better off.

You might have one or more cron tasks in there already - make sure you don't mess them up :). These tasks are user-specific, so you won't mess up any system ones, but take care if you see something else in there.

OK - now let's open up crontab and edit it:

```
  crontab -e
```

The lovely VI terminal should open up in front of your loving eyes. Now we get to enter our incantation:

```
  0 0 * * * /bin/bash -l -c 'cd /home/joe_user/web_app/current && RAILS_ENV=production rake pg:backup --silent'
```

<p class="note">
If you're running VI and you're completely lost here - hit "I" to insert text then past, or copy in, the command above. When you're done hit "escape", then enter ":x" to give the command to save and exit.
</p>

This command tells Linux to run a command at midnight (0 minutes, 0 hours - or 0:00) every day of the month, every month, every day of the week (the wildcards) and then gives it the command to execute. Which in our case is changing into our app's directory and running our rake task.

Adorable isn't it?]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/postgres.jpeg" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/postgres.jpeg" />
  </entry>
  <entry>
    <title>Massive: 400 Lines of Data Access Happiness</title>
    <link href="https://bigmachine.io/posts/and-i-shall-call-it-massive" rel="alternate" type="text/html"/>
    <updated>2011-02-16T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/and-i-shall-call-it-massive</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[![](https://bigmachine.io/img/massive.jpeg)

##  It's Massive Yo

If you want to play with it - here it is: [in my Github repo.](http://github.com/robconery/massive) 

It's a single file - something you can use, change, love, explore, beat up, cuddle, and generally ignore if you feel like it. It's simple data access - just about as simple as it gets. I needed to give it a name (because if I didn't someone else would and I'm afraid of what that would be) - so I decided to call it "Massive".

It started out at 350 lines of code, and then I refactored out `WebMatrix.Data` and pushed it well over 500 until [Dave Cowart](http://twitter.com/#!/davecowart) came along and squeezed it down to a readable 400 lines. Love that! Go Open Source!

I toyed around with calling it "SubSonic 4.0" - because honestly everything I ever tried to do with SubSonic is in here. But I thought the better of it - there are a number of really great people keeping SubSonic up and running, and I don't want to be a sh** and pull the rug out.

I just pushed this as a package to NuGet - so you can grab it right from there if I've packaged it right.

![](https://blog.bigmachine.io/img/massive_nuget.png)

## Design Choice 1: No Dependencies Other Than What's In The GAC

Originally I built this thing to sit on top of WebMatrix.Data. I like that little utility, but it meant that you needed to install MVC 3 etc to be able to use this thing. In addition, WebMatrix.Data does some funky things on it's own with sealed objects (DynamicRecord being one of them).

So I decided to kick that all to the curb and make sure the entire experience pivots on System.Data.Common and System.Dynamic.ExpandoObject. There is no longer a dependency on WebMatrix.Data.

## Design Choice 2: Ridiculously Simple To Use

To work with this thing you'll need...

 - .NET 4.0
 - A Database - SQL Server, SQL CE, MySQL, PostGres - anything with a System.Data.Common Provider
 - A connection to said database in the app or web.config
 - A basic understanding and comfort level working with dynamic "stuff"
 
 Let's use Northwind - because [everyone loves this little database.](http://www.hanselman.com/blog/CommunityCallToActionNOTNorthwind.aspx)
  
Make sure you have a connection to this DB in your app or web.config - call it "northwind".

Now, let's access some data:

```csharp
public class Products:DynamicModel {
    public Products():base("northwind") {
        PrimaryKeyField = "ProductID";
    }
}
```

I could name this class "Steve" if I wanted to  - but I'm using convention here and naming the class the same as the table. If I wanted to change the table name I would set the "TableName" property right under the PrimaryKeyField property.

Now we're ready to roll:

```csharp
var tbl = new Products();
var products = tbl.All();
```

That's the simples thing you can do. This will send a "SELECT * FROM Products" SQL call to the Database. From there a DbDataReader will be kicked up. Massive rolls over the reader, pushing the values into an IList - that "dynamic" being an ExpandoObject.

## Design Choice 3: Close To The Metal


Let's face it - no one likes writing SQL, yet we've sort of failed to abstract that dislike effectively. Spend some time with EntityFramework, NHibernate, or SubSonic and pretty soon you'll be wondering how/why/WTF with the SQL that these tools generate.

In other words: you need to know it anyway, or [your dickhead DBA :)](http://datachomp.com) will crawl right into your dark happy spot and make life miserable for you (OK Rob Sullivan isn't all that bad - I just like to poke fun :).

There's no better DSL for working with databases than SQL. It's concise, it's powerful, it's here to stay. I love the whole NoSQL thing and I wish that we could embrace it and move on. A guy can dream.

Anyway - you can work with any WHERE statement you like - just send it in as a named argument (same with OrderBy and Limit):

```csharp
var tbl = new Products();
var products = tbl.All(where: "CategoryID = 5 AND UnitPrice > 20", orderBy: "ProductName", limit: 20);
```

A few folks have noted that this represents a SQL Injection vulnerability. In truth ... well it doesn't since I'm not concatenating anything here - BUT the point is made that parameters are a good way to go. And indeed - Massive uses parameters:

```csharp
var tbl = new Products();
var products = tbl.All(where: "CategoryID = @0 AND UnitPrice > @1", orderBy: "ProductName", limit: 20, args: 5,20);
```

You can also sidestep, completely, the entire abstraction (hurrah!), and revel in the beautiful simplicity of your soiled code:

```csharp
var tbl = new Products();
var stuff = tbl.Query(@"SELECT ProductName, CategoryName from Products 
INNER JOIN Categories ON CategoryID = Products.CategoryID");
```

It all comes back as dynamic - so you can loop and pull ProductName and CategoryName as properties, strongly typed, and get on with your work.

## Design Choice 4: Working With Data Should Be Easy and Transactionable

One of the main reasons I moved off of WebMatrix.Data is because I couldn't get at the DbCommands that it spun up for each call. I wanted to grab those commands so I could work with multiple objects within a transaction. More on that in a second - for now, you can insert a new record like this:

```csharp
var tbl = new Products();
//Insert() will return the new ID
var newID = tbl.Insert(new {ProductName = "Steve", UnitPrice = 10.50});
```

This is using an Anonymous Object declaration - but you can also do this if you're using a web site:

```csharp
var tbl = new Products();
// Be sure to have a white list check that prevents over-posting!
var newID = tbl.Insert(Request.Form);
```

Update works in much the same way:

```csharp
var tbl = new Products();
tbl.Update(new {ProductName = "Cheesy Poofs", 12});
```

The same rules apply with form posts as well. To delete, you just do "tbl.Delete(12);" or "tbl.Delete('WHERE CategoryID = 5');".

With the move to System.Data.Common as the core rather than WebMatrix.Data, you can now do this:

```csharp
var tbl = new Products();
var products = tbl.All(where: "CategoryID = 5")
foreach(var item in products){item.CategoryID = 6;}
tbl.UpdateMany(products);
```

This will pull the records out (which are ExpandoObjects) with CategoryID of 5, set it to 6, and push the records back into the database within a transaction.

## Dynamics: Let's Talk About This For a Second

I know there are a number of people that don't care much for the dynamic bits coming in C# 4 - or regard them as a novelty. You lose intellisense with them and it's scary hippy code. This is a foundational change to C# - a language that has always relied on static typing and a compiler safety net - and as such can be a bit scary.

Stay with me a bit here. Relying on System.Dynamic has allowed me to remove about 95% of the cruft that would otherwise fill in this tiny little tool. It's 400 lines of code and does everything most other data access tools can do.

The secret sauce is the ExpandoObject. Everything that goes in and everything that comes out of Massive is an Expando - which allows you to do whatever you want with it. At it's core, an ExpandoObject is just an IDictionary - check it out - this is how I roll in the IDataReader from the database:

```csharp
/// 
/// Turns an IDataReader to a Dynamic list of things
///
public static List ToExpandoList(this IDataReader rdr) {
   var result = new List();
   //work with the Expando as a Dictionary
   while (rdr.Read()) {
       dynamic e = new ExpandoObject();
       var d = e as IDictionary;
       for (int i = 0; i 
```

You can cast your ExpandoObject as an IDictionary and work with it quite simply. There's no reflection required - it's actually quite fast.

It might be tempting to think that there's a ton of reflection going on here, which is bad for performance - but that's not the case. We're simply working with an IDictionary and getting away from the casting/boxxing/unboxxing craziness that dominates most data access code.

For instance - how many times have you had to wrestle with short vs. long? Decimal vs. Double? Guid vs. String? Me too - it's no fun when it bites you. You can forget all that with System.Dynamic (well, for the most part) - the DLR will do it's best to work with the CLR and coerce the type you need, WHEN you need it.

If I pull out a single product:

```csharp
var tbl = new Products();
var product = tbl.Single();
var price = product.UnitPrice.ToString("C");
```

The "price" variable there will be typed - but how is that type decided? It's already been decided by System.Data - a translation has happened when the query went off where the Database type was pushed to a System.Type - in this case a System.Decimal. If you type "product.UnitPrice.GetType()" into the Immediate Window in VS - you'll see this.

This is confusing, to say the least. If you've made it this far you'll either be viewing this as a curiosity or as something incredibly annoying. Stay with me.

When you use "var" you're essentially asking for a type to be figured out right then and there. The following, for example, won't work:

```csharp
var notReallyDynamic = new ExpandoObject();
notReallyDynamic.FictionalProperty = "Steve";
```

You'll get a Typing error - saying that "ExpandoObject does not contain a definition for FictionalProperty" - which makes no sense at all. The Expando isn't SUPPOSED TO.

But - if you use the "dynamic" keyword - you essentially tell the compiler you're going to be playing with a different set of rules:

```csharp
dynamic yesReallyDynamic = new ExpandoObject();
yesReallyDynamic.FictionalProperty = "Steve";
```

As Skeet would say: "Hurrah and Jolly Well Done!" So why am I going off on this? Because there's a curiosity at work here - something you'll need to understand if you're going to use Massive, and something that I think is actually kind of interesting.

If using "var" with an ExpandoObject takes the dynamic wind out of its sails - how then are we able to use "var" in this code:

```csharp
var tbl = new Products();
var product = tbl.Single();
var price = product.UnitPrice.ToString("C");
```

Here's the answer, and it's going to make your head hurt: **if you tell "var" that it's dynamic, everything's OK.**

I can hear you know: "WTF?!?!?!" Let's see if I can explain this.

The method "tbl.Single()" returns "dynamic" - NOT an ExpandoObject - "dynamic". A lot of people confuse the "dynamic" keyword to be synonymous with "var", but for dynamic stuff. That's not the case - it's a catch-all for anything and any dynamic type in System.Dynamic. It's a Voodoo Jedi Mind trick from Cthulu himself:

```csharp
/// 
/// Returns a single row from the database
///
/// 
ExpandoObject
public dynamic Single(object key) {
    var sql = string.Format("SELECT * FROM {0} WHERE {1} = @0", TableName, PrimaryKeyField);
    return Query(sql, key).FirstOrDefault();
}
```

Now I could have returned ExpandoObject here - but doing that would mean that all the code that uses "var" to pull a Single() record out would break - in the same way that using "var" with the ExpandoObject above broke - and you would be sad (me too).

## Thanks For The Lecture - Why Do I Care Again?

Good question. What if we worked up an object called "DynamicRequest" for our web site that looked something like this:

```csharp
/// 
/// A class that you can use to access HttpContext.Request a little easier
///
public class DynamicRequest: DynamicObject
{
    public override bool TryGetMember(GetMemberBinder binder, out object result) {
        string key = binder.Name;
        result = HttpContext.Current.Request[key] ?? "";

        return true;
    }
}
```

This is the equivalent of "MethodMissing" in Ruby and allows you work work with HttpContext.Request as if it were a regular old object.

In WebMatrix (or MS WebPages/Razor) - you can set this as a property on the Page object, which itself is dynamic. In _PageStart.cshtml (which gets called before every request to a page in that directory) you could initialize this little pretty:

```csharp
@{
  Page.Post = new DynamicRequest();
}
```

This is where it gets fun. On your Razor page (let's call it "Submit.cshtml") you can now do this:

```csharp
@{
  if(IsPost){
    var tbl = new Products()
    //as many of you will certainly point out - some type of whitelist should be implemented
    //to avoid over-posting.
    tbl.Insert(Page.Post)
  }
}
```

Given that we're using nothing but dynamic, Page.Post might as well be a Product as far as Massive is concerned. Types are out the window, so is the pain of working with them.

"But wait!" you say, "I don't understand because you can already use a NameValueCollection in your Insert() statement! Why is this such a cool thing?". GREAT question - the answer is because I can refactor this how I please, without touching the receiving code:

```csharp
@{
  if(IsPost){
    var tbl = new Products()
    dynamic product = new ExpandoObject();
    
    //let's White list this!
    product.ProductName = Request["ProductName"];
    product.UnitPrice = Request["UnitPrice"];

    tbl.Insert(product)
  }
}
```

Refactoring that was pretty darn simple wasn't it? Moreover - what's missing from the setting of the UnitPrice? CORRECT! Coercing it into a Decimal! Hey dynamics aren't so bad are they :).

## An Open Call For FUD


I know a number of people don't like Dynamics and they're tired of learning Yet Another Shiney New Whatever. All the old arguments apply, and people's noses will get out of joint. This is all good - it's part of the natural order that keeps dorks like me from jumping at Every New Sexxy Thing while at the same time adding some spice the dreck of the same old same old Data Access Story.

So, this is where you get to rev up your fear of how the horrible dev who works down the hall from you will utterly destroy all common sense, black goo dripping from the eyes of the innocent etc.

The short answer to that conundrum is that you're paying your dues. We've had to live with your crappy code, just as others have had to live with mine :). They took the time to educate me just as someone took the time to educate you. Time to pay it forward :).

In all seriousness :) I'd love to hear any thoughts you might have, positive or negative - but do me the courtesy of illustrating your thoughts with details rather than attacking me and my happiness with New and Shiney.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
  </entry>
  <entry>
    <title>A Dynamic Data Utility for WebMatrix</title>
    <link href="https://bigmachine.io/posts/the-super-dynamic-massive-freakshow" rel="alternate" type="text/html"/>
    <updated>2011-02-07T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/the-super-dynamic-massive-freakshow</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[## I Suppose It's Not an ORM


There's no Object, no Relational Mapping - it's all Dynamic - %100. It's not terribly structured either - I'm shooting for simple simple simple with a "Rails" flavor to it. There are other alternatives out there that embrace WebMatrix.Data - [Mark Rendle's Simple.Data (https://github.com/markrendle/Simple.Data) comes to mind.

There's nothing wrong with Mark's stuff - in fact [it looks very compelling.](http://blog.markrendle.net/2010/08/05/introducing-simple-data/) I'm just after something a bit different. I should mention that I mused on this over the weekend and currently I have everything compressed into 360 lines of code (give or take).

The LOC is important to me - I don't want to have a DLL that I can't tweak. I love the simplicity and transparency of a single code file.

Before I get to the code - I'll be talking about this and a few other neato things tomorrow (Tuesday 2/8/11) at the [MVC Conf.](http://www.mvcconf.com/). If you're not busy - drop by and I'll show it in action starting at 11am PST.

## Signatures

I don't mind writing inline SQL. Sort of - I don't mind writing SELECT queries, but INSERTs and UPDATEs bother me for some reason. I'd much rather write something like this:

```csharp
var table = new Products();
table.Insert(new { ProductName = "Jumbo Shrimp" });
//fix the name of product ID 10
table.Update(new { ProductName = "Da Kine Pig"}, 10)
```

Or something like

```csharp
var table = new Products();
dynamic thingy = new ExpandoObject();
thingy.ProductName = "Poi Poppers";
table.Insert(thingy);
```

Or maybe sidestep the whole object/dynamic thing:

```csharp
var table = new Products();
//make sure you have a whitelist!
table.Insert(Request.Post);
```

And that's what I made.

## DynamicModel


I wanted to stay as close to the "metal" as I possibly could. This (to me) meant that I didn't want to worry about Types, Tracking Containers, Contexts and so on. What I did want to do is describe my table - and this is the only type I have (which, ironically, is a Dynamic type):

```csharp
public class Products:DynamicModel {
    public Products():base(Database.Open("northwind")) {
        //set the PrimaryKeyField
        PrimaryKeyField = "ProductID";
        
        //I could optionally set the TableName too
        //or I could rely on convention, using the class type name
    }
}
```

In this class I'm using DynamicModel and setting the PrimaryKeyField. I don't need to do that, it would default to "ID" if I didn't. From here I can do fun things like run the Inserts/Updates above, or Selects:

```csharp
var table = new Products();

//all products
var products = table.All();

//products from category 5
var productsFive = table.All("where categoryID = @0", 5);

//get a single product
var product = table.Single(2);
```

That last line there is probably one of the more complicated aspects of the bits I wrote. Normally WebMatrix.Data would return a thing called "DynamicRecord" - which is basically a DynamicObject with some meta data that describes a database interaction.

I want to be able to use what I pull from the DB - so I set the rule that whatever comes out of the DB is an ExpandoObject - then I can set whatever I want on the object and send it back in:

```csharp
var table = new Products();

//get a single product
var product = table.Single(2);
product.ProductName = "Lomi Lomi Salmon";
table.Save(product);
```

It's also worth mentioning that I don't need to use any of these constructs - I can just use "table.Query" and "table.Execute" and send in raw SQL. The main point is to save time where possible - get out of the way the rest of the time.

## Validations


This is where the "Rails-y" part comes in. I really like model-level validations and how simple it is to "wire up" a set of validators on your model.

I **tried** to do that with `DynamicModel` - allowing you to wire in your validators in the constructor of your class:

```csharp
public class Products:DynamicModel {
    public Products():base(Database.Open("northwind")) {
        //set the PrimaryKeyField
        PrimaryKeyField = "ProductID";
        
        //this is a bit wordy - but I think it works
        ValidatesPresenceOfAlways("ProductName");
        
    }
}
```

If you use `table.Save()` or `Update()` or `Insert()` - these validations will fire. Currently there are 3 main types:

 - PresenceOf
 - Numericality
 - Custom
 
The custom validator works with a delegate (`Func`) that gets fired for the event you wire it to (insert, update, or both):

```csharp
public class Products:DynamicModel {
    public Products():base(Database.Open("northwind")) {
        //set the PrimaryKeyField
        PrimaryKeyField = "ProductID";

        //make sure that category id is between 1 and 5
        ValidatesOnInsert("Category ID Must be between 0 and 5", x => {
            return x.CategoryID > 0 && x.CategoryID 

```
It's simplistic - that's for sure and there's a lot more thought to put into their use. There are a number of validators out there that I could lean on - but they're focused on Statically Typed stuff. When you're working with dynamic/expando there are some gray areas.

Specifically - if you have a "ValidatesPresenceOf('CategoryID')" and you pass in an anonymous type like this:

```csharp
var table = new Products();
table.Update(new { ProductName = "Da Kine Pig"}, 10)
```

Is the `CategoryID` not present? Well, literally, that's true - but it *is* present in the database and we're not trying to change that. Although it's also possible that I could drop in a `CategoryID == null` here too, even though that would be silly.

I don't know - I might rip these things right out. Just thought I'd share where I'm at with them currently.

## Why Did I Do This Again?


The short answer is that I needed it. Mark's work (link above) is fine - but didn't do what I was hoping at the time. I just wanted a bit of a "utility" - not a full blown framework for running data. If Mark can the stuff I've kicked up here - hooray! I'd be happy to fork and contribute when it's ready. I'm not looking to start up Yet Another Data Access Tool.

My other goal is to show the possibilities of "Thinking Dynamic" with a light framework like WebMatrix. This is a super-raw prototype, but with a bit of polish it can take a large chunk of time out of your data work.

If you're wondering where the code is - I'll push it to Github in the coming weeks. I'll probably extract a few more things from what I'm doing, and I also need to make sure that it's not just plain stupid what I'm doing.

I also want to work in support for Transactions. At it's core, WebMatrix.Data uses System.Data.Common - and allows you to work directly against the Connection, which means you can send transactions in.

This is immensely helpful for large data transactions - I just need to figure out how to work it with a simple concept. I don't want to kick up an object tracking container, but I do have some ideas in the back of my mind.

## A Bit of a Teaser


If you do drop by MVC Conf tomorrow, I'll show you the other fun thing I've been working on for both MVC Conf and [Tekpub's forthcoming WebMatrix Production.](http://tekpub.com)
  
Like this little data toolio - it's not completely ready for prime time, but it's working for the most part.

Here's some pics:

![](http://bigmachine.io/img/massive-test.png)

![](http://rob.conery.io/img/massive-results.png)]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
  </entry>
  <entry>
    <title>Marla Singer Didn&apos;t Exist</title>
    <link href="https://bigmachine.io/posts/marla-singer-didnt-exist" rel="alternate" type="text/html"/>
    <updated>2011-01-03T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/marla-singer-didnt-exist</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[<p><img src="https://bigmachine.io/img/2011/01/marla2.png" alt="Marla Singer Didn&apos;t Exist" /></p>
## Digging A Little Deeper Into Fight Club

We know that the protagonist (Cornelius/Tyler/Ed Norton) is slowly losing his mind due to ... something. He has insomnia - but the cause is only lightly touched on. A few scenes are dedicated to graphically showing you why - rather than hitting you over the head with drippy dialog.

He's a "Recall Coordinator" that flies around the country staring at death and destruction - and has to apply math to decide whether to perform a recall. His recounting of this is incredibly cold:

> Take the number of vehicles in the field: A, multiplied by the probable rate of failure: B, then multiply the result by the average out of court settlement: C. A times B times C equals X - if X is less than the cost of a recall, then we don't do one.

This hints at the kind of person Tyler is. What kind of person takes a job like the one he's portraying? This isn't my main point - but it's important to understand that Tyler is a pressure-cooker that has two primary forces brewing inside of him: *Animal rage and the need to destroy* - not only himself but the world around him. This is fueled by the destruction he sees on a daily basis from his job.


**Guilt, remorse, regret, fear and pain** - this is fueled by the loss of life that is casually dealt with by his job 

<img src="/img/2011/01/tyler1.png" alt="The father must of been huge... you see where the fat's burned to the seat? Very modern art..." width="640" height="350" class="size-full wp-image-266" />


These psychological forces make a pretty tidy Yin-Yang. The two forces spinning in opposition that tear Tyler apart. He didn't drift into psychosis - no one ever does - he was pulled apart at the seams and we watch as the two main players in his psyche do battle (and have sex in a perfectly ironic love/destruction battle).

The problem is - the movie makes it clear that Tyler (Brad Pitt) was only a part of Ed Norton's imagination. Not so much for Marla. This is too single-sided, too convenient for a movie - a "Single Serving Plot" if you will. It's a subtle bit of film trickery - that by not doing the full reveal we're forced to take Marla home with us in our minds as a real being. A plot point.

But she's not. She's not "rounded" as a character. She has no reason to exist in the film other than to mirror Norton's psychosis. In every scene she antagonizes him - either directly or through guilt/fear and pain. She IS his guilt, fear and pain... and she shows herself when he tries to contain the forces inside of him by going to groups as a form of therapy...

## Enter Marla

Fight club is heavy on symbolism - and Marla's entrance is no exception. This is our first glimpse of her:

<img src="/img/2011/01/marla2.png" alt="This is cancer, right?" width="640" height="368" class="size-full wp-image-264" />

That's a perfect opening line for Marla. It's not a question - it's a statement, about herself. Tyler immediately recognizes her, and hates her almost immediately:

>Marla... the Big Tourist. Her lie reflected my lie.. and suddenly, I felt nothing... I couldn't cry. So once again, I couldn't sleep.


She is his remorse. His guilt. *His lies* - not only from posing in these Sickness Circles, but also the lie that is his life (IKEA catalog scene, his job - his whole life is a massive lie). He felt relief from his guilt and remorse for a bit, but his psyche mounted a counter-attack, in the form of Marla Singer.

You're probably thinking this is "plausible - but no way because later in the movie...". I'm getting there. I have to build the case just a bit more.

## Directorial Casting

A good director will straddle the line between giving it up entirely, and making your subconscience work. Symbolism is a direct pipe into your brain, and a really good director won't tell you AT ALL what it is they want you to react to, see, or feel. Fincher (Director of Fight Club) is a director that completely understands this.

He's also a bit playful. He comes right out it with it as a subplot of the movie: Tyler has a job in the theater where he splices images into films. In a way, Fincher is casting *himself* in the movie in a meta-tangential way. In many scenes you see Tyler spliced right in - like this one right after he sees Marla for the first time. He's staring after her, wondering who she is and why she's there:

<img src="/img/2011/01/tyler-clip-before.png" alt="Staring after Marla" width="640" height="346" class="size-full wp-image-268" />

And then BAM - guess who shows up:

<img src="/img/2011/01/tyler-clip.png" alt="Oh hello there!" width="640" height="341" class="size-full wp-image-269" />

The fact that Fincher splices Tyler in all over the beginning of the movie is telling you, in your face, I'm messing with you - inserting himself quite literally as part of the cast:

<img src="/img/2011/01/tyler-projectionist.png" alt="Inserting Tyler" width="640" height="305" class="size-full wp-image-270" />

The fact that the "Big Reveal" isn't given (that Marla wasn't real) should not dissuade you from contemplating it. The film is about the subtle loss of a grip on reality through the use of imagery and self-deception.

## Some Not-So-Subtle Hints

Consider the dialog when Tyler (Norton) confronts Marla and the ensuing scenes:

> Marla: I saw you practicing this
> Tyler: Practicing what?
> Telling me off... is it going as well as you hoped?


It's a challenge. You can't get rid of me that easy dude - **I'm your pain, your power, your remorse**. Don't think you can get rid of me by going to some groups and trying to get some sleep. Not that easy.

Something else to notice is Marla's demeanor and wardrobe. She's cool, confident, sexxy and strong. She's in control - she owns Tyler completely and he's absolutely afraid of her as witnessed by the scene above. She confronts him and he unravels - and get's a bit sensitive... which is another "tell" in the way the conversation is carried out:

<img src="/img/2011/01/tyler-marla-entwined.png" alt="They really listen to you..." class="size-full wp-image-265" />

> Tyler: when people think you're dying man they really really listen to you... instead of just -
> Marla: instead of just waiting for their turn to speak?


This is a sensitive moment - probably the most emotionally honest moment of the film for Tyler. He doesn't feel heard, he doesn't have a voice, he can't stand up for himself (which, ironically, is what's happening here with Marla). The twist of her finishing that sentence for him is delicious - and get this - **while they are entwined as a single being**.


In the ensuing scene - where Tyler insists that she go elsewhere - they begin to negotiate. I could go off on the 5 stages of grief and how this fits it... but let's stay on track. Marla charges into a laundry and steals some clothes, then barges into the street without looking (a death wish). Many will point to this scene and say "AHA! Your theory SUCKS!"

But notice how the scene plays out (which is repeated later in the movie) - and Tyler's reaction:

<img src="/img/2011/01/marla-street.png" alt="marla-street"  class="alignnone size-full wp-image-272" />

<img src="/img/2011/01/tyler-chasing-marla.png" alt="tyler-chasing-marla" class="alignnone size-full wp-image-273" />

The scene moves in a bit of a disorienting blur, following a car, over to Tyler who's waiting on the corner. The car honks loudly - as if at HIM, and he waves as if to say "sorry". How could this be if he's on the sidewalk?

The answer is the same as watching Pitt's Tyler talk to the crowd during the fights - he's watching himself as another persona. This gets amplified in the following scene, where Marla charges into traffic YET AGAIN - but this time, apparently emboldened, Tyler stays put.

And the cars don't honk. Don't care that she's there. Which is because she is not...

It's tempting to nit-pick this, but why else would there be a juxtaposition of **the exact same scene.**

## The Change Over

As the movie progresses Marla becomes more and more washed out. The initial scenes of her are all in the dark, amplifying and enforcing the darkness and remorse that she represents. She looks best here - her clothes are clean and black, she blends in, she's the darkness of Tyler's mind.

Tyler describes Marla in almost these exact terms:

>Marla: the little scratch on the roof of your mouth that would heal if only you could stop tonguing it, but you can't.

Self-induced pain and irritation that he can't stop ... tonguing. He's using this pain - flexxing it, tonguing it greedily because it's also creating Tyler (Pitt). In contrast to the idea that Pitt is the result of Norton's slide into insanity - he's not. He's a defense mechanism.

When Pitt comes in he's all about reacting to "what you're told to be and do". He reflects on parental betrayal and that you're "not a unique snowflake" etc (which we're all familiar with). He's trying to toughen up the Sad Tyler - to make him fight his own mental crap in order to get ahold of himself. To hit rock bottom, give up and accept that "God doesn't like him" (which brings us back to the 5 stages of grief... the last being acceptance).

**Tyler is trying to heal himself with his animal ego.**

The ensuing interactions between Pitt, Norton, and Marla are awash in psychological overtones. Sex between the alter egos igniting a Freudian freak show of pain, sex, and self-torture. This is made a bit Greek when Norton states how his parents used to pull the same disappearing act when he was a child.

Two parts of the whole - mother and father, female and male, and all the things that they represent... fighting for dominance in the main character.

As the fight for dominance continues, it's clear that Marla (remorse and guilt) is losing. Her scenes are dwindling, and in each she begins to look paler and act more and more defeated, culminating in this scene in the kitchen:

<img src="/img/2011/01/marla-sad.png" alt="marla-sad"  class="alignnone size-full wp-image-274" />

This scene ends with Marla being pushed out of the house (the house, of course, is a symbol for Tyler's psyche). We don't see Marla much as the movie transitions into "guy stuff". No more support groups, no more introspection - just blood and punches.

## Marla, Defeated

The next meaningful scene from Marla comes when Norton awakes from what seems to be a long, long nap and is wandering around his house, wondering what has happened (it's Project Mayhem). He intones:

> I'm all alone. My father dumped me, Tyler dumped me, I am Jack's Broken Heart...

Self-pity, self-loathing... pain and guilt. Identifying Tyler (Pitt) with his father and essentially lapsing back into his pathetic state at the beginning of the film:

<img src="/img/2011/01/tyler-pathetic.png" alt="tyler-pathetic"  class="alignnone size-full wp-image-275" />

And who should appear right then? Yep - Marla. Looking black again... and it's night time. She's literally in her element... and she strikes:

> Marla: Can I come in?
> Tyler: He's not here...
> Marla: Wh...what?

TYLER. ISN'T. HERE. Tyler went away. Tyler's GONE.

What many people don't realize about this scene is that Tyler has "flipped" into Pitt's character right here. His defenses are pricked up because Marla is trying to attack while Sad Tyler is vulnerable. Enter Animal Tyler - flatly stating that she can't come into the house (Tyler's brain) because Tyler is gone (he's MINE). You lose.

One of the great things about this movie is the acting. Helena Bonham Carter just nails this defeated look:

<img src="/img/2011/01/marla-defeated.png" alt="marla-defeated"  class="alignnone size-full wp-image-276" />

It's easy to look at this and say "that's rejection - not defeat" and write her off as the "girl, hurt". But look again. You can see the rage - the powerlessness. The fact that it takes place outside of Tyler's house is perfectly executed.

## Marla's Denouement

The main thrust of Marla's character (whether you believe she exists or not) is to balance and counter the animal that is Pitt's character. This is seen rather directly when Tyler realizes that he's been seeing things - and that Pitt's character is completely made up.

He feels fooled and deceived - by himself no less - so he does what anyone would do. Call mom (or in this case, what mom represents in a Greek way). Re-enter Marla. It's Marla that confirms that Tyler doesn't exist - while chastising and punishing Norton for giving into him.

What happens next is a scene that many people point to as proof that Marla did in fact exist: the restaurant scene. In that scene, the waiter suggests some food for her, after Norton states that he wants it "clean":

> In that case sir may I advise against the lady eating the clam chowder...

<img src="/img/2011/01/marla-clam-chowder.png" alt="marla-clam-chowder"  class="alignnone size-full wp-image-277" />

In this scene you can see the waiter acknowledge Marla's order, and then recommend something different for her. So she exists, right?

**Wrong**. Tyler's minions know he's insane - that's made clear throughout the movie with quips like "he only sleeps 2 hours and a night" and "he was born and raised in an insane asylum". They've been prepped by Pitt's character as to what to say when Norton tells them to stop Project Mayhem.

*They know he's nuts, and they compensate for it.*

The waiter doesn't speak to Marla - he can't, that would be crazy of HIM. He simply pretends she's there because Tyler is

**sitting right in front of him.**

Tyler is a rock star, a demi-god, and he better not screw up. So he goes along with Tyler's insanity - the telling part of the scene being where he talks directly to Tyler, **whispering** his suggestion for Marla. She's not there. He knows it - so he plays along as well as he can, which is actually rather clumsy.

Another scene that people will often refer to is the final scene - where Tyler looks out the window and sees his henchmen dragging Marla out of a bus. They've been told to find her, apparently, as she can undo the whole thing.

What happens in this final scene is incredibly debatable. In fact you could argue that none of it even existed at all - a grand illusion from the very definition of Salinger's Unreliable Narrator. The books suggests he's been committed already and we're simply seeing one very grand psychotic break. I think it might be a little bit more than that, and something possibly a bit more fun.

Henchmen? Dude in a tower who has the fair maiden brought to him as he watches his destructive hand take down civilization around him? The fact that we even believe this scene is a joke - mostly on us.

As you watch it play out, ludicrous things happen one after the other. Norton blows a hole in his neck - apparently "killing" Tyler. He should be dead - but he's not.

The henchmen come in and leave. "I'll meet you guys downstairs" - really? They just set charges all over town in these massive buildings, why are they leaving? They've brought beer and chips for a party - one of them asking "where is everybody" as they get out of the elevator.

## Walls of Jericho

That's the name of the final scene in the movie - which is a very interesting title. The phrase, which most people have heard, is from the Bible and refers to the walls erected around the town of Jericho in the land of Canaan. Long story short - the Israelites needed a home, and Joshua decided it would be Jericho.

In the Bible, God spoke to Joshua and gave him a rather extraordinary plan: surround the city with your soldiers at the call of a ram's horn - do this for 6 days (blowing horns around the town). On the 7th day - blow the horns again but this time, everyone shout at the same time as well, and the walls will crumble.

Joshua was a bit conflicted by this - does he trust God with this weird plan, or does he trust himself? He's a pretty good military leader - and he'll lose his command if God's plan doesn't work out. So he decides to stick with God and boom - [and the walls came down.](http://www.youtube.com/watch?v=_kX8lqXAONg)

This is one interesting scene. The dialog is plastic and 2-dimensional and Norton's character is neither pathetic - nor is he seemingly real. The whole thing is just... not realistic at all.

Norton stands there, with Marla, and watches as the buildings fall. A plan that he had no part in - it was carried out by an unseen force... a twisted Divine Plan if you will.

Both Marla and Tyler seem completely out of character here. Marla is unsure of everything - Tyler's dialog is cliche and trite. It's as if the ending is sort of being "made up" out of thin air - as if there is no ending - like we're being messed with.

And indeed we are:

<img src="/img/2011/01/tyler-marla-blurred.png" alt="tyler-marla-blurred"  class="alignnone size-full wp-image-278" />

Most people will remember the shot of a big ... male organ right at the end as the film skips around. I wanted to keep this somewhat safe for work ... so I didn't put here.

Fincher. What we're seeing is BS. A tidy ending that is utterly orthogonal to the story and reality. It's fantasy - all of it. The fact that Marla is in it is no more a justification of her existence than is the reality of the buildings falling.

OK - I have to get back to work now.

<img src="/img/2011/01/tyler-marla-ending.png" alt="tyler-marla-ending" class="alignnone size-full wp-image-279" />

## Update 2015

So many people have sent emails about this and pinged me on Twitter,  some nice, some not so nice. They all point to **literal circumstances** in the film - where person X saw Tyler talking to Marla so *therefore she must exist*.

While interesting, it doesn't really matter. The group of bar patrons walked up on Tyler and Pitt beating the crap out of each other in the parking lot ("hey fellas...") - only later did you realize he was hitting himself.

The difference with Marla is that **you're not told explicitly** like you are with Pitt. So, if your "evidence" is that someone looked at her, or reacted to her presence... well I spose I'd say *see the rest of the film where people react to Pitt as well*.

Ultimately: this is one person's massive psychotic break and *none of it is real* so this debate is really academic. In fact as I was watching it recently I noticed something incredibly telling.

The scene where he sits with the doctor asking for help so he can sleep. He begs the doctor for some pills. Watch the scene again and focus on Tyler's twitching hand, and see the doctor noticing it.

Tyler isn't just suffering from insomnia, **he's also having withdrawals**. He's trying to kick, and he's losing his mind... and we get to go along for the ride...

What a fun movie!]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://bigmachine.io/img/2011/01/marla2.png" />
    <media:content xmlns:media="http://search.yahoo.com/mrss/" medium="image" url="https://bigmachine.io/img/2011/01/marla2.png" />
  </entry>
  <entry>
    <title>Open ID Is A Nightmare</title>
    <link href="https://bigmachine.io/posts/open-id-is-a-party-that-happened" rel="alternate" type="text/html"/>
    <updated>2010-11-17T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/open-id-is-a-party-that-happened</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[## It Seemed Like Such a Great Idea


I know the first thing you're going to think: "StackOverflow seems to have worked it just fine". I said this very thing about 12 times over the last year as I pounded my head against my desk, wondering what I was missing. James then said to me "really? when's the last time you logged in?"

So I head over to Stackoverflow and login using the Open ID I use for everything - Google. And I'm met with this:

![](https://bigmachine.io/img/so_openid.png)

James is laughing hysterically as I sputter... "What the FU**! You GOTTA be KIDDING me!" I don't have a ton of rep over there - but I spose it's enough to care about. In truth I haven't had to login over there in a very long time - I've just been really busy.

So I start going through the recovery process:

![](https://blog.bigmachine.io/img/so_openid_recover.png)

Problem is I have a few emails that I use - and if I didn't use my Google login (as I thought I did above) - I really have no idea which email it is. So I start rifling off all of my emails, waiting to see if SO would know who I am.

Honestly - I feel dumb about this. How could I possibly forget how I logged in? Also - how could I not know which emails are associated with which providers I setup? And hey WAIT A MINUTE, if Stackoverflow doesn't require email... well what if my provider didn't send it? I have a lot more to say about that, but let's get back on track.

As I sit there and grumble about emails and Open ID, Avery is in my ear saying "Dude you used a different provider". Not a chance - I don't have another Open ID as far as I can remember.

*Oh shit. Right.*

I remember that I once setup a MyOpenID account! That was years ago and I never use it ... but YES! It's coming back to me now... So I head over to MyOpenID and go to login and...

Yep, username/password and I have no idea what it is. I try 5 or so combinations and finally figure it out. And I login to StackOverflow. "At least Jeff doesn't charge money" I think. If he did, I have a feeling this feature would be gone.

And James is still laughing.

## Welcome To My Nightmare

I'm both a developer and a business owner. I care about the technology, I care about my users more. The story I relayed above happens about 3 to 5 times a week for people who are paying me money. People forget their Open ID - it happens a lot more than you would believe. And when they forget, I need to help them remember.

I search our database and find 4-5 entries for them (we let anyone login and create an account with their Open ID) - once they login they want to go watch a video. And all they see is that their account doesn't have a subscription anymore and they get pissed off, and have to contact me.

You never want to piss off your customer. You NEVER NEVER want to make them feel stupid. Both of those things are happening now: You have to gently remind them that they logged in with Twitter, Facebook AND Google and ... "do you remember which one you created the order with?". Eventually you solve the issue - but they remain upset.

I caused this problem. I caused it because I'm making it too hard for the user to get into the sight - and that's really where the story ends for me. I don't care why, honestly. They don't have a problem with username/password - they need to use that for their Open ID provider anyway.

Boil this down from a User's perspective:*User wants to watch a Tekpub flick, so they come to our site

 - User has to login, our Open ID system kicks them over to their provider
 - They enter the username/password for that system
 - They come back and they watch their flick (if they've remembered correctly)
 
As a developer I'm happy about this because I'm not storing credentials (which is a solved problem as far as I'm concerned). As a business owner I'm wondering why we need 4 steps to happiness. They enter their username/password anyway? What's the damn difference?

## Anatomy of a Authentication Nightmare

When we started Tekpub, it was with pure Open ID using a javascript Open ID selector. I liked it, it worked pretty well in the beginning. And then the complaints started rolling in...

People would login successfully once, pay for a subscription, then login later and the sub would be gone. Turns out that Yahoo and Google have a different idea about what Open ID is supposed to do - because the the Identifier used for these users would change based on... some voodoo (sorry, but that's all I can deduce).

All of a sudden Google (by far our most popular provider) would change the token (the encrypted value on the end of the Open ID) and boom - you're completely lost to us. We have no other way of knowing who you are - and more than once I've had to track people by their PayPal accounts (we track the transaction ids - which we can look up through PayPal to find out who you are).

This problem gets worse when Google allows you to engineer your own URL - which they did about a year ago.

To mitigate this issue I decide that we need to use the email instead of the identifier. Open ID providers don't have to provide an email - in fact about half of them don't (which is why, I think, Stackoverflow doesn't require one. They can't reliably get one using Open ID). Google is good about this, however - they always return the email.

When sending a request to a provider for a user's credentials (after they've logged in) - you have to structure a bit of an incantation based on conflicting standards to get additional information. There's all kinds of stuff you can ask for - name, address, gender, and yes - email. We asked - more often then not, we were denied.

So all was working well with Google - we were using the email to identify people. **And then they decided that UK folks would have their extension changed from "gmail.com" to "googlemail.com".**

*And just like that, we are on our asses again.*

## The Dumbest Decision I've Ever Made

The code for our Open ID responder is spiraling, with just about every conditional setting in there you can imagine for the various providers out there. I'll save you the grumbling rant, but coding up Open ID stuff is utterly mind-numbing frustration. And while I'm doing this, I'm not recording videos... creating product.

Then I read about JanRain and their service RPXNow. I'll jump on this and say that as far as service providers go - they have been just fine. Their service is generally up 99.999999% of the time, which is pretty outstanding for a SaS provider.

In addition they offer access to other providers: Facebook, Twitter, Wordpress, Yahoo... and they do all the "heavy lifting" for you in terms of understanding which standards incantation goes to which provider.

I plumbed it in, it worked immediately and I was stoked! And then all hell broke loose...

Let me just preface this by saying of all the failure points in your business - you really don't want the door to be locked while you stand behind the counter waiting for business. No, let me rephrase that: you don't want the door jammed shut, completely unopenable while your customers wait outside - irate that you won't let them in.

You don't want that ever. Not even once, not even for 10 seconds. We're very, very small and this kind of thing will sink us.

3 times the RPX people changed their API on us and our authentication system went down. We were able to fix it within 20 - 30 minutes each time; once, however, was at 5 in the morning.

RPX as a service has gone down 3 times in the last year. This last time (about 4 months ago) it was down for about 4 hours - effectively shutting our doors for us. This isn't a small provder - there are many, many, many businesses that rely on them to authenticate their users.

It happens. Services go down and we are patient. As a developer and user I need to learn how to say that. As a business owner you can go f*** yourself if you think I'll use you again.

## Rolling Your Own


I've thought about it - the problem is that we're committed now. RPX uses OAuth to talk to Twitter and some special sauce to talk to Facebook Connect. I'm not going to write that code - I'd rather deal with the outages.

Coding against the Open ID spec (and now OAuth) is utterly ridiculous. This is a dying standard, being dominated by one or two providers who change their minds as they see fit - taking the specs with them. I'm sure you recall when Windows Live got into the Open ID circus - playing by their own rules. Google retaliated doing the same.

I'm caught in the middle of this "user information warfare" as a business owner - and I choose not to fight. I choose to walk away, my finger in the air proudly shown to both of them.

## The Solution

Starting with our next release (coming in a few weeks) I'm weaving in a stronger presence for "traditional authentication". We already have the ability to sign up with username/password - but we're going to make that the *only* way to register for our site from now on.

If you want, you can add Open ID to it moving forward. But no longer will my business be closed because Open ID gets flakey. I also want you, as our customer, NOT to feel stupid when you can't remember your login.

I'm sure there are some who will defend Open ID and tell me all the ways I screwed up. Unless you own a business of your own, running behind Open ID, I truly don't want to hear about it. Put money on your decisions - put the customer first - and you change your tune.

A friend of mine was offering a ton of solutions to my Open ID woes over Skype the other day - insisting that it's worth "investing in for the long run - the kinks will get worked out". I sort of agree as a dev - as a business owner I couldn't give a rat's ass.

The best part? 5 days after our discussion, his custom Open ID provider crapped out - I don't know what happened (you can be your own provider if you like - it means a bit of code here and there but some people like the custom domains). He couldn't login to Tekpub... and I had to push his subscription over to a new account.

What a mess.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
  </entry>
  <entry>
    <title>Creating IN Queries With Linq To Sql</title>
    <link href="https://bigmachine.io/posts/creating-in-queries-with-linq-to-sql" rel="alternate" type="text/html"/>
    <updated>2008-02-27T00:00:00.000Z</updated>
    <id>https://bigmachine.io/posts/creating-in-queries-with-linq-to-sql</id>
    <summary type="text">&gt;-</summary>
    <content type="html"><![CDATA[Props on this one go to Scott Hanselman who pulled me back from the edge of the cliff last night. I was particularly distraught in getting a MIX demo together where I had to do some queries using LINQ, and I couldn't for the life of me figure out how to fashion an IN query! With Scott's help (and patience) I figured it out, and thought I should blog for my own reference, at least.  
   
## It Depends On What Your Definition of "IN" Is...
    

An IN query will pull back a set of results from SQL that is within a given range. This range can be set manually, or can itself be a query. So if you have an eCommerce application and you want to know what products you have in a given user's cart, you could do this (using AdventureWorks):  

```sql
SELECT * FROM Production.Product WHERE ProductID IN (SELECT ProductID FROM Sales.ShoppingCartItem WHERE ShoppingCartID='RobsCart')
```
 
This will return all the Product records that are in my cart. This is a fundamental query structure and up until today I thought, for sure, that Linq To Sql doesn't support it. I was sort of right - but not really.  
   
## LINQ Is People To (Or It's Made From People...)

It's important to remember that the people that made LINQ were trying to approximate a "SQL within Code" sort of thing - this means that they built LINQ to query just about anything, and also built a SQL Translator called Linq To Sql. They ran into limitations with trying to contort a static language structure (VB or C#) into SQL, but for the most part if you think long enough (or Skype Hanselman) you can figure it out.
  
The key here is to think "Top Down" (please, no flames...to alleviate the "Top Down" reference, I'll use LOL Cats to describe the problem statement):  

```
WTF GIMMEH CART
    
CART CAN HAZ PRODUCTS?
    
GIMMEH PRODUCTS (NOM NOM NOM)
```
If you break it down this way (and not in SQL terms as above), you can begin to see how LINQ might make a query out of this: Start with the Cart (pretend my cart is ID=75144):  

```csharp
AdventureWorks.DB db=new DB();

var itemQuery = from cartItems in db.SalesOrderDetails
                where cartItems.SalesOrderID == 75144
                select cartItems.ProductID;
```

Next we need to get the products, but only those that are in the cart. We do this by using our first query, inside the second:

```csharp
var myProducts = from p in db.Products               
where itemQuery.Contains(p.ProductID)
select p;
```

Here is the key to this weirdness:

> Linq To Sql only constructs the query when the Enumerator is tripped.

So as whacky as this structure may look, know that what you're doing here is creating a set of Expressions that Linq To Sql is going to parse into a SQL Statement, and it will only execute that statement when you enumerate over the results, or ask it to actually do something with the result set (like Count(), ToList(), etc). **So despite how it looks - only one query is being executed**.

It might take you 10 different LINQ statements to get what you want - but know that you can nest all of them and only call the database once.

If you've looked over the "101 LINQ Examples" site, you may know this - but I found it really groovy that you can embed anything IQueryable inside of another IQueryable statement (IQueryable is what your "var" is when you do the above query).

Here's the generated SQL for the above query:

```sql
SELECT [t0].[ProductID], [t0].[Name], [t0].[ProductNumber], [t0].[MakeFlag], [t0].[FinishedGoodsFlag], 
[t0].[Color], [t0].[SafetyStockLevel], [t0].[ReorderPoint], [t0].[StandardCost], [t0].[ListPrice], 
[t0].[
Size], [t0].[SizeUnitMeasureCode], [t0].[WeightUnitMeasureCode], [t0].[Weight], 
[t0].[DaysToManufacture], [t0].[ProductLine], [t0].[
Class], [t0].[Style], [t0].[ProductSubcategoryID], 
[t0].[ProductModelID], [t0].[SellStartDate], [t0].[SellEndDate], [t0].[DiscontinuedDate], 
[t0].[rowguid], [t0].[ModifiedDate]

FROM [Production].[Product] 
AS [t0]

WHERE 
EXISTS(
    
SELECT 
NULL 
AS [EMPTY]
    
FROM [Sales].[SalesOrderDetail] 
AS [t1]
    
WHERE ([t1].[ProductID] = [t0].[ProductID]) 
AND ([t1].[SalesOrderID] = @p0)
)
```

Notice that rather than an "IN" statement, we get a "WHERE EXISTS" - which is just about synonymous with the IN statement. I had a bit of a gag reflex when I saw the `SELECT NULL AS [EMPTY]` but that's simply an empty return set - the SELECT lookup is not interested in returning the record - only that it EXISTS. So in terms of efficiency, this is about as good as it gets.

## What If IN Didn't EXIST?


I didn't really generate an IN statement - [but this guy did](http://coolthingoftheday.blogspot.com/2008/01/being-in-in-linq-to-sql-or-how-i.html) and he tipped me off to nesting the query bits. Notice that, in his case, he didn't need to create an IQueryable - he just used an Array. This is where the fun starts with these queries - LINQ is a whole mess of extensions (at it's core) that hang off of IEnumerable. Linq To Sql will (in most cases) parse these expressions out and allow you to work with them in the context of a query.


In other words, I could have written the LINQ query above, like this:

```csharp
int[] productList = new int[] { 1, 2, 3, 4 };
var myProducts = from p in db.Products
                where productList.Contains(p.ProductID)
                select p;
```

And the generated SQL would be:

```sql
SELECT [t0].[ProductID], [t0].[Name], [t0].[ProductNumber], [t0].[MakeFlag], [t0].[FinishedGoodsFlag], 
[t0].[Color], [t0].[SafetyStockLevel], [t0].[ReorderPoint], [t0].[StandardCost], [t0].[ListPrice], 
[t0].[
Size], [t0].[SizeUnitMeasureCode], [t0].[WeightUnitMeasureCode], [t0].[Weight], [t0].[DaysToManufacture], 
[t0].[ProductLine], [t0].[
Class], [t0].[Style], [t0].[ProductSubcategoryID], [t0].[ProductModelID], 
[t0].[SellStartDate], [t0].[SellEndDate], [t0].[DiscontinuedDate], [t0].[rowguid], [t0].[ModifiedDate]

FROM [Production].[Product] 
AS [t0]

WHERE [t0].[ProductID] 
**IN (@p0, @p1, @p2, @p3)**
```

Hey! Look at that! Something I didn't think was possible actually is!

I hope you're starting to see the pattern here - and that the IN statement is built on the reverse thinking of a SQL statement. In other words you're not saying "confine this result set to this range", it's more of a "use this range to confine the result set" - which is just the kind of thing a programmer might thing, and fits right in with the rest of the LINQ syntax. Sounds subtle - but it's very important when you doing this type of querying to remember that LINQ is a programmatic construct - NOT a SQL construct.]]></content>
    <author>
      <name>Rob Conery</name>
    </author>
  </entry>
</feed>